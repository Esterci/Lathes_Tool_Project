{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "error",
     "timestamp": 1573049995271,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "zz62z0U1FQ6O",
    "outputId": "be52ca7a-9aa4-4dc1-deec-96fe0643de4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "from psutil import cpu_percent\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import io\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from tsfresh.feature_extraction import extract_features, ComprehensiveFCParameters\n",
    "import os\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recovery (DataName):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/PCA_Analyses/\";\n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    PCA_Analyses_path = base_path + add_path1;\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "     \n",
    "    \n",
    "    if DataName == 'D_S_parameters':\n",
    "        \n",
    "        # Now change to Kernel directory\n",
    "    \n",
    "        os.chdir( Kernel_path );\n",
    "        \n",
    "        Final_Target = np.genfromtxt('FinalTarget.csv', delimiter = ',')\n",
    "        \n",
    "        # Now change to Recovery directory\n",
    "    \n",
    "        os.chdir( Recovery_path );\n",
    "        \n",
    "        P_N_groups = int(np.load('M_N_groups.npy'))\n",
    "        Output_Id = int(np.load('ID.npy'))\n",
    "        P_N_Ids = int(np.load('N_IDs.npy'))\n",
    "        \n",
    "        # Now change to base directory\n",
    "    \n",
    "        os.chdir( base_path );\n",
    "        \n",
    "        Output = {'FinalTarget': Final_Target,\n",
    "                  'M_N_groups': P_N_groups,\n",
    "                  'ID': Output_Id,\n",
    "                  'N_IDs': P_N_Ids}\n",
    "        \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Final working directory %s\" % retval)\n",
    "        \n",
    "        return Output\n",
    "    \n",
    "    elif DataName == 'ExtractedNames':\n",
    "        \n",
    "        # Now change to Recovery directory\n",
    "    \n",
    "        os.chdir( Recovery_path );\n",
    "        \n",
    "        extracted_names = np.load('extracted_names.npy')\n",
    "        \n",
    "        # Now change to base directory\n",
    "    \n",
    "        os.chdir( base_path );\n",
    "        \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Final working directory %s\" % retval)\n",
    "    \n",
    "        return extracted_names\n",
    "    \n",
    "    elif DataName == 'SelectedFeatures':\n",
    "        \n",
    "        # Now change to Recovery directory\n",
    "    \n",
    "        os.chdir( Recovery_path );\n",
    "        \n",
    "        Output_Id = int(np.load('ID.npy'))\n",
    "        \n",
    "        # Now change to Kernel directory\n",
    "    \n",
    "        os.chdir( Kernel_path );\n",
    "        \n",
    "        features_filtered_1 = pd.read_csv('features_filtered_' + str(Output_Id) + '.csv') \n",
    "        \n",
    "        # Now change to base directory\n",
    "    \n",
    "        os.chdir( base_path );\n",
    "        \n",
    "        Output = {'FeaturesFiltered': features_filtered_1,\n",
    "                  'ID': Output_Id}\n",
    "        \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Final working directory %s\" % retval)\n",
    "        \n",
    "        return Output\n",
    "        \n",
    "    elif DataName == 'ReducedFeatures':\n",
    "        \n",
    "        # Now change to Recovery directory\n",
    "    \n",
    "        os.chdir( Recovery_path );\n",
    "        \n",
    "        Output_Id = int(np.load('ID.npy'))\n",
    "        \n",
    "        # Now change to PCA Analyses directory\n",
    "    \n",
    "        os.chdir( PCA_Analyses_path );\n",
    "        \n",
    "        features_reduzidas = np.genfromtxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", delimiter=',')\n",
    "        \n",
    "        # Now change to base directory\n",
    "    \n",
    "        os.chdir( base_path );\n",
    "        \n",
    "        Output = {'ReducedFeatures': features_reduzidas,\n",
    "                  'ID': Output_Id}\n",
    "        \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Final working directory %s\" % retval)\n",
    "        \n",
    "        return Output\n",
    "        \n",
    "    elif DataName == 'SODA_parameters_processing_parameters':\n",
    "        \n",
    "        Output_Id = int(np.load('ID.npy'))\n",
    "        processing_parameters = np.genfromtxt('processing_parameters.csv',delimiter = ',')\n",
    "        distances = np.genfromtxt('Distances.csv',delimiter = ',')\n",
    "        min_granularity = np.genfromtxt('Min_g.csv',delimiter = ',')\n",
    "        max_granularity = np.genfromtxt('Max_g.csv',delimiter = ',')\n",
    "        pace = np.genfromtxt('Pace.csv',delimiter = ',')\n",
    "    \n",
    "        Output = {'Distances': distances,\n",
    "                  'Min_g': min_granularity,\n",
    "                  'Max_g': max_granularity,\n",
    "                  'Pace': pace,\n",
    "                  'ID': DataSetID}\n",
    "    \n",
    "        return Output, processing_parameters\n",
    "        \n",
    "    elif DataName == 'ClassificationPar':\n",
    "        \n",
    "        Output_Id = int(np.load('ID.npy'))\n",
    "        distances = np.genfromtxt('Distances.csv',delimiter = ',')\n",
    "        define_percent = np.genfromtxt('Percent.csv',delimiter = ',')\n",
    "    \n",
    "        Output = {'Percent': define_percent,\n",
    "                  'Distances': distances,\n",
    "                  'ID': DataSetID}\n",
    "        return Output\n",
    "        \n",
    "    else:\n",
    "        print(\"Wrong name lad/lass, please check de Recovery input\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Recovery Function\n",
    "\n",
    "D_S_parameters = Recovery('D_S_parameters');\n",
    "ExtractedNames = Recovery('ExtractedNames');\n",
    "SelectedFeatures = Recovery('SelectedFeatures');\n",
    "ReducedFeatures = Recovery('ReducedFeatures');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, x_min, x_max):\n",
    "    nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "    denom = X.max(axis=0) - X.min(axis=0)\n",
    "    if denom==0:\n",
    "        denom = 1\n",
    "    return x_min + nom/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Plot Formater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number):\n",
    "    # find number of multiples of pi/2\n",
    "    N = int(value)\n",
    "    if N == 0:\n",
    "        return \"X1\"\n",
    "    elif N == 50:\n",
    "        return \"X50\"\n",
    "    elif N == 100:\n",
    "        return \"X100\"\n",
    "    elif N == 150:\n",
    "        return \"X150\"\n",
    "    elif N == 200:\n",
    "        return \"X200\"\n",
    "    elif N == 250:\n",
    "        return \"X250\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-61BMUFZFQ6X"
   },
   "source": [
    "# Feature Extraction/Selection Module\n",
    "    .Data Slicer for saving RAM;\n",
    "    .TSFRESH feature extraction and selection;\n",
    "    .PCA dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# . Data Slicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCrplMylFQ6a"
   },
   "outputs": [],
   "source": [
    "def DataSlicer (Output_Id, id_per_group, Choice):\n",
    "    \n",
    "    print('Data Slicer Control Output');\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Input/\";\n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Input_path = base_path + add_path1;\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "     \n",
    "    # Now change to Input directory\n",
    "    \n",
    "    os.chdir( Input_path );\n",
    "    \n",
    "    # Loading the required input \n",
    "    \n",
    "    Full_data = np.genfromtxt('Output_' + str(int(Output_Id)) + '.csv', delimiter=',');\n",
    "    #E_data = np.genfromtxt('Eminence_Data_' + str(Output_Id) + '.csv', delimiter=',');\n",
    "    rows, columns = Full_data.shape\n",
    "    P_data = Full_data[:,0:columns-1];\n",
    "    Target = Full_data[:,columns-1];\n",
    "\n",
    "    print('Full Matrix: ' + str(Full_data.shape));\n",
    "    print('Main Data: ' + str(P_data.shape));\n",
    "    print('Labels: ' + str(Target.shape));\n",
    "    #print('Eminence Data: ' + str(E_data.shape));\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "          \n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                     ProDiMes Slicing Parameters                      ###\n",
    "\n",
    "\n",
    "    P_N_Ids = int(np.amax(P_data,axis=0)[0]);\n",
    "    P_N_voos = int(np.amax(P_data,axis=0)[1]);\n",
    "    P_last_group = int(P_N_Ids % id_per_group);\n",
    "\n",
    "    if P_last_group != 0:\n",
    "        P_N_groups = int((P_N_Ids / id_per_group) + 1);\n",
    "    else:\n",
    "        P_N_groups = int (P_N_Ids / id_per_group);\n",
    "\n",
    "    print ('Main data Number of Ids: ' + str(P_N_Ids ));\n",
    "    print ('Main data Number of mesures: ' + str(P_N_voos ));\n",
    "    print ('Main data Number of groups: ' + str(P_N_groups ));\n",
    "    print ('Main data Last group: ' + str(P_last_group ));\n",
    "    print ('___________________________________________')\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                    Eminences Slicing Parameters                      ###\n",
    "\n",
    "    #E_N_Ids = int(np.amax(E_data,axis=0)[0] - np.amax(P_data,axis=0)[0]);\n",
    "    #E_N_voos = int(np.amax(E_data,axis=0)[1]) + 1;\n",
    "    #E_last_group = int(E_N_Ids % id_per_group);\n",
    "\n",
    "    #if (E_last_group != 0):\n",
    "    #    E_N_groups = int((E_N_Ids / id_per_group) + 1);\n",
    "    #else:\n",
    "    #    E_N_groups = int (E_N_Ids / id_per_group);\n",
    "\n",
    "    #print ('Eminences Number of Ids: ' + str(E_N_Ids ));\n",
    "    #print ('Eminences Number of flights: ' + str(E_N_voos ));\n",
    "    #print ('Eminences Number of groups: ' + str(E_N_groups ));\n",
    "    #print ('Eminences Last group: ' + str(E_last_group ));\n",
    "\n",
    "\n",
    "    ### Formating Final Target ###\n",
    "\n",
    "    Final_Target = np.zeros((P_N_Ids));\n",
    "\n",
    "    for i in range (P_N_Ids):\n",
    "\n",
    "        Final_Target[i] = Target [i*P_N_voos];\n",
    "        \n",
    "    #np.savetxt(('Target_' + str(int(Output_Id)) + '.csv'), Final_Target, delimiter = ',');\n",
    "    \n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                      Slicing Prodimes Data                           ###\n",
    "\n",
    "    if (Choice =='Main Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (P_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * P_N_voos),columns-1));\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (P_N_voos):\n",
    "            \n",
    "                    if (i  < (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:];\n",
    "\n",
    "                    elif (P_last_group == 0) and (i == (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:];\n",
    "            \n",
    "            if (P_last_group != 0) and (i == (P_N_groups - 1)):     \n",
    "\n",
    "                Data = np.zeros(((P_last_group * P_N_voos),columns-1));\n",
    "            \n",
    "                for j in range (P_last_group):\n",
    "    \n",
    "                    for k in range (P_N_voos):\n",
    "    \n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:];\n",
    "        \n",
    "            np.savetxt(('Data_' + str(i) + '.csv'), Data, delimiter = ',');\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Slicing Eminences                           ###\n",
    "\n",
    "    if (Choice == 'Eminence Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (E_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * E_N_voos),columns-3));\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (E_N_voos):\n",
    "            \n",
    "                    if (i  < (E_N_groups - 1)):\n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:];\n",
    "                \n",
    "        \n",
    "            if (E_last_group != 0) and (i == (E_N_groups - 1)):\n",
    "            \n",
    "                Data = np.zeros(((E_last_group * E_N_voos),columns-3));\n",
    "            \n",
    "                for j in range (E_last_group):\n",
    "    \n",
    "                    for k in range (E_N_voos):\n",
    "    \n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:];\n",
    "    \n",
    "    \n",
    "            np.savetxt(('Eminence_' + str(i) + '.csv'), Data, delimiter = ',');\n",
    "    \n",
    "    \n",
    "    np.savetxt(('FinalTarget.csv'), Final_Target, delimiter = ',');\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "          \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    np.save(('M_N_groups.npy'), P_N_groups)\n",
    "    np.save(('ID.npy'), Output_Id);\n",
    "    np.save(('N_IDs.npy'), P_N_Ids);\n",
    "    \n",
    "    # Now change back to Base directory\n",
    "          \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    Output = {'FinalTarget': Final_Target,\n",
    "              'M_N_groups': P_N_groups,\n",
    "              'ID': Output_Id,\n",
    "              'N_IDs': P_N_Ids}\n",
    "    \n",
    "    return Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_Xgs4xlFQ6d"
   },
   "source": [
    "# .TSFRESH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Extraction(D_S_parameters):\n",
    "    \n",
    "    print('             ');\n",
    "    print('TSFRESH Control Output');\n",
    "    print('----------------------------------');\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "        \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                         Feature Extraction                          ###\n",
    "\n",
    "    #E_N_groups = np.load('E_N_groups.npy');\n",
    "    P_N_groups = D_S_parameters['M_N_groups'];\n",
    "    \n",
    "    for i in range(P_N_groups):\n",
    "        \n",
    "        Data = np.genfromtxt('Data_' + str(i) + '.csv', delimiter=',');\n",
    "        data = pd.DataFrame(Data, columns= ['id','time'] + ['Sensor_' + str(x) for x in range(1,(Data.shape[1]-1))]);\n",
    "        \n",
    "        Data_extracted_features = extract_features(data,column_id = \"id\", column_sort=\"time\");\n",
    "        extracted_names = list(Data_extracted_features.columns);\n",
    "        np.savetxt('Data_Features_' + str(i) + '.csv', Data_extracted_features, delimiter=',');\n",
    "        \n",
    "    #for i in range(E_N_groups):\n",
    "\n",
    "    \n",
    "    #    data = pd.DataFrame(np.genfromtxt('Eminence_' + str(i) + '.csv', delimiter=','), \n",
    "    #                        columns= ['id','time','sensor_1','sensor_2','sensor_3','sensor_4',\n",
    "    #                                            'sensor_5','sensor_6','sensor_7']);\n",
    "    #    extracted_features = extract_features(data, column_id = \"id\", column_sort=\"time\");\n",
    "    #    np.savetxt('Eminence_Features_' + str(i) + '.csv', extracted_features, delimiter=',');\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "    \n",
    "    os.chdir( Recovery_path );\n",
    "    \n",
    "    np.save('extracted_names.npy',extracted_names)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    return extracted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Selection(D_S_parameters,extracted_names):   \n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Feature Selection                           ###\n",
    "    \n",
    "    P_N_groups = D_S_parameters['M_N_groups'];\n",
    "    Output_Id = D_S_parameters['ID'];\n",
    "    y = D_S_parameters['FinalTarget'];\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "        \n",
    "    # Now change back to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    Data_Matrix = np.genfromtxt('Data_Features_0.csv', delimiter=',');\n",
    "    print('Data_Features_0.csv')\n",
    "    print(np.shape(Data_Matrix))\n",
    "\n",
    "    for i in range(1,P_N_groups):\n",
    "    \n",
    "        new_data = np.genfromtxt('Data_Features_' + str(i) + '.csv', delimiter=','); \n",
    "        \n",
    "        print('Data_Features_' + str(i) + '.csv')\n",
    "        print(np.shape(new_data))\n",
    "    \n",
    "        Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0);\n",
    "\n",
    "    \n",
    "    #for i in range(E_N_groups):\n",
    "    \n",
    "    #    new_data = np.genfromtxt('Eminence_Features_' + str(i) + '.csv', delimiter=','); \n",
    "    \n",
    "    #    Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0);\n",
    "    \n",
    "    #    print('Eminence_Features_' + str(i) + '.csv')\n",
    "    \n",
    "    features = pd.DataFrame(Data_Matrix, columns= extracted_names);\n",
    "    \n",
    "    impute(features)\n",
    "    features_filtered_1 = select_features(features, y)\n",
    "    features_filtered_1.sort_index(inplace = True)\n",
    "    \n",
    "    features_filtered_1.to_csv('features_filtered_' + str(Output_Id) + '.csv', index=False)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    Output = {'FeaturesFiltered': features_filtered_1,\n",
    "              'ID': Output_Id}\n",
    "    \n",
    "    return Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS1d7dxeFQ6j"
   },
   "source": [
    "# . PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####     Principal Components Calculation #####\n",
    "\n",
    "%matplotlib\n",
    "\n",
    "def PCA_calc (SelectedFeatures,N_PCs,Chose):\n",
    "    \n",
    "    if (Chose == 'Test') or (Chose == 'Calc') or (Chose == 'Specific') or (Chose == 'Analytics'):\n",
    "        \n",
    "        #Changing Work Folder\n",
    "        \n",
    "        add_path1 = \"/PCA_Analyses/\";\n",
    "        add_path2 = \"/Input/\";\n",
    "        add_path3 = \"/.Recovery/\";\n",
    "        add_path4 = \"/PCA_Analyses/Figures/\";        \n",
    "        base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "        PCA_Analyses_path = base_path + add_path1;\n",
    "        Input_path = base_path + add_path2;\n",
    "        Recovery_path = base_path + add_path3;\n",
    "        PCA_Figures_path = base_path + add_path4;\n",
    "\n",
    "        # Now change to PCA Figures directory\n",
    "\n",
    "        os.chdir( PCA_Figures_path );\n",
    "        \n",
    "        print('             ');\n",
    "        print('PCA Control Output');\n",
    "        print('----------------------------------');\n",
    "\n",
    "        Output_Id = SelectedFeatures['ID'];\n",
    "        features = SelectedFeatures['FeaturesFiltered'];\n",
    "        selected_names = list(features.columns);\n",
    "\n",
    "        #centralizar os dados e colocá-los com desvioPadrão=1\n",
    "        scaler = StandardScaler().fit(features);\n",
    "        features_padronizadas = scaler.transform(features);\n",
    "        #features_padronizadas = pd.DataFrame(features_padronizadas)\n",
    "\n",
    "        pca= PCA(n_components = N_PCs);\n",
    "        pca.fit(features_padronizadas);\n",
    "        variacao_percentual_pca = np.round(pca.explained_variance_ratio_ * 100, decimals = 2);\n",
    "\n",
    "        fig = plt.figure(figsize=[16,8])\n",
    "        ax = fig.subplots(1,1)\n",
    "        ax.bar(x=['PC' + str(x) for x in range(1,(N_PCs+1))],height=variacao_percentual_pca[0:N_PCs])\n",
    "\n",
    "        ax.set_ylabel('Percentage of Variance Held',fontsize=20)\n",
    "        ax.set_xlabel('Principal Components',fontsize=20)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=18)\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "        fig.savefig('Percentage_of_Variance_Held_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "        print('Variation maintained: %.2f' % variacao_percentual_pca.sum());\n",
    "        print('                  ');\n",
    "\n",
    "        if (Chose != 'Test'):\n",
    "            features_reduzidas = pca.transform(features_padronizadas)\n",
    "            print('Filtered Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_padronizadas,0))\n",
    "            print(np.size(features_padronizadas,1))\n",
    "            print('-' * 20)\n",
    "            print('Reduced Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_reduzidas,0))\n",
    "            print(np.size(features_reduzidas,1))\n",
    "\n",
    "            if (Chose != 'Test'):\n",
    "                ### Análise de atributos ###\n",
    "\n",
    "\n",
    "                eigen_matrix = np.array(pca.components_);\n",
    "                eigen_matrix = pow((pow(eigen_matrix,2)),0.5) #invertendo valores negativos\n",
    "\n",
    "                for i in range (eigen_matrix.shape[0]):\n",
    "\n",
    "                    LineSum = sum(eigen_matrix[i,:]);\n",
    "                    for j in range (eigen_matrix.shape[1]):\n",
    "                        eigen_matrix[i,j] = ((eigen_matrix[i,j]*100)/LineSum)\n",
    "\n",
    "\n",
    "                if Chose == 'Specific':\n",
    "                ### Análise Expecífica ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,int(8*N_PCs)])\n",
    "\n",
    "                    fig.suptitle('Contribution percentage per PC', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(int(N_PCs),1)\n",
    "\n",
    "                    for i in range (int(N_PCs)):\n",
    "\n",
    "                        s = eigen_matrix[i,:];\n",
    "\n",
    "                        ax[i].bar(x=range(0,(eigen_matrix.shape[1])),height=s)\n",
    "                        ax[i].set(xlabel='Features', ylabel='Contribution Percentage', title = 'PC ' + str(i+1))\n",
    "                        ax[i].grid()\n",
    "\n",
    "\n",
    "                    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "                    for axs in ax.flat:\n",
    "                        axs.label_outer()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Contribution_Percentage_Per_PC_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                if (Chose == 'Analytics'):\n",
    "                    ### Análise Geral ###\n",
    "\n",
    "                    weighted_contribution = np.zeros(eigen_matrix.shape[1]);\n",
    "\n",
    "                    for i in range (eigen_matrix.shape[1]):\n",
    "                        NumeratorSum = 0;\n",
    "                        for j in range (N_PCs):\n",
    "                            NumeratorSum += eigen_matrix[j,i] * variacao_percentual_pca[j];\n",
    "\n",
    "                        weighted_contribution[i] = NumeratorSum / sum(variacao_percentual_pca);\n",
    "\n",
    "                    named_weighted_contribution = np.vstack((selected_names,np.asarray(weighted_contribution,object)));\n",
    "                    named_weighted_contribution = named_weighted_contribution.transpose()\n",
    "\n",
    "                    df_weighted_contribution = pd.DataFrame(named_weighted_contribution,columns = ['Feature','Contribution'])\n",
    "                    #pd.set_option('display.max_rows', len(df_weighted_contribution))\n",
    "                    #print(df_weighted_contribution)\n",
    "                    #pd.reset_option('display.max_rows')\n",
    "\n",
    "                    #Creating Separated Data Frames por Sensors and Features Contribution \n",
    "\n",
    "                    sensors_names = [None] * int(df_weighted_contribution.shape[0]);\n",
    "                    features_names = [None] * int(df_weighted_contribution.shape[0]);\n",
    "\n",
    "\n",
    "                    for i in range (df_weighted_contribution.shape[0]):\n",
    "\n",
    "                        names = df_weighted_contribution.loc[i,'Feature']\n",
    "                        c = '_';\n",
    "                        words = names.split(c)\n",
    "                        sensors_names[i] = c.join(words[:2])\n",
    "                        features_names[i] = c.join(words[2:])\n",
    "\n",
    "                        #print(names)\n",
    "                        #print(words)\n",
    "                        #print(sensors_names[i])\n",
    "                        #print(features_names[i])\n",
    "                        #print(50*'-')\n",
    "\n",
    "\n",
    "                    unique_sensors_names = np.ndarray.tolist(np.unique(np.array(sensors_names))) \n",
    "                    unique_features_names = np.ndarray.tolist(np.unique(np.array(features_names))) \n",
    "                    sensor_dt = np.transpose(np.vstack((unique_sensors_names,np.asarray(np.zeros(np.shape(unique_sensors_names)[0]),object))));\n",
    "                    feature_dt = np.transpose(np.vstack((unique_features_names,np.asarray(np.zeros(np.shape(unique_features_names)[0]),object))));\n",
    "\n",
    "                    sensors_contribution = pd.DataFrame(sensor_dt,columns = ['Sensor','Contribution'])\n",
    "                    features_contribution = pd.DataFrame(feature_dt,columns = ['Feature','Contribution'])\n",
    "\n",
    "\n",
    "                    #Creating dictionaries form Data Frame orientation\n",
    "\n",
    "                    sensors_dic = {}\n",
    "                    for i in range(len(unique_sensors_names)):\n",
    "                        sensors_dic[unique_sensors_names[i]] = i\n",
    "\n",
    "                    features_dic = {}\n",
    "                    for i in range(len(unique_features_names)):\n",
    "                        features_dic[unique_features_names[i]] = i\n",
    "\n",
    "\n",
    "                    #Suming the contibution for Sensors and Features\n",
    "\n",
    "                    for i in range(df_weighted_contribution.shape[0]):\n",
    "\n",
    "                        names = df_weighted_contribution.loc[i,'Feature'];\n",
    "                        c = '_';\n",
    "                        words = names.split(c);           \n",
    "                        S= c.join(words[:2]);\n",
    "                        F= c.join(words[2:]);\n",
    "\n",
    "                        sensors_contribution.loc[sensors_dic[S],'Contribution'] += df_weighted_contribution.loc[i,'Contribution'];\n",
    "                        features_contribution.loc[features_dic[F],'Contribution'] += df_weighted_contribution.loc[i,'Contribution'];\n",
    "\n",
    "                    sensors_contribution = sensors_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "                    features_contribution = features_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "\n",
    "                    sensors_indexes = [x for x in range(1,(sensors_contribution.shape[0])+1)]\n",
    "                    features_indexes = [x for x in range(1,(features_contribution.shape[0])+1)]\n",
    "\n",
    "                    sensors_contribution.set_index(pd.Index(sensors_indexes))\n",
    "                    features_contribution.set_index(pd.Index(features_indexes))\n",
    "\n",
    "                    #Ploting Cntribution Sensors Results\n",
    "\n",
    "                    sorted_sensors_contribution = sensors_contribution.values[:,1];\n",
    "                    sorted_features_contribution = features_contribution.values[:,1];\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Sensors Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_sensors_contribution[:];\n",
    "\n",
    "                    ax.bar(x=sensors_contribution.values[:,0],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20);\n",
    "                    plt.xlabel('Sensors',fontsize = 20);\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Sensor_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                    #Ploting Cntribution Features Results\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[:];\n",
    "\n",
    "                    ax.bar(x=range(0,(sorted_features_contribution.shape[0])),height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20);\n",
    "                    plt.xlabel('Features',fontsize = 20);\n",
    "                    plt.tick_params(axis='x', labelsize=16);\n",
    "                    plt.tick_params(axis='y', labelsize=18);\n",
    "                    ax.xaxis.set_major_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_minor_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Features_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    ### Análise Geral para os 20 primeiros PC's ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Best Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    #print('Porcentagem de pertinência: ', np.sum(sorted_features_contribution[0:140]))\n",
    "                    #print('Number of Selected Features: ', sorted_features_contribution.shape[0])\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[0:20];\n",
    "\n",
    "                    ax.bar(x=['X' + str(x) for x in range(1,(20+1))],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20);\n",
    "                    plt.xlabel('Features',fontsize = 20);\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('{}th_Best_Features_Weighted_Contribution_Percentage_{}.png'.format(20,Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    #Ploting the data of the most relevant sensor with the best features\n",
    "\n",
    "                    sensors_contribution.values[:,0]\n",
    "\n",
    "                    name_1 = sensors_contribution.values[0,0] + '_' + features_contribution.values[0,0];\n",
    "                    name_2 = sensors_contribution.values[0,0] + '_' + features_contribution.values[1,0];\n",
    "                    name_3 = sensors_contribution.values[0,0] + '_' + features_contribution.values[2,0];\n",
    "\n",
    "\n",
    "                    #pd.set_option('display.max_columns', len(features))\n",
    "                    #print(features)\n",
    "                    #pd.reset_option('display.max_columns')\n",
    "\n",
    "                    x = features.loc[:,name_1].values\n",
    "                    y = features.loc[:,name_2].values\n",
    "                    z = features.loc[:,name_3].values\n",
    "\n",
    "                    x = scale(x,-1,1)\n",
    "                    y = scale(y,-1,1)\n",
    "                    z = scale(z,-1,1)\n",
    "                    \n",
    "                    # Now change to Input directory\n",
    "\n",
    "                    os.chdir( Input_path );\n",
    "                    \n",
    "                    Target = np.genfromtxt('Output_' + str(Output_Id) + '.csv', delimiter=',')\n",
    "                    \n",
    "                    # Now change to PCA Figures directory\n",
    "\n",
    "                    os.chdir( PCA_Figures_path );\n",
    "                    \n",
    "                    N_Mesures =int(np.amax(Target,axis=0)[1])\n",
    "                    Target = Target[::N_Mesures,4]\n",
    "\n",
    "                    x_bom=[]\n",
    "                    x_ruim=[]\n",
    "                    y_bom=[]\n",
    "                    y_ruim=[]\n",
    "                    z_bom=[]\n",
    "                    z_ruim=[]\n",
    "                    for i in range(len(Target)):\n",
    "                        if Target[i] == 0:\n",
    "                            x_bom.append(x[i])\n",
    "                            y_bom.append(y[i])\n",
    "                            z_bom.append(z[i])\n",
    "                        if Target[i] == 1:\n",
    "                            x_ruim.append(x[i])\n",
    "                            y_ruim.append(y[i])\n",
    "                            z_ruim.append(z[i])\n",
    "\n",
    "                    fig = plt.figure(figsize=[14,10])\n",
    "                    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "                    ax.scatter(x_bom, y_bom, z_bom, c = 'blue' )\n",
    "                    ax.scatter(x_ruim, y_ruim, z_ruim, c = 'red' )\n",
    "\n",
    "                    plt.ylabel('X2',fontsize = 20,labelpad=18);\n",
    "                    plt.xlabel('X1',fontsize = 20, labelpad=18);\n",
    "                    ax.set_zlabel('X3', fontsize = 20, labelpad=12)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=16)\n",
    "                    plt.tick_params(axis='z', labelsize=16)\n",
    "                    ax.grid()\n",
    "                    red_patch = mpatches.Patch(color='red', label='Non-Funcional Tools')\n",
    "                    blue_patch = mpatches.Patch(color='blue', label='Funcional Tools')\n",
    "                    plt.legend(handles=[red_patch,blue_patch],fontsize = 20)\n",
    "                    plt.show()\n",
    "                    fig.savefig('ScatterPlot_PCA_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    default_features = np.array(['abs_energy','absolute_sum_of_changes','agg_autocorrelation','agg_linear_trend','approximate_entropy',\n",
    "                 'ar_coefficient','augmented_dickey_fuller','autocorrelation','benford_correlation','binned_entropy','c3',\n",
    "                 'change_quantiles','cid_ce','count_above','count_above_mean','count_below','count_below_mean',\n",
    "                 'cwt_coefficients','energy_ratio_by_chunks','fft_aggregated','fft_coefficient','first_location_of_maximum',\n",
    "                 'first_location_of_minimum','fourier_entropy','friedrich_coefficients','has_duplicate', 'has_duplicate_max',\n",
    "                 'has_duplicate_min','index_mass_quantile','kurtosis','large_standard_deviation','last_location_of_maximum',\n",
    "                 'last_location_of_minimum','lempel_ziv_complexity','length','linear_trend','linear_trend_timewise',\n",
    "                 'longest_strike_above_mean','longest_strike_below_mean','max_langevin_fixed_point','maximum','mean',\n",
    "                 'mean_abs_change','mean_change','mean_second_derivative_central','median','minimum','number_crossing_m',\n",
    "                 'number_cwt_peaks','number_peaks','partial_autocorrelation',\n",
    "                 'percentage_of_reoccurring_datapoints_to_all_datapoints','percentage_of_reoccurring_values_to_all_values',\n",
    "                 'permutation_entropy','quantile','range_count','ratio_beyond_r_sigma',\n",
    "                 'ratio_value_number_to_time_series_length','sample_entropy','set_property','skewness','spkt_welch_density',\n",
    "                 'standard_deviation','sum_of_reoccurring_data_points','sum_of_reoccurring_values','sum_values',\n",
    "                 'symmetry_looking','time_reversal_asymmetry_statistic','value_count','variance',\n",
    "                 'variance_larger_than_standard_deviation','variation_coefficient']);\n",
    "                    default_features_comp = [None] * int(default_features.shape[0]);\n",
    "                    unique_features = [None] * int(np.size(unique_features_names));\n",
    "                    k=0;\n",
    "\n",
    "                    for i in range (np.size(default_features)):\n",
    "\n",
    "                        c = '_';\n",
    "                        default_features_words = default_features[i].split(c);          \n",
    "                        default_features_comp[i] = default_features_words;\n",
    "                        size = int(np.shape(default_features_comp[i])[0]);\n",
    "\n",
    "                        for j in range (np.size(unique_features_names)):\n",
    "\n",
    "                            unique_features_words = unique_features_names[j].split(c);\n",
    "\n",
    "                            unique_features[j] = unique_features_words[:size];\n",
    "\n",
    "                            if default_features_comp[i] == unique_features[j]:\n",
    "                                k +=1;\n",
    "\n",
    "                    features_used = [None] * k;\n",
    "\n",
    "                    k=0;\n",
    "\n",
    "                    for i in range (np.size(default_features)):\n",
    "\n",
    "                        c = '_';\n",
    "                        default_features_words = default_features[i].split(c);          \n",
    "                        default_features_comp[i] = default_features_words;\n",
    "                        size = int(np.shape(default_features_comp[i])[0]);\n",
    "\n",
    "                        for j in range (np.size(unique_features_names)):\n",
    "\n",
    "                            unique_features_words = unique_features_names[j].split(c);\n",
    "\n",
    "                            unique_features[j] = unique_features_words[:size];\n",
    "\n",
    "                            if default_features_comp[i] == unique_features[j]:\n",
    "\n",
    "                                features_used[k] = c.join(default_features_comp[i]);\n",
    "                                k +=1;\n",
    "\n",
    "                    unique_features_used = np.ndarray.tolist(np.unique(np.array(features_used))) \n",
    "                    unique_features_used = pd.DataFrame(unique_features_used);\n",
    "                    \n",
    "                    # Now change to PCA Analyses directory\n",
    "\n",
    "                    os.chdir( PCA_Analyses_path );\n",
    "\n",
    "                    unique_features_used.to_csv('unique_features_used_{}.csv'.format(Output_Id),index = False)\n",
    "\n",
    "                    sensors_contribution.to_csv('sensors_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "                    features_contribution.to_csv('features_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "\n",
    "            # Now change to PCA Analyses directory\n",
    "\n",
    "            os.chdir( PCA_Analyses_path );\n",
    "            \n",
    "            np.savetxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", features_reduzidas, delimiter=',')\n",
    "\n",
    "            Output = {'ReducedFeatures': features_reduzidas,\n",
    "                      'ID': Output_Id} \n",
    "        elif (Chose == 'Test'): \n",
    "\n",
    "            Output = {'ID': Output_Id}\n",
    "        \n",
    "        # Now change back to base directory\n",
    "\n",
    "        os.chdir( base_path );\n",
    "\n",
    "        return Output;\n",
    "    \n",
    "    else:\n",
    "        print(\"Wrong Choose entry, verify this input.\")\n",
    "        return;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEeXLskLFQ6q"
   },
   "source": [
    "# Data Partitioning Module\n",
    "    .SODA\n",
    "    .Grouping Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAunBilsFQ6s"
   },
   "source": [
    "# . SODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-ol3Jpe1bO"
   },
   "outputs": [],
   "source": [
    "### Thread to calculate duration and mean cpu percente usage in a SODA classifier\n",
    "class cpu_usage(threading.Thread):\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.control = True\n",
    "        \n",
    "    def run(self):\n",
    "        cpu = []\n",
    "        t_inicial = time.time()\n",
    "        while self.control:\n",
    "            cpu.append(cpu_percent(interval=1, percpu=True))\n",
    "        t_final = time.time()\n",
    "        self.deltatime = t_final - t_inicial\n",
    "        self.mean_cpu = np.mean(cpu)\n",
    "        \n",
    "    def stop(self):\n",
    "        self.control = False\n",
    "        \n",
    "    def join(self):\n",
    "        threading.Thread.join(self)\n",
    "        return self.deltatime, self.mean_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep1uNgbAFQ6t"
   },
   "outputs": [],
   "source": [
    "def grid_set(data, N):\n",
    "    _ , W = data.shape\n",
    "    AvD1 = data.mean(0)\n",
    "    X1 = np.mean(np.sum(np.power(data,2),axis=1))\n",
    "    grid_trad = np.sqrt(2*(X1 - AvD1*AvD1.T))/N\n",
    "    Xnorm = np.sqrt(np.sum(np.power(data,2),axis=1))\n",
    "    aux = Xnorm\n",
    "    for i in range(W-1):\n",
    "        aux = np.insert(aux,0,Xnorm.T,axis=1)\n",
    "    data = data / aux\n",
    "    seq = np.argwhere(np.isnan(data))\n",
    "    if tuple(seq[::]): data[tuple(seq[::])] = 1\n",
    "    AvD2 = data.mean(0)\n",
    "    grid_angl = np.sqrt(1-AvD2*AvD2.T)/N\n",
    "    return X1, AvD1, AvD2, grid_trad, grid_angl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHAa-LjmFQ6y"
   },
   "outputs": [],
   "source": [
    "def pi_calculator(Uniquesample, mode):\n",
    "    UN, W = Uniquesample.shape\n",
    "    if mode == 'euclidean' or mode == 'mahalanobis' or mode == 'cityblock' or mode == 'chebyshev' or mode == 'canberra':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA1)\n",
    "        #aux2 = [Uniquesample[i]-aux[i] for i in range(UN)]\n",
    "        #uspi = np.sum(np.power(aux2,2),axis=1)+DT1\n",
    "        uspi = np.power(cdist(Uniquesample, aux, mode),2)+DT1\n",
    "        uspi = uspi[:,0]\n",
    "    \n",
    "    if mode == 'minkowski':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = np.matrix(AA1)\n",
    "        for i in range(UN-1): aux = np.insert(aux,0,AA1,axis=0)\n",
    "        aux = np.array(aux)\n",
    "        uspi = np.sum(np.power(cdist(Uniquesample, aux, mode, p=1.5),2),1)+DT1\n",
    "    \n",
    "    if mode == 'cosine':\n",
    "        Xnorm = np.matrix(np.sqrt(np.sum(np.power(Uniquesample,2),axis=1))).T\n",
    "        aux2 = Xnorm\n",
    "        for i in range(W-1):\n",
    "            aux2 = np.insert(aux2,0,Xnorm.T,axis=1)\n",
    "        Uniquesample1 = Uniquesample / aux2\n",
    "        AA2 = np.mean(Uniquesample1,0)\n",
    "        X2 = 1\n",
    "        DT2 = X2 - np.sum(np.power(AA2,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA2)\n",
    "        aux2 = [Uniquesample1[i]-aux[i] for i in range(UN)]\n",
    "        uspi = np.sum(np.sum(np.power(aux2,2),axis=1),axis=1)+DT2\n",
    "        \n",
    "    return uspi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU3o7WCFQ64"
   },
   "outputs": [],
   "source": [
    "def Globaldensity_Calculator(data, distancetype):\n",
    "    Uniquesample, J, K = np.unique(data, axis=0, return_index=True, return_inverse=True)\n",
    "    Frequency, _ = np.histogram(K,bins=len(J))\n",
    "    uspi1 = pi_calculator(Uniquesample, distancetype)\n",
    "    sum_uspi1 = sum(uspi1)\n",
    "    Density_1 = uspi1 / sum_uspi1\n",
    "    uspi2 = pi_calculator(Uniquesample, 'cosine')\n",
    "    sum_uspi2 = sum(uspi2)\n",
    "    Density_2 = uspi1 / sum_uspi2\n",
    "    GD = (Density_2+Density_1) * Frequency\n",
    "    index = GD.argsort()[::-1]\n",
    "    GD = GD[index]\n",
    "    Uniquesample = Uniquesample[index]\n",
    "    Frequency = Frequency[index]\n",
    "    return GD, Uniquesample, Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCOT-mm5FQ68"
   },
   "outputs": [],
   "source": [
    "def chessboard_division(Uniquesample, MMtypicality, interval1, interval2, distancetype):\n",
    "    L, W = Uniquesample.shape\n",
    "    if distancetype == 'euclidean':\n",
    "        W = 1\n",
    "    BOX = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_miu = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_S = [1]*W\n",
    "    BOX_X = [sum(Uniquesample[k]**2) for k in range(W)]\n",
    "    NB = W\n",
    "    BOXMT = [MMtypicality[k] for k in range(W)]\n",
    "    \n",
    "    for i in range(W,L):\n",
    "        if distancetype == 'minkowski':\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype, p=1.5)\n",
    "        else:\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype)\n",
    "        \n",
    "        b = np.sqrt(cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric='cosine'))\n",
    "        distance = np.array([a[0],b[0]]).T\n",
    "        SQ = []\n",
    "        for j,d in enumerate(distance):\n",
    "            if d[0] < interval1 and d[1] < interval2:\n",
    "                SQ.append(j)\n",
    "        #SQ = np.argwhere(distance[::,0]<interval1 and (distance[::,1]<interval2))\n",
    "        COUNT = len(SQ)\n",
    "        if COUNT == 0:\n",
    "            BOX.append(Uniquesample[i])\n",
    "            NB = NB + 1\n",
    "            BOX_S.append(1)\n",
    "            BOX_miu.append(Uniquesample[i])\n",
    "            BOX_X.append(sum(Uniquesample[i]**2))\n",
    "            BOXMT.append(MMtypicality[i])\n",
    "        if COUNT >= 1:\n",
    "            DIS = distance[SQ[::],0]/interval1 + distance[SQ[::],1]/interval2\n",
    "            b = np.argmin(DIS)\n",
    "            BOX_S[SQ[b]] = BOX_S[SQ[b]] + 1\n",
    "            BOX_miu[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_miu[SQ[b]] + Uniquesample[i]/BOX_S[SQ[b]]\n",
    "            BOX_X[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_X[SQ[b]] + sum(Uniquesample[i]**2)/BOX_S[SQ[b]]\n",
    "            BOXMT[SQ[b]] = BOXMT[SQ[b]] + MMtypicality[i]\n",
    "    return BOX, BOX_miu, BOX_X, BOX_S, BOXMT, NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2TZty8dFQ6_"
   },
   "outputs": [],
   "source": [
    "def ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,Internval1,Internval2, distancetype):\n",
    "    Centers = []\n",
    "    n = 2\n",
    "    ModeNumber = 0\n",
    "    \n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype, p=1.5))\n",
    "    else:\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype))        \n",
    "    \n",
    "    distance2 = np.sqrt(squareform(pdist(BOX_miu,metric='cosine')))\n",
    "    for i in range(NB):\n",
    "        seq = []\n",
    "        for j,(d1,d2) in enumerate(zip(distance1[i],distance2[i])):\n",
    "            if d1 < n*Internval1 and d2 < n*Internval2:\n",
    "                seq.append(j)\n",
    "        Chessblocak_typicality = [BOXMT[j] for j in seq]\n",
    "        if max(Chessblocak_typicality) == BOXMT[i]:\n",
    "            Centers.append(BOX_miu[i])\n",
    "            ModeNumber = ModeNumber + 1\n",
    "    return Centers, ModeNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gNgPN7dFQ7D"
   },
   "outputs": [],
   "source": [
    "def cloud_member_recruitment(ModelNumber,Center_samples,Uniquesample,grid_trad,grid_angl, distancetype):\n",
    "    L, W = Uniquesample.shape\n",
    "    Membership = np.zeros((L,ModelNumber))\n",
    "    Members = np.zeros((L,ModelNumber*W))\n",
    "    Count = []\n",
    "\n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype, p=1.5)/grid_trad\n",
    "    else:\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype)/grid_trad\n",
    "\n",
    "    distance2 = np.sqrt(cdist(Uniquesample, Center_samples, metric='cosine'))/grid_angl\n",
    "    distance3 = distance1 + distance2\n",
    "    B = distance3.argmin(1);\n",
    "    for i in range(ModelNumber):\n",
    "        seq = []\n",
    "        for j,b in enumerate(B):\n",
    "            if b == i:\n",
    "                seq.append(j)\n",
    "        Count.append(len(seq))\n",
    "        Membership[:Count[i]:,i] = seq\n",
    "        Members[:Count[i]:,W*i:W*(i+1)] = [Uniquesample[j] for j in seq]\n",
    "    MemberNumber = Count\n",
    "    return Members,MemberNumber,Membership,B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzmeIoLIFQ7I"
   },
   "outputs": [],
   "source": [
    "def SelfOrganisedDirectionAwareDataPartitioning(Input, Mode):\n",
    "    if Mode == 'Offline':\n",
    "        data = Input['StaticData']\n",
    "        L, W = data.shape\n",
    "        N = Input['GridSize']\n",
    "        distancetype = Input['DistanceType']\n",
    "        X1, AvD1, AvD2, grid_trad, grid_angl = grid_set(data,N)\n",
    "        GD, Uniquesample, Frequency = Globaldensity_Calculator(data, distancetype)\n",
    "        BOX,BOX_miu,BOX_X,BOX_S,BOXMT,NB = chessboard_division(Uniquesample,GD,grid_trad,grid_angl, distancetype)\n",
    "        Center,ModeNumber = ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,grid_trad,grid_angl, distancetype)\n",
    "        Members,Membernumber,Membership,IDX = cloud_member_recruitment(ModeNumber,Center,data,grid_trad,grid_angl, distancetype)\n",
    "        \n",
    "        Boxparameter = {'BOX': BOX,\n",
    "                'BOX_miu': BOX_miu,\n",
    "                'BOX_S': BOX_S,\n",
    "                'NB': NB,\n",
    "                'XM': X1,\n",
    "                'L': L,\n",
    "                'AvM': AvD1,\n",
    "                'AvA': AvD2,\n",
    "                'GridSize': N}\n",
    "        \n",
    "    if Mode == 'Evolving':\n",
    "        print(Mode)\n",
    "    \n",
    "    Output = {'C': Center,\n",
    "              'IDX': IDX,\n",
    "              'SystemParams': Boxparameter,\n",
    "              'DistanceType': distancetype}\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tayjmyeOFQ7N"
   },
   "source": [
    "#### Distâncias ####\n",
    " \n",
    " - euclidean - linha reta entre os pontos\n",
    " - mahalanobis - correlação entre as variaveis (determina similaridade)\n",
    " - cityblock - distancia das projeções dos pontos (taxicab/manhattan)\n",
    " - chebyshev - maior distancia entre as coordenadas (rei)\n",
    " - minkowski - generalização de outras distâncias:\n",
    "  - p = 1 $\\rightarrow$ cityblock,\n",
    "  - p = 2 $\\rightarrow$ euclidean,\n",
    "  - p = $\\infty$ $\\rightarrow$ chebyshev.\n",
    " - canberra - versão com pesos da cityblock, sensivel para pontos proximos à origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PE6QHo8bFQ7O"
   },
   "outputs": [],
   "source": [
    "def SODA (ReducedFeatures, min_granularity, max_granularity, pace):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "    \n",
    "    DataSetID = ReducedFeatures['ID'];\n",
    "    data = ReducedFeatures['ReducedFeatures'];\n",
    "    data = np.matrix(data);\n",
    "\n",
    "    distances = ['euclidean', 'mahalanobis', 'cityblock', 'chebyshev', 'minkowski', 'canberra'];\n",
    "    processing_parameters = []\n",
    "    \n",
    "    #### Looping SODA within the chosen granularities and distances ####\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "\n",
    "        for d in distances:\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "            \n",
    "            Input = {'GridSize':g, 'StaticData':data, 'DistanceType': d}\n",
    "            \n",
    "            out = SelfOrganisedDirectionAwareDataPartitioning(Input,'Offline')\n",
    "            \n",
    "            ### Interrupt Thread and Calculate Parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            pp = {'DistanceType': d,\n",
    "                  'Granularity': g,\n",
    "                  'Time': deltatime,\n",
    "                  'CPUPercent': mean_cpu}\n",
    "            processing_parameters.append(pp)\n",
    "\n",
    "            \n",
    "            np.savetxt('SODA_' + d + '_label_' + str (DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', out['IDX'],delimiter=',')\n",
    "            \n",
    "    #np.savetxt('processing_parameters.csv',processing_parameters,delimiter = ',')\n",
    "    #np.savetxt('Distances.csv',distances,delimiter = ',')\n",
    "    #np.savetxt('Min_g.csv',min_granularity,delimiter = ',')\n",
    "    #np.savetxt('Max_g.csv',max_granularity,delimiter = ',')\n",
    "    #np.savetxt('Pace.csv',pace,delimiter = ',')\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "    \n",
    "    Output = {'Distances': distances,\n",
    "              'Min_g': min_granularity,\n",
    "              'Max_g': max_granularity,\n",
    "              'Pace': pace,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    return Output, processing_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Final working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model\n"
     ]
    }
   ],
   "source": [
    "SODA_parameters, processing_parameters = SODA(ReducedFeatures,2,4,0.25) # (Features reduzidas, granularidade mínima,\n",
    "                                                 # granularidade máxima, passo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ijABlt6FQ7R"
   },
   "source": [
    "# . Grouping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlPWb0RZFQ7g"
   },
   "outputs": [],
   "source": [
    "def GroupingAlgorithm (SODA_parameters,define_percent,n_IDs_gp0, processing_parameters):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/PCA_Analyses/\";\n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    PCA_Analyses_path = base_path + add_path1;\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "    \n",
    "    print('             ');\n",
    "    print('Grouping Algorithm Control Output');\n",
    "    print('----------------------------------');\n",
    "    \n",
    "    \n",
    "    ####   imput data    ####\n",
    "    DataSetID = SODA_parameters['ID'];\n",
    "    min_granularity = SODA_parameters['Min_g'];\n",
    "    max_granularity = SODA_parameters['Max_g'];\n",
    "    pace = SODA_parameters['Pace'];\n",
    "    distances = SODA_parameters['Distances'];\n",
    "    # functional engines\n",
    "    #n_IDs_gp1 = 0; # non-functional engines\n",
    "    #n_IDs_gp2 = 3598; # eminent  fault  engines\n",
    "\n",
    "    for d in distances:\n",
    "        \n",
    "        for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "    \n",
    "            s = 'SODA_' + d + '_label_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv';\n",
    "            \n",
    "\n",
    "            #### Data-base Imput ####\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path );\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            SodaOutput = np.genfromtxt( s , delimiter=',');\n",
    "            \n",
    "            # Now change to PCA Analyses directory\n",
    "    \n",
    "            os.chdir( PCA_Analyses_path );\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            SelectedFeatures = np.genfromtxt('features_reduzidas_' + str(DataSetID) + '.csv' , delimiter=',');\n",
    "\n",
    "            #### Program Matrix's and Variables ####\n",
    "\n",
    "            n_DA_planes = np.max(SodaOutput) + 1;\n",
    "            Percent = np.zeros((int(n_DA_planes),3));\n",
    "            n_IDs_per_gp = np.zeros((int(n_DA_planes),2));\n",
    "            n_tot_Id_per_DA = np.zeros((int(n_DA_planes),1));\n",
    "            decision = np.zeros(int(n_DA_planes));\n",
    "            n_DA_excluded = 0;\n",
    "            n_excluded = 0;\n",
    "            n_gp0 = 0;\n",
    "            n_gp1 = 0;\n",
    "            n_gp2 = 0;\n",
    "            n_data_def = 0;\n",
    "            k = 0;\n",
    "\n",
    "            #### Definition Percentage Calculation #####\n",
    "\n",
    "            for i in range(SodaOutput.shape[0]):\n",
    "    \n",
    "                if i < n_IDs_gp0:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]),0] += 1 ;\n",
    "                else:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]),1] += 1 ;\n",
    "\n",
    "                n_tot_Id_per_DA [int(SodaOutput[i])] += 1 ;\n",
    "\n",
    "\n",
    "            for i in range(int(n_DA_planes)):\n",
    "    \n",
    "                Percent[i,0] = (n_IDs_per_gp[i,0] / n_tot_Id_per_DA[i]) * 100;\n",
    "                Percent[i,1] = (n_IDs_per_gp[i,1] / n_tot_Id_per_DA[i]) * 100;\n",
    "                Percent[i,2] = ((n_tot_Id_per_DA[i]  -  (n_IDs_per_gp[i,0] + n_IDs_per_gp[i,1])) \n",
    "                    / n_tot_Id_per_DA[i]) * 100;\n",
    "    \n",
    "            #### Using Definition Percentage as Decision Parameter ####\n",
    "\n",
    "            for i in range(Percent.shape[0]):\n",
    "    \n",
    "                if (Percent[i,0] >= define_percent):\n",
    "                    n_gp0 = n_gp0 + 1 ;        \n",
    "                elif (Percent[i,1] >= define_percent):    \n",
    "                    n_gp1 = n_gp1 + 1 ;\n",
    "                    decision[i] = 1;\n",
    "                elif (Percent[i,2] >= define_percent):\n",
    "                    n_gp2 = n_gp2 + 1 ;\n",
    "                    decision[i] = 2;\n",
    "                else:\n",
    "                    n_DA_excluded += 1;\n",
    "                    decision[i] = -1;\n",
    "            \n",
    "            #### Using decision matrix to determine the number of excluded data\n",
    "                       \n",
    "            for i in range (len (decision)):\n",
    "\n",
    "                if decision[i] == -1:\n",
    "                    \n",
    "                    n_excluded += np.sum(n_IDs_per_gp[i,:]);\n",
    "                    \n",
    "        \n",
    "            #### Passing data of well defined DA planes to SelectedData and defining labels\n",
    "\n",
    "            SelectedData = np.zeros((int(SelectedFeatures.shape[0] - n_excluded),int(SelectedFeatures.shape[1])));\n",
    "            ClassifiersLabel = np.zeros((int(SelectedFeatures.shape[0] - n_excluded)));\n",
    "            \n",
    "            \n",
    "            for i in range (SodaOutput.shape[0]):\n",
    "                if decision[int (SodaOutput[i]-1)] != -1:\n",
    "    \n",
    "                    SelectedData[k] = SelectedFeatures[i];\n",
    "                    ClassifiersLabel [k] = decision[int (SodaOutput[i]-1)];\n",
    "                \n",
    "                    if k < int(SelectedFeatures.shape[0] - n_excluded - 1):\n",
    "                        k += 1;\n",
    "\n",
    "            #### Saving Processed Data, ID's and Percentage\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path );\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            np.savetxt('X_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', SelectedData, delimiter=',');\n",
    "            np.savetxt('Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', ClassifiersLabel, delimiter=',');\n",
    "    \n",
    "            ### Interrupt Thread and recalculate parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            for pp in processing_parameters:\n",
    "                if pp['DistanceType'] == d and pp['Granularity'] == g:\n",
    "                    aux = pp\n",
    "                    break\n",
    "            totaltime = deltatime + aux['Time']\n",
    "            cpu_percent = (mean_cpu + aux['CPUPercent'])/2\n",
    "            \n",
    "            \n",
    "            ### Printig Analitics results\n",
    "        \n",
    "            print(s);\n",
    "            print('Numero de DA planes: %d' % n_DA_planes)\n",
    "            print('Numero de grupos de dados bons: %d' % n_gp0)\n",
    "            print('Numero de grupos de dados ruins: %d' % n_gp1)\n",
    "            print('Numero de DA planes excluido: %d' % n_DA_excluded)\n",
    "            print('Perda de representação dos dados: %.2f' % (100-((SelectedData.shape[0] / SelectedFeatures.shape[0]) * 100)))\n",
    "            print('Tempo de execução: %.6f segundos' % totaltime)\n",
    "            print('Media de porcentagem de uso da CPU: %.2f' % cpu_percent)\n",
    "            print('---------------------------------------------------');\n",
    "    \n",
    "    #np.savetxt('Percent.csv',define_percent,delimiter = ',')\n",
    "    \n",
    "    Output = {'Percent': define_percent,\n",
    "              'Distances': distances,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             \n",
      "Grouping Algorithm Control Output\n",
      "----------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_2.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.015822 segundos\n",
      "Media de porcentagem de uso da CPU: 30.57\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.029791 segundos\n",
      "Media de porcentagem de uso da CPU: 23.95\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.014633 segundos\n",
      "Media de porcentagem de uso da CPU: 27.08\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.013841 segundos\n",
      "Media de porcentagem de uso da CPU: 27.00\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.015771 segundos\n",
      "Media de porcentagem de uso da CPU: 24.35\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.026191 segundos\n",
      "Media de porcentagem de uso da CPU: 30.11\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010846 segundos\n",
      "Media de porcentagem de uso da CPU: 29.40\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_euclidean_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.004523 segundos\n",
      "Media de porcentagem de uso da CPU: 23.70\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_2.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.011755 segundos\n",
      "Media de porcentagem de uso da CPU: 22.82\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010073 segundos\n",
      "Media de porcentagem de uso da CPU: 32.14\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.013668 segundos\n",
      "Media de porcentagem de uso da CPU: 32.12\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.015112 segundos\n",
      "Media de porcentagem de uso da CPU: 39.64\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.006894 segundos\n",
      "Media de porcentagem de uso da CPU: 24.93\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010292 segundos\n",
      "Media de porcentagem de uso da CPU: 27.02\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.008191 segundos\n",
      "Media de porcentagem de uso da CPU: 33.46\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_mahalanobis_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.004949 segundos\n",
      "Media de porcentagem de uso da CPU: 21.46\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_2.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.014064 segundos\n",
      "Media de porcentagem de uso da CPU: 21.41\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.013545 segundos\n",
      "Media de porcentagem de uso da CPU: 42.51\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.025708 segundos\n",
      "Media de porcentagem de uso da CPU: 31.65\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.033171 segundos\n",
      "Media de porcentagem de uso da CPU: 28.54\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.019626 segundos\n",
      "Media de porcentagem de uso da CPU: 28.66\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.012128 segundos\n",
      "Media de porcentagem de uso da CPU: 23.76\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010582 segundos\n",
      "Media de porcentagem de uso da CPU: 31.16\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_cityblock_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010073 segundos\n",
      "Media de porcentagem de uso da CPU: 21.17\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_2.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.011103 segundos\n",
      "Media de porcentagem de uso da CPU: 26.24\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010789 segundos\n",
      "Media de porcentagem de uso da CPU: 30.81\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010960 segundos\n",
      "Media de porcentagem de uso da CPU: 24.11\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.019417 segundos\n",
      "Media de porcentagem de uso da CPU: 33.95\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.005956 segundos\n",
      "Media de porcentagem de uso da CPU: 25.49\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.011632 segundos\n",
      "Media de porcentagem de uso da CPU: 33.24\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.008664 segundos\n",
      "Media de porcentagem de uso da CPU: 31.21\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_chebyshev_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.014889 segundos\n",
      "Media de porcentagem de uso da CPU: 19.24\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_2.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.016944 segundos\n",
      "Media de porcentagem de uso da CPU: 25.64\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.017972 segundos\n",
      "Media de porcentagem de uso da CPU: 24.42\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.024537 segundos\n",
      "Media de porcentagem de uso da CPU: 26.20\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010983 segundos\n",
      "Media de porcentagem de uso da CPU: 18.91\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.013494 segundos\n",
      "Media de porcentagem de uso da CPU: 23.41\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.019872 segundos\n",
      "Media de porcentagem de uso da CPU: 37.99\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.013298 segundos\n",
      "Media de porcentagem de uso da CPU: 38.64\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_minkowski_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.010525 segundos\n",
      "Media de porcentagem de uso da CPU: 15.01\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_2.00.csv\n",
      "Numero de DA planes: 3\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 1\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.019336 segundos\n",
      "Media de porcentagem de uso da CPU: 33.62\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_2.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.011162 segundos\n",
      "Media de porcentagem de uso da CPU: 22.05\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_2.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.011665 segundos\n",
      "Media de porcentagem de uso da CPU: 17.81\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_2.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.012997 segundos\n",
      "Media de porcentagem de uso da CPU: 26.01\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_3.00.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.008698 segundos\n",
      "Media de porcentagem de uso da CPU: 29.41\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_3.25.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.014511 segundos\n",
      "Media de porcentagem de uso da CPU: 34.23\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_3.50.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.007786 segundos\n",
      "Media de porcentagem de uso da CPU: 18.70\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/PCA_Analyses\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model/.Kernel\n",
      "SODA_canberra_label_4_3.75.csv\n",
      "Numero de DA planes: 4\n",
      "Numero de grupos de dados bons: 2\n",
      "Numero de grupos de dados ruins: 2\n",
      "Numero de DA planes excluido: 0\n",
      "Perda de representação dos dados: 0.00\n",
      "Tempo de execução: 2.022995 segundos\n",
      "Media de porcentagem de uso da CPU: 23.39\n",
      "---------------------------------------------------\n",
      "Current working directory /home/thiago/Projetos/Academicos/CNPQ_usinagem/Python/Autonomous_Model\n"
     ]
    }
   ],
   "source": [
    "ClassificationPar = GroupingAlgorithm(SODA_parameters,60,20, processing_parameters) # (Labels do SODA, Porcentagem de definição,\n",
    "                                                             # numero de ID's boas, parametros de processamento)\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "882l7VYuFQ7s"
   },
   "source": [
    "# Classification Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNNEQZlzrRuC"
   },
   "source": [
    "# .Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR3VB7RRFQ7t"
   },
   "outputs": [],
   "source": [
    "def Classification (ClassificationPar,n_a,g):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Grouping_Analyses/\";\n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Grouping_Analyses_path = base_path + add_path1;\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma='scale'),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    for d in ClassificationPar['Distances']:\n",
    "        \n",
    "        # Now change to Kernel directory\n",
    "    \n",
    "        os.chdir( Kernel_path );\n",
    "    \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Current working directory %s\" % retval)\n",
    "        \n",
    "        # preprocess dataset, split into training and test part\n",
    "        Accuracy = np.zeros((n_a, len(names)))\n",
    "        #\"Y_60_euclidean_Labels_7_1.25.csv\"\n",
    "        s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.csv';\n",
    "        X = np.genfromtxt(('X_' + s) , delimiter=',')    \n",
    "        y = np.genfromtxt(('Y_' + s), delimiter=',') \n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "        #Loop into numbeer od samples\n",
    "        for i in range(Accuracy.shape[0]):\n",
    "            k = 0\n",
    "            # iterate over classifiers\n",
    "            for name, clf in zip(names, classifiers):\n",
    "        \n",
    "                clf.fit(X_train, y_train)\n",
    "                score = clf.score(X_test, y_test)\n",
    "                Accuracy[i,k] = (score*100)\n",
    "                k +=1\n",
    "        \n",
    "        #Creating Matrix for Mean an Std. Derivatio\n",
    "        results = np.zeros((len(names),2))\n",
    "\n",
    "        #Calculinng Mean and Std. Derivation \n",
    "        for i in range(len(names)):\n",
    "            results[i,0] = round (np.mean(Accuracy[:,i]), 2 )\n",
    "            results[i,1] = round (np.std(Accuracy[:,i]), 2)\n",
    "            \n",
    "        # Now change to Grouping Analyses directory\n",
    "    \n",
    "        os.chdir( Grouping_Analyses_path );\n",
    "    \n",
    "        #retval = os.getcwd()\n",
    "        #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "        results = pd.DataFrame(results, index = names, columns = ['Media','Desvio'])       \n",
    "        results.to_csv((\"Classification_result_\" + s) )\n",
    "        \n",
    "        print(d + ' ' + str(g))\n",
    "        print('-------------------------------------')\n",
    "        print(results)\n",
    "        print(' ')\n",
    "        \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RNHpxwRre-e"
   },
   "source": [
    "# .Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7AB1utxrfIg"
   },
   "outputs": [],
   "source": [
    "def Model_Train (ClassificationPar,d, Model_Name, g):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "    \n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "    for name,clf in zip(names ,classifiers):\n",
    "        \n",
    "        if  name == Model_Name:\n",
    "            \n",
    "            model = clf;\n",
    "            \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    \n",
    "    s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.csv';\n",
    "    X = np.genfromtxt(('X_' + s) , delimiter=',')    \n",
    "    y = np.genfromtxt(('Y_' + s), delimiter=',') \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    Output = {'Model': model,\n",
    "              'X': X_test,\n",
    "              'Y': y_test}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "\n",
    "    return Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWrXWuGQrfXq"
   },
   "outputs": [],
   "source": [
    "def Model_Predict (ModelPar):\n",
    "    for i in range (ModelPar['Y'].shape[0]):\n",
    "        model = ModelPar['Model']\n",
    "        y_predict = model.predict(ModelPar['X'][i,:].reshape(1, -1))\n",
    "    \n",
    "        if y_predict[0] == 0:\n",
    "            print('Ferramenta Boa')\n",
    "        else:\n",
    "            print('Ferramenta Ruim')\n",
    "    \n",
    "        print ('Label de Teste: %d' % int (ModelPar['Y'] [i]))\n",
    "        print ('Label dado pale NN: %d' % int (y_predict[0]))\n",
    "        print('___________________')\n",
    "        print('                   ')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZFxLQalFQ7y"
   },
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "Output_ID = 30\n",
    "\n",
    "D_S_parameters = DataSlicer(Output_ID,20,'Main Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "ExtractedNames = TSFRESH_Extraction(D_S_parameters) #(Extração de atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "SelectedFeatures = TSFRESH_Selection(D_S_parameters,ExtractedNames) # (Parametros e resultados da divisão de dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "ExtractedNames = TSFRESH_Extraction(D_S_parameters) #(Extração de atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "ReducedFeatures = PCA_calc(SelectedFeatures,3,'Calc') # (Feautures selecionadas, numero de PC's a manter)\n",
    "                                                          #('Test','Calc','Specific', 'Analytics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "SODA_parameters, processing_parameters = SODA(ReducedFeatures,1,6,0.25) # (Features reduzidas, granularidade mínima,\n",
    "                                                 # granularidade máxima, passo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "ClassificationPar = GroupingAlgorithm(SODA_parameters,60,20, processing_parameters) # (Labels do SODA, Porcentagem de definição,\n",
    "                                                             # numero de ID's boas, parametros de processamento)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "Classification(ClassificationPar,33,2.5) #(Parametros da data-set, numero de vezes a classificar, granularidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3"
   },
   "outputs": [],
   "source": [
    "ModelPar = Model_Train(ClassificationPar,'euclidean',\"Neural Net\",2.75) #(Parametros da data-set, distância, Modelo\n",
    "                                                                        # granularidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "error",
     "timestamp": 1573050031557,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "QL7D4igWFQ74",
    "outputId": "278a9608-a1de-4189-d5d9-ea00e67a10a3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Model_Predict(ModelPar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Unified_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
