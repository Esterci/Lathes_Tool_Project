{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "error",
     "timestamp": 1573049995271,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "zz62z0U1FQ6O",
    "outputId": "be52ca7a-9aa4-4dc1-deec-96fe0643de4b"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "import tsfresh\n",
    "from psutil import cpu_percent\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import io\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from tsfresh.feature_extraction import extract_features, ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction.settings import from_columns\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recovery (DataName): #Recovery function \n",
    "\n",
    "    #Changing Work Folder\n",
    "\n",
    "    add_path1 = \"/PCA_Analyses/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd ()\n",
    "    working_path = os.getcwd() + '/Model'\n",
    "    PCA_Analyses_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    \n",
    "    if DataName == 'D_S_parameters':\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "            \n",
    "            Final_Target = np.genfromtxt('FinalTarget.csv', delimiter = ',')\n",
    "            \n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            P_N_groups = int(np.load('M_N_groups.npy'))\n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            P_N_Ids = int(np.load('N_IDs.npy'))\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'FinalTarget': Final_Target,\n",
    "                    'M_N_groups': P_N_groups,\n",
    "                    'ID': Output_Id,\n",
    "                    'N_IDs': P_N_Ids}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "            print(\"D_S_parameters Recovered!\")\n",
    "        \n",
    "            return Output\n",
    "            \n",
    "        except:\n",
    "\n",
    "            print(\"D_S_parameters not recovered =(\" + '\\033[0m')\n",
    "    \n",
    "    elif DataName == 'ExtractedNames':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            extracted_names = np.load('extracted_names.npy')\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "\n",
    "            print(\"ExtractedNames recovered!\")\n",
    "            return extracted_names\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ExtractedNames not recovered =(\" + '\\033[0m')\n",
    "                 \n",
    "    elif DataName == 'SelectedFeatures':\n",
    "\n",
    "        try:    \n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "        \n",
    "            os.chdir( Kernel_path )\n",
    "            \n",
    "            features_filtered_1 = pd.read_csv('features_filtered_' + str(Output_Id) + '.csv') \n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'FeaturesFiltered': features_filtered_1,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "\n",
    "            print(\"SelectedFeatures recovered!\")            \n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"SelectedFeatures not recovered =(\" + '\\033[0m')\n",
    "        \n",
    "    elif DataName == 'ReducedFeatures':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            \n",
    "            # Now change to PCA Analyses directory\n",
    "        \n",
    "            os.chdir( PCA_Analyses_path )\n",
    "            \n",
    "            features_reduzidas = np.genfromtxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", delimiter=',')\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'ReducedFeatures': features_reduzidas,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ReducedFeatures recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ReducedFeatures not recovered =(\" + '\\033[0m')\n",
    "        \n",
    "    elif DataName == 'SODA_parameters_processing_parameters':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            processing_parameters = np.load(('processing_parameters.npy'), allow_pickle=True) \n",
    "            processing_parameters = processing_parameters.tolist() \n",
    "            distances = np.load(('distances.npy'), allow_pickle=True) \n",
    "            distances = distances.tolist() \n",
    "            min_granularity = np.load('Min_g.npy') \n",
    "            max_granularity = np.load('Max_g.npy') \n",
    "            pace = np.load('Pace.npy') \n",
    "\n",
    "            Output = {'Distances': distances,\n",
    "                    'Min_g': min_granularity,\n",
    "                    'Max_g': max_granularity,\n",
    "                    'Pace': pace,\n",
    "                    'ID': Output_Id}\n",
    "\n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            print(\"SODA_parameters_processing_parameters recovered!\")\n",
    "            return Output, processing_parameters\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"SODA_parameters_processing_parameters not recovered =(\" + '\\033[0m')\n",
    "                \n",
    "    elif DataName == 'ClassificationPar':\n",
    "\n",
    "        try:\n",
    "        \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            pace = np.load(\"Pace.npy\")\n",
    "            distances = np.load(('distances.npy'), allow_pickle=True) \n",
    "            distances = distances.tolist() \n",
    "            define_percent = np.load('define_percent.npy')\n",
    "            \n",
    "            Output = {'Percent': define_percent,\n",
    "                    'Distances': distances,\n",
    "                    'Pace': pace,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ClassificationPar recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ClassificationPar not recovered =(\" + '\\033[0m')    \n",
    "\n",
    "    elif DataName == 'ModelPar':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            # load the model from disk\n",
    "            model = pickle.load(open(\"Model.sav\", 'rb'))\n",
    "            X_test = np.load('X_test.npy') \n",
    "            y_test = np.load('y_test.npy') \n",
    "\n",
    "            Output = {'Model': model,\n",
    "                    'X': X_test,\n",
    "                    'Y': y_test}\n",
    "            \n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ModelPar recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ModelPar not recovered =(\" + '\\033[0m')   \n",
    "        \n",
    "    else:\n",
    "        print('\\033[93m' + \"Wrong name lad/lass, please check de Recovery input\" + '\\033[0m')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Recovery Control Output')\n",
    "print('----------------------------------')\n",
    "\n",
    "D_S_parameters =  Recovery('D_S_parameters') \n",
    "ExtractedNames =  Recovery('ExtractedNames') \n",
    "SelectedFeatures =  Recovery('SelectedFeatures') \n",
    "ReducedFeatures =  Recovery('ReducedFeatures') \n",
    "    \n",
    "Output_ID = int(D_S_parameters['ID'])\n",
    "\n",
    "print('The current Data ID is ', Output_ID)\n",
    "    \n",
    "print('__________________________________________')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, x_min, x_max): #Normalization\n",
    "\n",
    "    nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "    denom = X.max(axis=0) - X.min(axis=0)\n",
    "    if denom==0:\n",
    "        denom = 1\n",
    "    return x_min + nom/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Plot Formater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number): #Plot Formater\n",
    "    # find number of multiples of pi/2\n",
    "    N = int(value)\n",
    "    if N == 0:\n",
    "        return \"X1\"\n",
    "    elif N == 50:\n",
    "        return \"X50\"\n",
    "    elif N == 100:\n",
    "        return \"X100\"\n",
    "    elif N == 150:\n",
    "        return \"X150\"\n",
    "    elif N == 200:\n",
    "        return \"X200\"\n",
    "    elif N == 250:\n",
    "        return \"X250\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-61BMUFZFQ6X"
   },
   "source": [
    "# Feature Extraction/Selection Module\n",
    "    .Data Slicer for saving RAM;\n",
    "    .TSFRESH feature extraction and selection;\n",
    "    .PCA dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# . Data Slicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCrplMylFQ6a"
   },
   "outputs": [],
   "source": [
    "def DataSlicer (Output_Id, id_per_group, Choice): #Data Slicer\n",
    "    \n",
    "    print('Data Slicer Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Input/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Input_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "     \n",
    "    # Now change to Input directory\n",
    "    \n",
    "    os.chdir( Input_path )\n",
    "    \n",
    "    # Loading the required input \n",
    "    \n",
    "    Full_data = np.genfromtxt('Output_' + str(int(Output_Id)) + '.csv', delimiter=',')\n",
    "    #E_data = np.genfromtxt('Eminence_Data_' + str(Output_Id) + '.csv', delimiter=',')\n",
    "    columns = Full_data.shape[1]\n",
    "    data = Full_data[:,2:columns-1]\n",
    "    info = Full_data[:,0:2]\n",
    "    #centralizar os dados e colocá-los com desvioPadrão=1\n",
    "    scaler = StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    \n",
    "    P_data = np.concatenate((info,data), axis=1)\n",
    "    \n",
    "    Target = Full_data[:,columns-1]\n",
    "\n",
    "    print('Full Matrix: ' + str(Full_data.shape))\n",
    "    print('Main Data: ' + str(P_data.shape))\n",
    "    print('Labels: ' + str(Target.shape))\n",
    "    #print('Eminence Data: ' + str(E_data.shape))\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "          \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    pickle.dump(scaler, open('scaler.sav', 'wb'))\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                     ProDiMes Slicing Parameters                      ###\n",
    "\n",
    "\n",
    "    P_N_Ids = int(np.amax(P_data,axis=0)[0])\n",
    "    P_N_voos = int(np.amax(P_data,axis=0)[1])\n",
    "    P_last_group = int(P_N_Ids % id_per_group)\n",
    "\n",
    "    if P_last_group != 0:\n",
    "        P_N_groups = int((P_N_Ids / id_per_group) + 1)\n",
    "    else:\n",
    "        P_N_groups = int (P_N_Ids / id_per_group)\n",
    "\n",
    "    print ('Main data Number of Ids: ' + str(P_N_Ids ))\n",
    "    print ('Main data Number of mesures: ' + str(P_N_voos ))\n",
    "    print ('Main data Number of groups: ' + str(P_N_groups ))\n",
    "    print ('Main data Last group: ' + str(P_last_group ))\n",
    "    print ('___________________________________________')\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                    Eminences Slicing Parameters                      ###\n",
    "\n",
    "    #E_N_Ids = int(np.amax(E_data,axis=0)[0] - np.amax(P_data,axis=0)[0])\n",
    "    #E_N_voos = int(np.amax(E_data,axis=0)[1]) + 1\n",
    "    #E_last_group = int(E_N_Ids % id_per_group)\n",
    "\n",
    "    #if (E_last_group != 0):\n",
    "    #    E_N_groups = int((E_N_Ids / id_per_group) + 1)\n",
    "    #else:\n",
    "    #    E_N_groups = int (E_N_Ids / id_per_group)\n",
    "\n",
    "    #print ('Eminences Number of Ids: ' + str(E_N_Ids ))\n",
    "    #print ('Eminences Number of flights: ' + str(E_N_voos ))\n",
    "    #print ('Eminences Number of groups: ' + str(E_N_groups ))\n",
    "    #print ('Eminences Last group: ' + str(E_last_group ))\n",
    "\n",
    "\n",
    "    ### Formating Final Target ###\n",
    "\n",
    "    Final_Target = np.zeros((P_N_Ids))\n",
    "\n",
    "    for i in range (P_N_Ids):\n",
    "\n",
    "        Final_Target[i] = Target [i*P_N_voos]\n",
    "        \n",
    "    #np.savetxt(('Target_' + str(int(Output_Id)) + '.csv'), Final_Target, delimiter = ',')\n",
    "    \n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                      Slicing Prodimes Data                           ###\n",
    "\n",
    "    if (Choice =='Main Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (P_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * P_N_voos),columns-1))\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (P_N_voos):\n",
    "            \n",
    "                    if (i  < (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "\n",
    "                    elif (P_last_group == 0) and (i == (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "            \n",
    "            if (P_last_group != 0) and (i == (P_N_groups - 1)):     \n",
    "\n",
    "                Data = np.zeros(((P_last_group * P_N_voos),columns-1))\n",
    "            \n",
    "                for j in range (P_last_group):\n",
    "    \n",
    "                    for k in range (P_N_voos):\n",
    "    \n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "        \n",
    "            np.savetxt(('Data_' + str(i) + '.csv'), Data, delimiter = ',')\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Slicing Eminences                           ###\n",
    "    '''\n",
    "    if (Choice == 'Eminence Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (E_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * E_N_voos),columns-3))\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (E_N_voos):\n",
    "            \n",
    "                    if (i  < (E_N_groups - 1)):\n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:]\n",
    "                \n",
    "        \n",
    "            if (E_last_group != 0) and (i == (E_N_groups - 1)):\n",
    "            \n",
    "                Data = np.zeros(((E_last_group * E_N_voos),columns-3))\n",
    "            \n",
    "                for j in range (E_last_group):\n",
    "    \n",
    "                    for k in range (E_N_voos):\n",
    "    \n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:]\n",
    "    \n",
    "    \n",
    "            np.savetxt(('Eminence_' + str(i) + '.csv'), Data, delimiter = ',')\n",
    "    '''\n",
    "\n",
    "    np.savetxt(('FinalTarget.csv'), Final_Target, delimiter = ',')\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "          \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    np.save(('M_N_groups.npy'), P_N_groups)\n",
    "    np.save(('ID.npy'), Output_Id)\n",
    "    np.save(('N_IDs.npy'), P_N_Ids)\n",
    "    \n",
    "    # Now change back to Base directory\n",
    "          \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    Output = {'FinalTarget': Final_Target,\n",
    "              'M_N_groups': P_N_groups,\n",
    "              'ID': Output_Id,\n",
    "              'N_IDs': P_N_Ids}\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_Xgs4xlFQ6d"
   },
   "source": [
    "# .TSFRESH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Extraction(D_S_parameters): #TSFRESH Extraction\n",
    "    \n",
    "    print('             ')\n",
    "    print('TSFRESH Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "        \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                         Feature Extraction                           ###\n",
    "\n",
    "    #E_N_groups = np.load('E_N_groups.npy')\n",
    "    P_N_groups = D_S_parameters['M_N_groups']\n",
    "    \n",
    "    for i in range(P_N_groups):\n",
    "        \n",
    "        Data = np.genfromtxt('Data_' + str(i) + '.csv', delimiter=',')\n",
    "        data = pd.DataFrame(Data, columns= ['id','time'] + ['Sensor_' + str(x) for x in range(1,(Data.shape[1]-1))])\n",
    "        \n",
    "        Data_extracted_features = extract_features(data,column_id = \"id\", column_sort=\"time\")\n",
    "        extracted_names = list(Data_extracted_features.columns)\n",
    "        np.savetxt('Data_Features_' + str(i) + '.csv', Data_extracted_features.values, delimiter=',')\n",
    "        \n",
    "    #for i in range(E_N_groups):\n",
    "\n",
    "    \n",
    "    #    data = pd.DataFrame(np.genfromtxt('Eminence_' + str(i) + '.csv', delimiter=','), \n",
    "    #                        columns= ['id','time','sensor_1','sensor_2','sensor_3','sensor_4',\n",
    "    #                                            'sensor_5','sensor_6','sensor_7'])\n",
    "    #    extracted_features = extract_features(data, column_id = \"id\", column_sort=\"time\")\n",
    "    #    np.savetxt('Eminence_Features_' + str(i) + '.csv', extracted_features, delimiter=',')\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    np.save('extracted_names.npy',extracted_names)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return extracted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Selection(D_S_parameters,extracted_names): #TSFRESH Selection\n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Feature Selection                           ###\n",
    "    \n",
    "    P_N_groups = D_S_parameters['M_N_groups']\n",
    "    Output_Id = D_S_parameters['ID']\n",
    "    y = D_S_parameters['FinalTarget']\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "        \n",
    "    # Now change back to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    Data_Matrix = np.genfromtxt('Data_Features_0.csv', delimiter=',')\n",
    "    print('Data_Features_0.csv')\n",
    "\n",
    "    for i in range(1,P_N_groups):\n",
    "    \n",
    "        new_data = np.genfromtxt('Data_Features_' + str(i) + '.csv', delimiter=',') \n",
    "        \n",
    "        print('Data_Features_' + str(i) + '.csv')\n",
    "\n",
    "        Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0)\n",
    "\n",
    "    \n",
    "    #for i in range(E_N_groups):\n",
    "    \n",
    "    #    new_data = np.genfromtxt('Eminence_Features_' + str(i) + '.csv', delimiter=',') \n",
    "    \n",
    "    #    Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0)\n",
    "    \n",
    "    #    print('Eminence_Features_' + str(i) + '.csv')\n",
    "    \n",
    "    features = pd.DataFrame(Data_Matrix, columns= extracted_names)\n",
    "    \n",
    "    impute(features)\n",
    "    features_filtered_1 = select_features(features, y)\n",
    "    features_filtered_1.sort_index(inplace = True)\n",
    "    \n",
    "    features_filtered_1.to_csv('features_filtered_' + str(Output_Id) + '.csv', index=False)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    Output = {'FeaturesFiltered': features_filtered_1,\n",
    "              'FinalTarget': y,\n",
    "              'ID': Output_Id}\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_tsfresh (output_id):\n",
    "\n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Input/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Input_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    \n",
    "    # Change folder to Kernel\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    # Load the the filtered features from the seed data-set\n",
    "\n",
    "    features_filtered = pd.read_csv('features_filtered_{}.csv'.format(output_id))\n",
    "\n",
    "    # Extract the useful information of it\n",
    "\n",
    "    columns = np.array(features_filtered.columns)\n",
    "    kind_to_fc_parameters = tsfresh.feature_extraction.settings.from_columns(features_filtered.columns)\n",
    "\n",
    "    sensors_names = [None] * int(features_filtered.shape[1]);\n",
    "\n",
    "\n",
    "    for i in range (columns.shape[0]):\n",
    "        name = columns[i]\n",
    "        c = '__';\n",
    "        words = name.split(c)\n",
    "\n",
    "        sensors_names[i] = words[0]\n",
    "\n",
    "        '''if i < 20:\n",
    "\n",
    "            print(name)\n",
    "            print(words)\n",
    "            print(features_names[i])\n",
    "            print(sensors_names[i])\n",
    "            print('_______')'''\n",
    "\n",
    "    columns = columns.tolist()\n",
    "    unique_sensors_names = np.unique(np.array(sensors_names))\n",
    "\n",
    "    # Change folder to Input\n",
    "\n",
    "    os.chdir( Input_path )\n",
    "\n",
    "    # Load the incoming data\n",
    "    \n",
    "    Data = np.genfromtxt('Output_3.csv', delimiter=',')\n",
    "    data_frame = pd.DataFrame(Data[:,0:11], columns= ['id','time'] + ['Sensor_' + str(x) for x in range(1,(Data.shape[1]-2))])\n",
    "\n",
    "    # Feature extraction guided by the seed data-set\n",
    "\n",
    "    extraction_df = pd.DataFrame(data_frame.loc[::,'id':unique_sensors_names[0]].values,columns= ['id','time','Sensor'])\n",
    "    #print(extraction_df.head())\n",
    "    arrayList = [] \n",
    "\n",
    "    for sensor in unique_sensors_names:\n",
    "        \n",
    "        #print(extraction_df.head())\n",
    "        #print('_____')\n",
    "        extraction_df.loc[::,'Sensor'] = data_frame.loc[::,sensor]\n",
    "        \n",
    "        #print(extraction_df.head())\n",
    "        #print('_____')\n",
    "        \n",
    "        extraction_df = extraction_df.rename(columns={'Sensor': sensor})\n",
    "        \n",
    "        tsfresh_parameters = kind_to_fc_parameters[sensor]\n",
    "        \n",
    "        extracted_features = extract_features(extraction_df, column_id=\"id\", column_sort=\"time\", default_fc_parameters=tsfresh_parameters)\n",
    "\n",
    "        arrayList.append(extracted_features)\n",
    "\n",
    "        extraction_df = extraction_df.rename(columns={sensor : 'Sensor'})    \n",
    "\n",
    "    original_space_features = pd.concat(arrayList,axis=1)\n",
    "\n",
    "    # Sort the features in accordance with the seed data-set\n",
    "    \n",
    "    original_space_features = original_space_features[columns]\n",
    "    impute(original_space_features)\n",
    "    original_space_features.sort_index(inplace = True)\n",
    "\n",
    "    # Change folder to origin\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return original_space_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS1d7dxeFQ6j"
   },
   "source": [
    "# . PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_calc (SelectedFeatures,N_PCs,Chose):#PCA\n",
    "    \n",
    "    #%matplotlib%\n",
    "\n",
    "    if (Chose == 'Test') or (Chose == 'Calc') or (Chose == 'Specific') or (Chose == 'Analytics'):\n",
    "        \n",
    "        #Changing Work Folder\n",
    "        \n",
    "        add_path1 = \"/PCA_Analyses/\"\n",
    "        add_path2 = \"/Input/\"\n",
    "        add_path3 = \"/.Kernel/\"\n",
    "        add_path4 = \"/PCA_Analyses/Figures/\"        \n",
    "        base_path = os.getcwd()\n",
    "        working_path = os.getcwd()\n",
    "        PCA_Analyses_path = working_path + add_path1\n",
    "        Input_path = working_path + add_path2\n",
    "        Kernel_path = working_path + add_path3\n",
    "        PCA_Figures_path = working_path + add_path4\n",
    "        \n",
    "        # Now change to PCA Figures directory\n",
    "\n",
    "        os.chdir( Kernel_path )\n",
    "        \n",
    "        print('             ')\n",
    "        print('PCA Control Output')\n",
    "        print('----------------------------------')\n",
    "\n",
    "        Output_Id = SelectedFeatures['ID']\n",
    "        features = SelectedFeatures['FeaturesFiltered']\n",
    "        Target = SelectedFeatures['FinalTarget']\n",
    "        selected_names = list(features.columns)\n",
    "\n",
    "        #centralizar os dados e colocá-los com desvioPadrão=1\n",
    "        scaler = StandardScaler().fit(features)\n",
    "        features_padronizadas = scaler.transform(features)\n",
    "        #features_padronizadas = pd.DataFrame(features_padronizadas)\n",
    "\n",
    "        pca= PCA(n_components = N_PCs)\n",
    "        pca.fit(features_padronizadas)\n",
    "        \n",
    "        # save the model to disk\n",
    "\n",
    "        pickle.dump(pca, open('pca.sav', 'wb'))\n",
    "        \n",
    "        variacao_percentual_pca = np.round(pca.explained_variance_ratio_ * 100, decimals = 2)\n",
    "        \n",
    "        # Now change to PCA Figures directory\n",
    "        \n",
    "        fig = plt.figure(figsize=[16,8])\n",
    "        ax = fig.subplots(1,1)\n",
    "        ax.bar(x=['PC' + str(x) for x in range(1,(N_PCs+1))],height=variacao_percentual_pca[0:N_PCs])\n",
    "\n",
    "        ax.set_ylabel('Percentage of Variance Held',fontsize=20)\n",
    "        ax.set_xlabel('Principal Components',fontsize=20)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=18)\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "        fig.savefig('Percentage_of_Variance_Held_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "        print('Variation maintained: %.2f' % variacao_percentual_pca.sum())\n",
    "        print('                  ')\n",
    "\n",
    "        if (Chose != 'Test'):\n",
    "            features_reduzidas = pca.transform(features)\n",
    "            print('Filtered Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_padronizadas,0))\n",
    "            print(np.size(features_padronizadas,1))\n",
    "            print('-' * 20)\n",
    "            print('Reduced Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_reduzidas,0))\n",
    "            print(np.size(features_reduzidas,1))\n",
    "\n",
    "            if (Chose != 'Test'):\n",
    "                \n",
    "                ### Análise de atributos ###\n",
    "\n",
    "\n",
    "                eigen_matrix = np.array(pca.components_)\n",
    "                eigen_matrix = pow((pow(eigen_matrix,2)),0.5) #invertendo valores negativos\n",
    "\n",
    "                for i in range (eigen_matrix.shape[0]):\n",
    "\n",
    "                    LineSum = sum(eigen_matrix[i,:])\n",
    "                    for j in range (eigen_matrix.shape[1]):\n",
    "                        eigen_matrix[i,j] = ((eigen_matrix[i,j]*100)/LineSum)\n",
    "\n",
    "\n",
    "                if Chose == 'Specific':\n",
    "                ### Análise Expecífica ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,int(8*N_PCs)])\n",
    "\n",
    "                    fig.suptitle('Contribution percentage per PC', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(int(N_PCs),1)\n",
    "\n",
    "                    for i in range (int(N_PCs)):\n",
    "\n",
    "                        s = eigen_matrix[i,:]\n",
    "\n",
    "                        ax[i].bar(x=range(0,(eigen_matrix.shape[1])),height=s)\n",
    "                        ax[i].set(xlabel='Features', ylabel='Contribution Percentage', title = 'PC ' + str(i+1))\n",
    "                        ax[i].grid()\n",
    "\n",
    "\n",
    "                    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "                    for axs in ax.flat:\n",
    "                        axs.label_outer()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Contribution_Percentage_Per_PC_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                if (Chose == 'Analytics'):\n",
    "                    ### Análise Geral ###\n",
    "\n",
    "                    weighted_contribution = np.zeros((2,eigen_matrix.shape[1]))\n",
    "\n",
    "                    for i in range (eigen_matrix.shape[1]):\n",
    "                        NumeratorSum = 0\n",
    "                        for j in range (N_PCs):\n",
    "                            NumeratorSum += eigen_matrix[j,i] * variacao_percentual_pca[j]\n",
    "\n",
    "                        weighted_contribution[0,i] = NumeratorSum / sum(variacao_percentual_pca)\n",
    "\n",
    "                    df_weighted_contribution = pd.DataFrame(weighted_contribution,columns=selected_names)\n",
    "                    df_weighted_contribution = df_weighted_contribution.drop([1])                    \n",
    "                    df_weighted_contribution = df_weighted_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    \n",
    "                    \n",
    "                    #pd.set_option('display.max_rows', len(df_weighted_contribution))\n",
    "                    #print(type(df_weighted_contribution))\n",
    "                    #print(df_weighted_contribution.head())\n",
    "                    #pd.reset_option('display.max_rows')\n",
    "\n",
    "                    #Creating Separated Data Frames por Sensors and Features Contribution \n",
    "\n",
    "                    sensors_names = [None] * int(df_weighted_contribution.shape[1])\n",
    "                    features_names = [None] * int(df_weighted_contribution.shape[1])\n",
    "                    general_features = [None] * int(df_weighted_contribution.shape[1])\n",
    "\n",
    "\n",
    "                    for i, names in zip(range (df_weighted_contribution.shape[1]), df_weighted_contribution.columns):\n",
    "\n",
    "                        c = '__'\n",
    "                        words = names.split(c)\n",
    "                        \n",
    "                        sensors_names[i] = words[0]\n",
    "                        general_features[i]= words[1]\n",
    "                        features_names[i] = c.join(words[1:])\n",
    "\n",
    "                        #print(names)\n",
    "                        #print(words)\n",
    "                        #print(sensors_names[i])\n",
    "                        #print(features_names[i])\n",
    "                        #print(50*'-')\n",
    "\n",
    "                    \n",
    "                    unique_sensors_names = np.ndarray.tolist(np.unique(np.array(sensors_names))) \n",
    "                    unique_general_feature = np.ndarray.tolist(np.unique(np.array(general_features))) \n",
    "                    unique_features_names = np.ndarray.tolist(np.unique(np.array(features_names)))\n",
    "                    sensors_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_sensors_names)[0])), columns=unique_sensors_names)\n",
    "                    general_features_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_general_feature)[0])), columns=unique_general_feature)\n",
    "                    features_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_features_names)[0])), columns=unique_features_names)\n",
    "                    sensors_contribution = sensors_contribution.drop([1])\n",
    "                    general_features_contribution = general_features_contribution.drop([1])\n",
    "                    features_contribution = features_contribution.drop([1])\n",
    "                    \n",
    "                    \n",
    "                    # For the output Formating\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    unique_sensors_names = np.ndarray.tolist(np.unique(np.array(sensors_names))) \n",
    "                    unique_features_names = np.ndarray.tolist(np.unique(np.array(features_names)))\n",
    "                    sensor_dt = np.transpose(np.vstack((unique_sensors_names,np.asarray(np.zeros(np.shape(unique_sensors_names)[0]),object))))\n",
    "                    feature_dt = np.transpose(np.vstack((unique_features_names,np.asarray(np.zeros(np.shape(unique_features_names)[0]),object))))\n",
    "                    sensors_contribution = pd.DataFrame(sensor_dt,columns = ['Sensor','Contribution'])\n",
    "                    features_contribution = pd.DataFrame(feature_dt,columns = ['Feature','Contribution'])\n",
    "                    \"\"\"\n",
    "                    #print(sensors_contribution.head())\n",
    "                    #print(features_contribution.head())\n",
    "\n",
    "                    #Creating dictionaries form Data Frame orientation\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    Creates a mapping from kind names to fc_parameters objects\n",
    "                    (which are itself mappings from feature calculators to settings)\n",
    "                    to extract only the features contained in the columns.\n",
    "                    To do so, for every feature name in columns this method\n",
    "\n",
    "                    1. split the column name into col, feature, params part\n",
    "                    2. decide which feature we are dealing with (aggregate with/without params or apply)\n",
    "                    3. add it to the new name_to_function dict\n",
    "                    4. set up the params\n",
    "\n",
    "                    :param columns: containing the feature names\n",
    "                    :type columns: list of str\n",
    "                    :param columns_to_ignore: columns which do not contain tsfresh feature names\n",
    "                    :type columns_to_ignore: list of str\n",
    "\n",
    "                    :return: The kind_to_fc_parameters object ready to be used in the extract_features function.\n",
    "                    :rtype: dict\n",
    "                    \"\"\"\n",
    "\n",
    "                    weighted_contribution_dic = {}\n",
    "\n",
    "                    for col in df_weighted_contribution.columns:\n",
    "\n",
    "                        # Split according to our separator into <col_name>, <feature_name>, <feature_params>\n",
    "                        parts = col.split('__')\n",
    "                        n_parts = len(parts)\n",
    "\n",
    "                        if n_parts == 1:\n",
    "                            raise ValueError(\"Splitting of columnname {} resulted in only one part.\".format(col))\n",
    "\n",
    "                        kind = parts[0]\n",
    "                        feature = c.join(parts[1:])\n",
    "                        feature_name = parts[1]\n",
    "\n",
    "                        if kind not in weighted_contribution_dic:\n",
    "                            weighted_contribution_dic[kind] = {}\n",
    "\n",
    "                        if not hasattr(feature_calculators, feature_name):\n",
    "                            raise ValueError(\"Unknown feature name {}\".format(feature_name))\n",
    "                            \n",
    "                        sensors_contribution.loc[0,kind] += df_weighted_contribution.loc[0,col]\n",
    "                        general_features_contribution.loc[0,feature_name] += df_weighted_contribution.loc[0,col]\n",
    "                        features_contribution.loc[0,feature] += df_weighted_contribution.loc[0,col]\n",
    "                        weighted_contribution_dic[kind][feature] = df_weighted_contribution.loc[0,col]\n",
    "                    \n",
    "                        \n",
    "                    # End of the tsfresh stolen function\n",
    "\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    sensors_dic = {}\n",
    "                    for i in range(len(unique_sensors_names)):\n",
    "                        sensors_dic[unique_sensors_names[i]] = i\n",
    "\n",
    "                    features_dic = {}\n",
    "                    for i in range(len(unique_features_names)):\n",
    "                        features_dic[unique_features_names[i]] = i\n",
    "\n",
    "                    #Suming the contibution for Sensors and Features\n",
    "\n",
    "                    for i in range(df_weighted_contribution.shape[0]):\n",
    "\n",
    "                        names = df_weighted_contribution.loc[i,'tsfresh_info']\n",
    "                        c = '__'\n",
    "                        words = names.split(c)           \n",
    "                        S= words[0]\n",
    "                        F= c.join(words[1:])\n",
    "\n",
    "                        sensors_contribution.loc[sensors_dic[S],'Contribution'] += df_weighted_contribution.loc[i,'Contribution']\n",
    "                        features_contribution.loc[features_dic[F],'Contribution'] += df_weighted_contribution.loc[i,'Contribution']\n",
    "\n",
    "                    sensors_contribution = sensors_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "                    features_contribution = features_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    features_contribution = features_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    general_features_contribution = general_features_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    \n",
    "                    features_indexes = [x for x in range(1,(features_contribution.shape[0])+1)]\n",
    "                    general_features_indexes = [x for x in range(1,(general_features_contribution.shape[0])+1)]\n",
    "\n",
    "                    features_contribution.set_index(pd.Index(features_indexes))\n",
    "                    general_features_contribution.set_index(pd.Index(general_features_indexes))\n",
    "                    \n",
    "                    sorted_sensors_contribution = sensors_contribution.values[0,:]\n",
    "                    sorted_features_contribution = features_contribution.values[0,:]\n",
    "                    sorted_general_features_contribution = general_features_contribution.values[0,:]\n",
    "\n",
    "                    #Ploting Cntribution Sensors Results\n",
    "                    \n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Sensors Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_sensors_contribution[:]\n",
    "\n",
    "                    ax.bar(x=sensors_contribution.columns,height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Sensors',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Sensor_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                    #Ploting Cntribution Features Results\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[:]\n",
    "\n",
    "                    ax.bar(x=range(0,(sorted_features_contribution.shape[0])),height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.xaxis.set_major_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_minor_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Features_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    ### Análise Geral para os 20 melhores atributos completos ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Best Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    #print('Porcentagem de pertinência: ', np.sum(sorted_features_contribution[0:140]))\n",
    "                    #print('Number of Selected Features: ', sorted_features_contribution.shape[0])\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[0:20]\n",
    "\n",
    "                    ax.bar(x=['X' + str(x) for x in range(1,(20+1))],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('{}th_Best_Features_Weighted_Contribution_Percentage_{}.png'.format(20,Output_Id), bbox_inches='tight')\n",
    "\n",
    "                    ### Análise Geral para os 20 melhores atributos gerais ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Best Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    #print('Porcentagem de pertinência: ', np.sum(sorted_features_contribution[0:140]))\n",
    "                    #print('Number of Selected Features: ', sorted_features_contribution.shape[0])\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[0:20]\n",
    "\n",
    "                    ax.bar(x=['X' + str(x) for x in range(1,(20+1))],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('General Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('{}th_Best_Features_Weighted_Contribution_Percentage_{}.png'.format(20,Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    #Ploting the data of the most relevant sensor with the best features\n",
    "\n",
    "                    sensors_contribution.values[:,0]\n",
    "\n",
    "                    name_1 = df_weighted_contribution.columns[0]\n",
    "                    name_2 = df_weighted_contribution.columns[1]\n",
    "                    name_3 = df_weighted_contribution.columns[2]\n",
    "\n",
    "\n",
    "                    #pd.set_option('display.max_columns', len(features))\n",
    "                    #print(features)\n",
    "                    #pd.reset_option('display.max_columns')\n",
    "\n",
    "                    x = features.loc[:,name_1].values\n",
    "                    y = features.loc[:,name_2].values\n",
    "                    z = features.loc[:,name_3].values\n",
    "\n",
    "\n",
    "                    x = scale(x,-1,1)\n",
    "                    y = scale(y,-1,1)\n",
    "                    z = scale(z,-1,1)\n",
    "\n",
    "                    x_bom=[]\n",
    "                    x_ruim=[]\n",
    "                    y_bom=[]\n",
    "                    y_ruim=[]\n",
    "                    z_bom=[]\n",
    "                    z_ruim=[]\n",
    "                    \n",
    "                    for i in range(len(Target)):\n",
    "                        if Target[i] == 0:\n",
    "                            x_bom.append(x[i])\n",
    "                            y_bom.append(y[i])\n",
    "                            z_bom.append(z[i])\n",
    "                        if Target[i] == 1:\n",
    "                            x_ruim.append(x[i])\n",
    "                            y_ruim.append(y[i])\n",
    "                            z_ruim.append(z[i])\n",
    "                            \n",
    "                    os.chdir( base_path )\n",
    "                            \n",
    "                    np.savetxt('x_bom.csv', x_bom, delimiter=',')\n",
    "                    np.savetxt('x_ruim.csv', x_ruim, delimiter=',')\n",
    "                    \n",
    "                    os.chdir( PCA_Figures_path )\n",
    "\n",
    "                    fig = plt.figure(figsize=[14,10])\n",
    "                    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "                    ax.scatter(x_bom, y_bom, z_bom, c = 'blue' )\n",
    "                    ax.scatter(x_ruim, y_ruim, z_ruim, c = 'red' )\n",
    "\n",
    "                    plt.ylabel('X2',fontsize = 20,labelpad=18)\n",
    "                    plt.xlabel('X1',fontsize = 20, labelpad=18)\n",
    "                    ax.set_zlabel('X3', fontsize = 20, labelpad=12)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=16)\n",
    "                    plt.tick_params(axis='z', labelsize=16)\n",
    "                    ax.grid()\n",
    "                    red_patch = mpatches.Patch(color='red', label='Non-Funcional Tools')\n",
    "                    blue_patch = mpatches.Patch(color='blue', label='Funcional Tools')\n",
    "                    plt.legend(handles=[red_patch,blue_patch],fontsize = 20)\n",
    "                    plt.show()\n",
    "                    fig.savefig('ScatterPlot_PCA_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "                    \n",
    "                    # Now change to PCA Analyses directory\n",
    "\n",
    "                    os.chdir( PCA_Analyses_path )\n",
    "\n",
    "                    general_features_contribution.to_csv('unique_features_used_{}.csv'.format(Output_Id),index = False)\n",
    "                    sensors_contribution.to_csv('sensors_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "                    features_contribution.to_csv('features_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "\n",
    "            # Now change to PCA Analyses directory\n",
    "\n",
    "            os.chdir( PCA_Analyses_path )\n",
    "            \n",
    "            np.savetxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", features_reduzidas, delimiter=',')\n",
    "\n",
    "            Output = {'ReducedFeatures': features_reduzidas,\n",
    "                      'ID': Output_Id} \n",
    "        elif (Chose == 'Test'): \n",
    "\n",
    "            Output = {'ID': Output_Id}\n",
    "        \n",
    "        # Now change back to base directory\n",
    "\n",
    "        os.chdir( base_path )\n",
    "\n",
    "        return Output\n",
    "    \n",
    "        print(\"Wrong Choose entry, verify this input.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_projection (features):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "        \n",
    "    add_path3 = \"/.Kernel/\"\n",
    "  \n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path3\n",
    "        \n",
    "    # Now change to PCA Figures directory\n",
    "\n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    # load the model from disk\n",
    "    loaded_pca = pickle.load(open('pca.sav', 'rb'))\n",
    "\n",
    "    scaler = StandardScaler().fit(features)\n",
    "    features_padronizadas = scaler.transform(features)\n",
    "\n",
    "    features_reduzidas = loaded_pca.transform(features_padronizadas)\n",
    "    \n",
    "    print('Filtered Features')\n",
    "    print('-' * 20)\n",
    "    print(np.size(features_padronizadas,0))\n",
    "    print(np.size(features_padronizadas,1))\n",
    "    print('-' * 20)\n",
    "    print('Reduced Features')\n",
    "    print('-' * 20)\n",
    "    print(np.size(features_reduzidas,0))\n",
    "    print(np.size(features_reduzidas,1))\n",
    "    \n",
    "    # Now chance to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return features_reduzidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEeXLskLFQ6q"
   },
   "source": [
    "# Data Partitioning Module\n",
    "    .SODA\n",
    "    .Grouping Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAunBilsFQ6s"
   },
   "source": [
    "# . SODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-ol3Jpe1bO"
   },
   "outputs": [],
   "source": [
    "class cpu_usage(threading.Thread):### Thread to calculate duration and mean cpu percente usage in a SODA classifier\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.control = True\n",
    "        \n",
    "    def run(self):\n",
    "        cpu = []\n",
    "        t_inicial = time.time()\n",
    "        while self.control:\n",
    "            cpu.append(cpu_percent(interval=1, percpu=True))\n",
    "        t_final = time.time()\n",
    "        self.deltatime = t_final - t_inicial\n",
    "        self.mean_cpu = np.mean(cpu)\n",
    "        \n",
    "    def stop(self):\n",
    "        self.control = False\n",
    "        \n",
    "    def join(self):\n",
    "        threading.Thread.join(self)\n",
    "        return self.deltatime, self.mean_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep1uNgbAFQ6t"
   },
   "outputs": [],
   "source": [
    "def grid_set(data, N): #SODA process\n",
    "    _ , W = data.shape\n",
    "    AvD1 = data.mean(0)\n",
    "    X1 = np.mean(np.sum(np.power(data,2),axis=1))\n",
    "    grid_trad = np.sqrt(2*(X1 - AvD1*AvD1.T))/N\n",
    "    Xnorm = np.sqrt(np.sum(np.power(data,2),axis=1))\n",
    "    aux = Xnorm\n",
    "    for _ in range(W-1):\n",
    "        aux = np.insert(aux,0,Xnorm.T,axis=1)\n",
    "    data = data / aux\n",
    "    seq = np.argwhere(np.isnan(data))\n",
    "    if tuple(seq[::]): data[tuple(seq[::])] = 1\n",
    "    AvD2 = data.mean(0)\n",
    "    grid_angl = np.sqrt(1-AvD2*AvD2.T)/N\n",
    "    return X1, AvD1, AvD2, grid_trad, grid_angl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHAa-LjmFQ6y"
   },
   "outputs": [],
   "source": [
    "def pi_calculator(Uniquesample, mode):#SODA process\n",
    "    UN, W = Uniquesample.shape\n",
    "    if mode == 'euclidean' or mode == 'mahalanobis' or mode == 'cityblock' or mode == 'chebyshev' or mode == 'canberra':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA1)\n",
    "        aux2 = [Uniquesample[i]-aux[i] for i in range(UN)]\n",
    "        uspi = np.sum(np.power(aux2,2),axis=1)+DT1\n",
    "    \n",
    "    if mode == 'minkowski':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = np.matrix(AA1)\n",
    "        for i in range(UN-1): aux = np.insert(aux,0,AA1,axis=0)\n",
    "        aux = np.array(aux)\n",
    "        uspi = np.sum(np.power(cdist(Uniquesample, aux, mode, p=1.5),2),1)+DT1\n",
    "    \n",
    "    if mode == 'cosine':\n",
    "        Xnorm = np.matrix(np.sqrt(np.sum(np.power(Uniquesample,2),axis=1))).T\n",
    "        aux2 = Xnorm\n",
    "        for i in range(W-1):\n",
    "            aux2 = np.insert(aux2,0,Xnorm.T,axis=1)\n",
    "        Uniquesample1 = Uniquesample / aux2\n",
    "        AA2 = np.mean(Uniquesample1,0)\n",
    "        X2 = 1\n",
    "        DT2 = X2 - np.sum(np.power(AA2,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA2)\n",
    "        aux2 = [Uniquesample1[i]-aux[i] for i in range(UN)]\n",
    "        uspi = np.sum(np.sum(np.power(aux2,2),axis=1),axis=1)+DT2\n",
    "        \n",
    "    return uspi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU3o7WCFQ64"
   },
   "outputs": [],
   "source": [
    "def Globaldensity_Calculator(data, distancetype):#SODA process\n",
    "    \n",
    "    Uniquesample, J, K = np.unique(data, axis=0, return_index=True, return_inverse=True)\n",
    "    Frequency, _ = np.histogram(K,bins=len(J))\n",
    "    uspi1 = pi_calculator(Uniquesample, distancetype)\n",
    "    sum_uspi1 = sum(uspi1)\n",
    "    Density_1 = uspi1 / sum_uspi1\n",
    "    uspi2 = pi_calculator(Uniquesample, 'cosine')\n",
    "    sum_uspi2 = sum(uspi2)\n",
    "    Density_2 = uspi1 / sum_uspi2\n",
    "    \n",
    "    GD = (Density_2+Density_1) * Frequency\n",
    "\n",
    "    index = GD.argsort()[::-1]\n",
    "    GD = GD[index]\n",
    "    Uniquesample = Uniquesample[index]\n",
    "    Frequency = Frequency[index]\n",
    " \n",
    "    return GD, Uniquesample, Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCOT-mm5FQ68"
   },
   "outputs": [],
   "source": [
    "def chessboard_division(Uniquesample, MMtypicality, interval1, interval2, distancetype):#SODA process\n",
    "    L, W = Uniquesample.shape\n",
    "    if distancetype == 'euclidean':\n",
    "        W = 1\n",
    "    BOX = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_miu = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_S = [1]*W\n",
    "    BOX_X = [sum(Uniquesample[k]**2) for k in range(W)]\n",
    "    NB = W\n",
    "    BOXMT = [MMtypicality[k] for k in range(W)]\n",
    "    \n",
    "    for i in range(W,L):\n",
    "        if distancetype == 'minkowski':\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype, p=1.5)\n",
    "        else:\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype)\n",
    "        \n",
    "        b = np.sqrt(cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric='cosine'))\n",
    "        distance = np.array([a[0],b[0]]).T\n",
    "        SQ = []\n",
    "        for j,d in enumerate(distance):\n",
    "            if d[0] < interval1 and d[1] < interval2:\n",
    "                SQ.append(j)\n",
    "        #SQ = np.argwhere(distance[::,0]<interval1 and (distance[::,1]<interval2))\n",
    "        COUNT = len(SQ)\n",
    "        if COUNT == 0:\n",
    "            BOX.append(Uniquesample[i])\n",
    "            NB = NB + 1\n",
    "            BOX_S.append(1)\n",
    "            BOX_miu.append(Uniquesample[i])\n",
    "            BOX_X.append(sum(Uniquesample[i]**2))\n",
    "            BOXMT.append(MMtypicality[i])\n",
    "        if COUNT >= 1:\n",
    "            DIS = distance[SQ[::],0]/interval1 + distance[SQ[::],1]/interval2 # pylint: disable=E1136  # pylint/issues/3139\n",
    "            b = np.argmin(DIS)\n",
    "            BOX_S[SQ[b]] = BOX_S[SQ[b]] + 1\n",
    "            BOX_miu[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_miu[SQ[b]] + Uniquesample[i]/BOX_S[SQ[b]]\n",
    "            BOX_X[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_X[SQ[b]] + sum(Uniquesample[i]**2)/BOX_S[SQ[b]]\n",
    "            BOXMT[SQ[b]] = BOXMT[SQ[b]] + MMtypicality[i]\n",
    "\n",
    "\n",
    "    return BOX, BOX_miu, BOX_X, BOX_S, BOXMT, NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2TZty8dFQ6_"
   },
   "outputs": [],
   "source": [
    "def ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,Internval1,Internval2, distancetype):#SODA process\n",
    "    Centers = []\n",
    "    n = 2\n",
    "    ModeNumber = 0\n",
    "           \n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype, p=1.5))\n",
    "    else:\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype))        \n",
    "\n",
    "    distance2 = np.sqrt(squareform(pdist(BOX_miu,metric='cosine')))\n",
    "      \n",
    "    for i in range(NB):\n",
    "        seq = []\n",
    "        for j,(d1,d2) in enumerate(zip(distance1[i],distance2[i])):\n",
    "            if d1 < n*Internval1 and d2 < n*Internval2:\n",
    "                seq.append(j)\n",
    "        Chessblocak_typicality = [BOXMT[j] for j in seq]\n",
    "\n",
    "        if max(Chessblocak_typicality) == BOXMT[i]:\n",
    "            Centers.append(BOX_miu[i])\n",
    "            ModeNumber = ModeNumber + 1\n",
    "\n",
    "    return Centers, ModeNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gNgPN7dFQ7D"
   },
   "outputs": [],
   "source": [
    "def cloud_member_recruitment(ModelNumber,Center_samples,Uniquesample,grid_trad,grid_angl, distancetype):#SODA process\n",
    "    L, W = Uniquesample.shape\n",
    "    Membership = np.zeros((L,ModelNumber))\n",
    "    Members = np.zeros((L,ModelNumber*W))\n",
    "    Count = []\n",
    "    \n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype, p=1.5)/grid_trad\n",
    "    else:\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype)/grid_trad\n",
    "\n",
    "    distance2 = np.sqrt(cdist(Uniquesample, Center_samples, metric='cosine'))/grid_angl\n",
    "    distance3 = distance1 + distance2\n",
    "    B = distance3.argmin(1)\n",
    "\n",
    "    for i in range(ModelNumber):\n",
    "        seq = []\n",
    "        for j,b in enumerate(B):\n",
    "            if b == i:\n",
    "                seq.append(j)\n",
    "        Count.append(len(seq))\n",
    "        Membership[:Count[i]:,i] = seq\n",
    "        Members[:Count[i]:,W*i:W*(i+1)] = [Uniquesample[j] for j in seq]\n",
    "    MemberNumber = Count\n",
    "    \n",
    "    #Converte a matriz para vetor E SOMA +1 PARA NAO TER CONJUNTO 0'\n",
    "    B = B.A1\n",
    "    B = [x+1 for x in B]\n",
    "    return Members,MemberNumber,Membership,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzmeIoLIFQ7I"
   },
   "outputs": [],
   "source": [
    "def SelfOrganisedDirectionAwareDataPartitioning(Input, Mode):#SODA process\n",
    "    \n",
    "    \"\"\"\n",
    "    Self-organising Direction-Aware Data Partitioning (offline version)\n",
    "    :params:\n",
    "    \n",
    "    :Input: dict containing gridsize, data and distance methodology\n",
    "    :Mode: Offline or Evolving (online)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if Mode == 'Offline':\n",
    "        data = Input['StaticData']\n",
    "\n",
    "        L = data.shape[0]\n",
    "        N = Input['GridSize']\n",
    "        distancetype = Input['DistanceType']\n",
    "        X1, AvD1, AvD2, grid_trad, grid_angl = grid_set(data,N)\n",
    "        GD, Uniquesample, Frequency = Globaldensity_Calculator(data, distancetype)\n",
    "\n",
    "        BOX,BOX_miu,BOX_X,BOX_S,BOXMT,NB = chessboard_division(Uniquesample,GD,grid_trad,grid_angl, distancetype)\n",
    "        Center,ModeNumber = ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,grid_trad,grid_angl, distancetype)\n",
    "        Members,Membernumber,Membership,IDX = cloud_member_recruitment(ModeNumber,Center,data,grid_trad,grid_angl, distancetype)\n",
    "        \n",
    "        Boxparameter = {'BOX': BOX,\n",
    "                'BOX_miu': BOX_miu,\n",
    "                'BOX_S': BOX_S,\n",
    "                'NB': NB,\n",
    "                'XM': X1,\n",
    "                'L': L,\n",
    "                'AvM': AvD1,\n",
    "                'AvA': AvD2,\n",
    "                'GridSize': N}\n",
    "        \n",
    "    if Mode == 'Evolving':\n",
    "        print(Mode)\n",
    "\n",
    "    Output = {'C': Center,\n",
    "              'IDX': IDX,\n",
    "              'SystemParams': Boxparameter,\n",
    "              'DistanceType': distancetype}\n",
    "           \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tayjmyeOFQ7N"
   },
   "source": [
    "#### Distâncias ####\n",
    " \n",
    " - euclidean - linha reta entre os pontos\n",
    " - mahalanobis - correlação entre as variaveis (determina similaridade)\n",
    " - cityblock - distancia das projeções dos pontos (taxicab/manhattan)\n",
    " - chebyshev - maior distancia entre as coordenadas (rei)\n",
    " - minkowski - generalização de outras distâncias:\n",
    "  - p = 1 $\\rightarrow$ cityblock,\n",
    "  - p = 2 $\\rightarrow$ euclidean,\n",
    "  - p = $\\infty$ $\\rightarrow$ chebyshev.\n",
    " - canberra - versão com pesos da cityblock, sensivel para pontos proximos à origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PE6QHo8bFQ7O"
   },
   "outputs": [],
   "source": [
    "def SODA (ReducedFeatures, min_granularity, max_granularity, pace):#SODA\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    \n",
    "    DataSetID = ReducedFeatures['ID']\n",
    "    data = ReducedFeatures['ReducedFeatures']\n",
    "    data = np.matrix(data)\n",
    "\n",
    "    distances = ['euclidean']#, 'mahalanobis', 'cityblock', 'chebyshev', 'minkowski', 'canberra']\n",
    "    processing_parameters = []\n",
    "    \n",
    "    #### Looping SODA within the chosen granularities and distances ####\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "\n",
    "        for d in distances:\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "            \n",
    "            Input = {'GridSize':g, 'StaticData':data, 'DistanceType': d}\n",
    "            \n",
    "            out = SelfOrganisedDirectionAwareDataPartitioning(Input,'Offline')\n",
    "            \n",
    "            ### Interrupt Thread and Calculate Parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            pp = {'DistanceType': d,\n",
    "                  'Granularity': g,\n",
    "                  'Time': deltatime,\n",
    "                  'CPUPercent': mean_cpu}\n",
    "            processing_parameters.append(pp)\n",
    "\n",
    "            \n",
    "            np.savetxt('SODA_' + d + '_label_' + str (DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', out['IDX'],delimiter=',')\n",
    "\n",
    "            \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "    \n",
    "    np.save(('processing_parameters.npy'), processing_parameters)\n",
    "    np.save(('distances.npy'), distances)\n",
    "    np.save(('Min_g.npy'), min_granularity)\n",
    "    np.save(('Max_g.npy'), max_granularity)\n",
    "    np.save(('Pace.npy'), pace)\n",
    "    \n",
    "    Output = {'Distances': distances,\n",
    "              'Min_g': min_granularity,\n",
    "              'Max_g': max_granularity,\n",
    "              'Pace': pace,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "    \n",
    "    return Output, processing_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ijABlt6FQ7R"
   },
   "source": [
    "# . Grouping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlPWb0RZFQ7g"
   },
   "outputs": [],
   "source": [
    "def GroupingAlgorithm (SODA_parameters,define_percent, processing_parameters): #Grouping Algorithm\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/PCA_Analyses/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    add_path4 = \"/Grouping_Analyses/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    PCA_Analyses_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    Grouping_Analyses_path = working_path + add_path4\n",
    "    \n",
    "    print('             ')\n",
    "    print('Grouping Algorithm Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    \n",
    "    ####   imput data    ####\n",
    "    DataSetID = SODA_parameters['ID']\n",
    "    min_granularity = SODA_parameters['Min_g']\n",
    "    max_granularity = SODA_parameters['Max_g']\n",
    "    pace = SODA_parameters['Pace']\n",
    "    distances = SODA_parameters['Distances']\n",
    "\n",
    "    # Change to Kernel directory\n",
    "    os.chdir(Kernel_path)\n",
    "\n",
    "    y_original = np.genfromtxt('FinalTarget.csv', delimiter=',')\n",
    "\n",
    "    # functional engines\n",
    "    #n_IDs_gp1 = 0 # non-functional engines\n",
    "    #n_IDs_gp2 = 3598 # eminent  fault  engines\n",
    "\n",
    "    for d in distances:\n",
    "        \n",
    "        for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "    \n",
    "            s = 'SODA_' + d + '_label_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv'\n",
    "            \n",
    "\n",
    "            #### Data-base Imput ####\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            SodaOutput = np.genfromtxt( s , delimiter=',')\n",
    "            \n",
    "            # Now change to PCA Analyses directory\n",
    "    \n",
    "            os.chdir( PCA_Analyses_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            SelectedFeatures = np.genfromtxt('features_reduzidas_' + str(DataSetID) + '.csv' , delimiter=',')\n",
    "\n",
    "            #### Program Matrix's and Variables ####\n",
    "\n",
    "            n_DA_planes = np.max(SodaOutput)\n",
    "            Percent = np.zeros((int(n_DA_planes),3))\n",
    "            n_IDs_per_gp = np.zeros((int(n_DA_planes),2))\n",
    "            n_tot_Id_per_DA = np.zeros((int(n_DA_planes),1))\n",
    "            decision = np.zeros(int(n_DA_planes))\n",
    "            selected_samples = np.zeros(2)\n",
    "            n_DA_excluded = 0\n",
    "            n_excluded = 0\n",
    "            n_gp0 = 0\n",
    "            n_gp1 = 0\n",
    "            n_gp2 = 0\n",
    "            n_data_def = 0\n",
    "            k = 0\n",
    "\n",
    "            #### Definition Percentage Calculation #####\n",
    "\n",
    "            for i in range(y_original.shape[0]):\n",
    "    \n",
    "                if y_original[i] == 0:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]-1),0] += 1 \n",
    "                else:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]-1),1] += 1 \n",
    "\n",
    "                n_tot_Id_per_DA [int(SodaOutput[i]-1)] += 1 \n",
    "\n",
    "\n",
    "            for i in range(int(n_DA_planes)):\n",
    "    \n",
    "                Percent[i,0] = (n_IDs_per_gp[i,0] / n_tot_Id_per_DA[i]) * 100\n",
    "                Percent[i,1] = (n_IDs_per_gp[i,1] / n_tot_Id_per_DA[i]) * 100\n",
    "                #Percent[i,2] = ((n_tot_Id_per_DA[i]  -  (n_IDs_per_gp[i,0] + n_IDs_per_g[i,1])) / n_tot_Id_per_DA[i]) * 100\n",
    "    \n",
    "            #### Using Definition Percentage as Decision Parameter ####\n",
    "\n",
    "            for i in range(Percent.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "    \n",
    "                if (Percent[i,0] >= define_percent):\n",
    "                    n_gp0 = n_gp0 + 1         \n",
    "                    decision[i] = 0\n",
    "                elif (Percent[i,1] >= define_percent):    \n",
    "                    n_gp1 = n_gp1 + 1 \n",
    "                    decision[i] = 1\n",
    "                elif (Percent[i,2] >= define_percent):\n",
    "                    n_gp2 = n_gp2 + 1 \n",
    "                    decision[i] = 2\n",
    "                else:\n",
    "                    n_DA_excluded += 1\n",
    "                    decision[i] = -1\n",
    "            \n",
    "            #### Using decision matrix to determine the number of excluded data\n",
    "                       \n",
    "            for i in range (len (decision)):\n",
    "\n",
    "                if decision[i] == -1:\n",
    "                    \n",
    "                    n_excluded += np.sum(n_IDs_per_gp[i,:])\n",
    "                    \n",
    "        \n",
    "            #### Passing data of well defined DA planes to SelectedData and defining labels\n",
    "\n",
    "            SelectedData = np.zeros((int(SelectedFeatures.shape[0] - n_excluded),int(SelectedFeatures.shape[1])))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            ClassifiersLabel = np.zeros((int(SelectedFeatures.shape[0] - n_excluded)))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            ComparisonLabel = np.zeros((int(y_original.shape[0] - n_excluded)))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            \n",
    "            for i in range (SodaOutput.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "                if decision[int (SodaOutput[i]-1)] != -1:\n",
    "    \n",
    "                    SelectedData[k] = SelectedFeatures[i]\n",
    "                    ClassifiersLabel [k] = decision[int (SodaOutput[i]-1)]\n",
    "                    ComparisonLabel [k] = y_original[i]\n",
    "                    k += 1\n",
    "\n",
    "            for i in range (decision.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "\n",
    "                if decision[i] != -1:\n",
    "                    \n",
    "                    selected_samples[0] += n_IDs_per_gp[i,0]\n",
    "                    selected_samples[1] += n_IDs_per_gp[i,1]      \n",
    "\n",
    "            #### Printing Processed Data, ID's and Percentage\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            #np.savetxt('X_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', SelectedData, delimiter=',')\n",
    "            #np.savetxt('Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', ClassifiersLabel, delimiter=',')\n",
    "            #np.savetxt('Original_Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', ComparisonLabel, delimiter=',')\n",
    "            np.save('X_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', SelectedData)\n",
    "            np.save('Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', ClassifiersLabel)\n",
    "            np.save('Original_Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', ComparisonLabel)\n",
    "            ### Interrupt Thread and recalculate parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            for pp in processing_parameters:\n",
    "                if pp['DistanceType'] == d and pp['Granularity'] == g:\n",
    "                    aux = pp\n",
    "                    break\n",
    "            totaltime = deltatime + aux['Time']\n",
    "            cpu_percent = (mean_cpu + aux['CPUPercent'])/2\n",
    "            \n",
    "            \n",
    "            ### Printig Analitics results\n",
    "            \n",
    "            print(s)\n",
    "            print('Number of data clouds: %d' % n_DA_planes)\n",
    "            print('Number of good tools groups: %d' % n_gp0)\n",
    "            print('Number of worn tools groups: %d' % n_gp1)\n",
    "            print('Number of excluded data clouds: %d' % n_DA_excluded)\n",
    "            print('Number of samples: %d' % int(SodaOutput.shape[0])) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            print('Number of good tools samples: %d' % int(selected_samples[0]))\n",
    "            print('Number of worn tools samples: %d' % int(selected_samples[1]))\n",
    "            print('Number of excluded samples: %d' % n_excluded)\n",
    "            print('Data representation loss: %.2f' % (100-((SelectedData.shape[0] / SelectedFeatures.shape[0]) * 100))) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            print('Analyse execution time: %.6f segundos' % totaltime)\n",
    "            print('Avarage CPU usage: %.2f' % cpu_percent)\n",
    "            print('---------------------------------------------------')\n",
    "            \n",
    "            #### Saving Processed Data, ID's and Percentage\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Grouping_Analyses_path )\n",
    "            \n",
    "            Grouping_Analyse = open(\"Grouping_Analyse_ID_\" + str(DataSetID) + \"_min_\" + str(min_granularity) + \"_max_\" + str(max_granularity) + '_' + str(define_percent) +\"%.txt\",\"w+\")\n",
    "            Grouping_Analyse.write(s)\n",
    "            Grouping_Analyse.write('\\nNumber of data clouds: %d\\n' % n_DA_planes)\n",
    "            Grouping_Analyse.write('Number of good tools groups: %d\\n' % n_gp0)\n",
    "            Grouping_Analyse.write('Number of worn tools groups: %d\\n' % n_gp1)\n",
    "            Grouping_Analyse.write('Number of excluded data clouds: %d\\n' % n_DA_excluded)\n",
    "            Grouping_Analyse.write('Number of samples: %d\\n' % int(SodaOutput.shape[0]))\n",
    "            Grouping_Analyse.write('Number of good tools samples: %d\\n' % int(selected_samples[0]))\n",
    "            Grouping_Analyse.write('Number of worn tools samples: %d\\n' % int(selected_samples[1]))\n",
    "            Grouping_Analyse.write('Number of excluded samples: %d\\n' % n_excluded)\n",
    "            Grouping_Analyse.write('Data representation loss: %.2f\\n' % (100-((SelectedData.shape[0] / SelectedFeatures.shape[0]) * 100))) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            Grouping_Analyse.write('Analyse execution time: %.6f segundos\\n' % totaltime)\n",
    "            Grouping_Analyse.write('Avarage CPU usage: %.2f\\n' % cpu_percent)\n",
    "            Grouping_Analyse.write('---------------------------------------------------')\n",
    "            \n",
    "            Grouping_Analyse.close()\n",
    "    #np.savetxt('Percent.csv',define_percent,delimiter = ',')\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "\n",
    "    np.save(\"define_percent.npy\",define_percent)\n",
    "    \n",
    "    Output = {'Percent': define_percent,\n",
    "              'Distances': distances,\n",
    "              'Pace': pace,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "882l7VYuFQ7s"
   },
   "source": [
    "# Classification Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNNEQZlzrRuC"
   },
   "source": [
    "# .Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR3VB7RRFQ7t"
   },
   "outputs": [],
   "source": [
    "def Classification (ClassificationPar, min_granularity,max_granularity,n_a, plot_matrix=False): #Classifiers\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    add_path1 = \"/Classification/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Classification_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "\n",
    "    # Change to Kernel directory\n",
    "    os.chdir(Kernel_path)\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma='scale'),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "    Output_ID = ClassificationPar['ID']\n",
    "    distances = ClassificationPar['Distances']\n",
    "    pace = ClassificationPar['Pace']\n",
    "    gra = np.arange(min_granularity,max_granularity,pace)\n",
    "\n",
    "    for d in distances:\n",
    "        for g in gra:\n",
    "            try:\n",
    "                # Now change to Kernel directory\n",
    "\n",
    "                os.chdir( Kernel_path )\n",
    "\n",
    "                #retval = os.getcwd()\n",
    "                #print (\"Current working directory %s\" % retval)  \n",
    "                # preprocess dataset, split into training and test part\n",
    "                Accuracy = np.zeros((n_a, len(names)))\n",
    "                #\"Y_60_euclidean_Labels_7_1.25.csv\"\n",
    "                s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.npy'\n",
    "                X = np.load('X_' + s)    \n",
    "                y_soda = np.load('Y_' + s)\n",
    "                y_original = np.load('Original_Y_' + s)\n",
    "\n",
    "                X_train, X_test, y_train_soda, y_test_soda, y_train_original, y_test_original = train_test_split(X, y_soda, y_original, test_size=.4, random_state=42)\n",
    "\n",
    "                #Loop into numbeer od samples\n",
    "                for i in range(Accuracy.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "                    k = 0\n",
    "                    # iterate over classifiers\n",
    "                    for name, clf in zip(names, classifiers):\n",
    "\n",
    "                        clf.fit(X_train, y_train_soda)\n",
    "                        score = clf.score(X_test, y_test_original)\n",
    "                        Accuracy[i,k] = (score*100)\n",
    "                        k +=1\n",
    "                            #if plot_matrix:\n",
    "                             #   ClassifiersLabel = list(clf.predict(X_test))\n",
    "                              #  confusionmatrix(ClassificationPar['ID'], d, g, ClassifiersLabel, 'Classifiers', name, y_test_original,plot=True)\n",
    "                              #  print('Claasificatiojn funfando')\n",
    "\n",
    "\n",
    "                        ClassifiersLabel = list(clf.predict(X_test))\n",
    "                            #confusionmatrix(ClassificationPar['ID'], d, g, ClassifiersLabel, 'Classifiers', name, y_test_original)\n",
    "                    #Creating Matrix for Mean an Std. Derivatio\n",
    "                results = np.zeros((len(names),2))\n",
    "\n",
    "                #Calculinng Mean and Std. Derivation \n",
    "                for i in range(len(names)):\n",
    "                    results[i,0] = round (np.mean(Accuracy[:,i]), 2 )\n",
    "                    results[i,1] = round (np.std(Accuracy[:,i]), 2)\n",
    "\n",
    "                # Now change to Grouping Analyses directory\n",
    "\n",
    "                os.chdir( Classification_path )\n",
    "\n",
    "                    #retval = os.getcwd()\n",
    "                    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "                results = pd.DataFrame(results, index = names, columns = ['Media','Desvio'])       \n",
    "                results.to_csv((\"Classification_result_\" + s) )\n",
    "\n",
    "\n",
    "                print('*** {} - {} - {:.2f}  ***'.format(ClassificationPar['ID'], d, g))\n",
    "                print('-------------------------------------')\n",
    "                print(results)\n",
    "                print(' ')\n",
    "\n",
    "            except:\n",
    "                print('*** {} - {} - {:.2f}  ***'.format(Output_ID, d, g))\n",
    "        \n",
    "    # Now change to base directory\n",
    "\n",
    "    os.chdir( base_path )\n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RNHpxwRre-e"
   },
   "source": [
    "# .Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7AB1utxrfIg"
   },
   "outputs": [],
   "source": [
    "def Model_Train (ClassificationPar,d, Model_Name, g):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.getcwd();\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "    \n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "    for name,clf in zip(names ,classifiers):\n",
    "        \n",
    "        if  name == Model_Name:\n",
    "            \n",
    "            model = clf;\n",
    "            \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    \n",
    "    s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.csv';\n",
    "    X = np.genfromtxt(('X_' + s) , delimiter=',')    \n",
    "    y = np.genfromtxt(('Y_' + s), delimiter=',') \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # save the model to disk\n",
    "\n",
    "    pickle.dump(model, open('model.sav', 'wb'))\n",
    "    \n",
    "    \n",
    "    Output = {'Model': model,\n",
    "              'X': X_test,\n",
    "              'Y': y_test}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "\n",
    "    return Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWrXWuGQrfXq"
   },
   "outputs": [],
   "source": [
    "def Model_Predict (projected_data):\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    model = pickle.load(open('model.sav', 'rb'))\n",
    "    \n",
    "    for i in range (projected_data.shape[0]):\n",
    "        \n",
    "        y_predict = model.predict(projected_data[i,:].reshape(1, -1))\n",
    "    \n",
    "        if y_predict[0] == 0:\n",
    "            print('Ferramenta Boa')\n",
    "        else:\n",
    "            print('Ferramenta Ruim')\n",
    "    \n",
    "        #print ('Label de Teste: %d' % int (projected_data[i]))\n",
    "        print ('Label dado pale NN: %d' % int (y_predict[0]))\n",
    "        print('___________________')\n",
    "        print('                   ')\n",
    "        \n",
    "    # Now change to the base directory\n",
    "    \n",
    "    os.chdir( base_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZFxLQalFQ7y"
   },
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/pi/Documents/Matheus/Lathes_Tool_Project/Model/IPython')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Slicer Control Output\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "output_id = 1\n",
    "\n",
    "D_S_parameters = DataSlicer(output_id,20,'Main Data')\n",
    "\n",
    "ExtractedNames = TSFRESH_Extraction(D_S_parameters) #(Extração de atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SelectedFeatures = TSFRESH_Selection(D_S_parameters,ExtractedNames) # (Parametros e resultados da divisão de dados)\n",
    "\n",
    "ReducedFeatures = PCA_calc(SelectedFeatures,12,'Analytics') # (Feautures selecionadas, numero de PC's a manter, mode ('Test','Calc','Specific', 'Analytics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SODA_parameters, processing_parameters = SODA(ReducedFeatures,2,8.25,0.25) # (Features reduzidas, granularidade mínima, granularidade máxima, passo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ClassificationPar = GroupingAlgorithm(SODA_parameters,80, processing_parameters) # (Labels do SODA, Porcentagem de definição, numero de ID's boas, parametros de processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "Classification (ClassificationPar, 2,8, 1, plot_matrix=False) #(Parametros do data-set,  min_grid_size, max_grid_size, numero de vezes a simular, plotar matriz de confusão (True or False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ModelPar = Model_Train(ClassificationPar,'euclidean',\"Neural Net\",2) #(Parametros da data-set, distância, Modelo\n",
    "                                                                        # granularidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZFxLQalFQ7y"
   },
   "source": [
    "# Raspberry Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dynamic_tsfresh(output_id)\n",
    "\n",
    "projected_data = PCA_projection(features)\n",
    "\n",
    "Model_Predict(projected_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/thiago/Repositories/Lathes_Tool_Project/Model/IPython/')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Unified_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
