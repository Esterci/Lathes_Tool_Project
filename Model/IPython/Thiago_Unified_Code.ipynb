{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "error",
     "timestamp": 1573049995271,
     "user": {
      "displayName": "Thiago Esterci",
      "photoUrl": "",
      "userId": "15369105314376006785"
     },
     "user_tz": 120
    },
    "id": "zz62z0U1FQ6O",
    "outputId": "be52ca7a-9aa4-4dc1-deec-96fe0643de4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "import tsfresh\n",
    "from psutil import cpu_percent\n",
    "from tsfresh import extract_features\n",
    "from tsfresh import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import io\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.spatial.distance import pdist, cdist, squareform\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from tsfresh.feature_extraction import extract_features, ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction.settings import from_columns\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "import os\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Recovery (DataName): #Recovery function \n",
    "\n",
    "    #Changing Work Folder\n",
    "\n",
    "    add_path1 = \"/PCA_Analyses/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd ()\n",
    "    working_path = os.getcwd() + '/Model'\n",
    "    PCA_Analyses_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    \n",
    "    if DataName == 'D_S_parameters':\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "            \n",
    "            Final_Target = np.genfromtxt('FinalTarget.csv', delimiter = ',')\n",
    "            \n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            P_N_groups = int(np.load('M_N_groups.npy'))\n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            P_N_Ids = int(np.load('N_IDs.npy'))\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'FinalTarget': Final_Target,\n",
    "                    'M_N_groups': P_N_groups,\n",
    "                    'ID': Output_Id,\n",
    "                    'N_IDs': P_N_Ids}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "            print(\"D_S_parameters Recovered!\")\n",
    "        \n",
    "            return Output\n",
    "            \n",
    "        except:\n",
    "\n",
    "            print(\"D_S_parameters not recovered =(\" + '\\033[0m')\n",
    "    \n",
    "    elif DataName == 'ExtractedNames':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            extracted_names = np.load('extracted_names.npy')\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "\n",
    "            print(\"ExtractedNames recovered!\")\n",
    "            return extracted_names\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ExtractedNames not recovered =(\" + '\\033[0m')\n",
    "                 \n",
    "    elif DataName == 'SelectedFeatures':\n",
    "\n",
    "        try:    \n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "        \n",
    "            os.chdir( Kernel_path )\n",
    "            \n",
    "            features_filtered_1 = pd.read_csv('features_filtered_' + str(Output_Id) + '.csv') \n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'FeaturesFiltered': features_filtered_1,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "\n",
    "            print(\"SelectedFeatures recovered!\")            \n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"SelectedFeatures not recovered =(\" + '\\033[0m')\n",
    "        \n",
    "    elif DataName == 'ReducedFeatures':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to Recovery directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            \n",
    "            # Now change to PCA Analyses directory\n",
    "        \n",
    "            os.chdir( PCA_Analyses_path )\n",
    "            \n",
    "            features_reduzidas = np.genfromtxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", delimiter=',')\n",
    "            \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( base_path )\n",
    "            \n",
    "            Output = {'ReducedFeatures': features_reduzidas,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Final working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ReducedFeatures recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ReducedFeatures not recovered =(\" + '\\033[0m')\n",
    "        \n",
    "    elif DataName == 'SODA_parameters_processing_parameters':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path )\n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            processing_parameters = np.load(('processing_parameters.npy'), allow_pickle=True) \n",
    "            processing_parameters = processing_parameters.tolist() \n",
    "            distances = np.load(('distances.npy'), allow_pickle=True) \n",
    "            distances = distances.tolist() \n",
    "            min_granularity = np.load('Min_g.npy') \n",
    "            max_granularity = np.load('Max_g.npy') \n",
    "            pace = np.load('Pace.npy') \n",
    "\n",
    "            Output = {'Distances': distances,\n",
    "                    'Min_g': min_granularity,\n",
    "                    'Max_g': max_granularity,\n",
    "                    'Pace': pace,\n",
    "                    'ID': Output_Id}\n",
    "\n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            print(\"SODA_parameters_processing_parameters recovered!\")\n",
    "            return Output, processing_parameters\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"SODA_parameters_processing_parameters not recovered =(\" + '\\033[0m')\n",
    "                \n",
    "    elif DataName == 'ClassificationPar':\n",
    "\n",
    "        try:\n",
    "        \n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            Output_Id = int(np.load('ID.npy'))\n",
    "            pace = np.load(\"Pace.npy\")\n",
    "            distances = np.load(('distances.npy'), allow_pickle=True) \n",
    "            distances = distances.tolist() \n",
    "            define_percent = np.load('define_percent.npy')\n",
    "            \n",
    "            Output = {'Percent': define_percent,\n",
    "                    'Distances': distances,\n",
    "                    'Pace': pace,\n",
    "                    'ID': Output_Id}\n",
    "            \n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ClassificationPar recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ClassificationPar not recovered =(\" + '\\033[0m')    \n",
    "\n",
    "    elif DataName == 'ModelPar':\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Now change to base directory\n",
    "        \n",
    "            os.chdir( Recovery_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            # load the model from disk\n",
    "            model = pickle.load(open(\"Model.sav\", 'rb'))\n",
    "            X_test = np.load('X_test.npy') \n",
    "            y_test = np.load('y_test.npy') \n",
    "\n",
    "            Output = {'Model': model,\n",
    "                    'X': X_test,\n",
    "                    'Y': y_test}\n",
    "            \n",
    "            # Now change to base directory\n",
    "\n",
    "            os.chdir( base_path ) \n",
    "\n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            print(\"ModelPar recovered!\")\n",
    "            return Output\n",
    "\n",
    "        except:\n",
    "            print('\\033[93m' + \"ModelPar not recovered =(\" + '\\033[0m')   \n",
    "        \n",
    "    else:\n",
    "        print('\\033[93m' + \"Wrong name lad/lass, please check de Recovery input\" + '\\033[0m')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovery Control Output\n",
      "----------------------------------\n",
      "D_S_parameters not recovered =(\u001b[0m\n",
      "\u001b[93mExtractedNames not recovered =(\u001b[0m\n",
      "\u001b[93mSelectedFeatures not recovered =(\u001b[0m\n",
      "\u001b[93mReducedFeatures not recovered =(\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-016b9c769363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mReducedFeatures\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mRecovery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ReducedFeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mOutput_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_S_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The current Data ID is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print('Recovery Control Output')\n",
    "print('----------------------------------')\n",
    "\n",
    "D_S_parameters =  Recovery('D_S_parameters') \n",
    "ExtractedNames =  Recovery('ExtractedNames') \n",
    "SelectedFeatures =  Recovery('SelectedFeatures') \n",
    "ReducedFeatures =  Recovery('ReducedFeatures') \n",
    "    \n",
    "Output_ID = int(D_S_parameters['ID'])\n",
    "\n",
    "print('The current Data ID is ', Output_ID)\n",
    "    \n",
    "print('__________________________________________')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X, x_min, x_max): #Normalization\n",
    "\n",
    "    nom = (X-X.min(axis=0))*(x_max-x_min)\n",
    "    denom = X.max(axis=0) - X.min(axis=0)\n",
    "    if denom==0:\n",
    "        denom = 1\n",
    "    return x_min + nom/denom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# .Plot Formater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_func(value, tick_number): #Plot Formater\n",
    "    # find number of multiples of pi/2\n",
    "    N = int(value)\n",
    "    if N == 0:\n",
    "        return \"X1\"\n",
    "    elif N == 50:\n",
    "        return \"X50\"\n",
    "    elif N == 100:\n",
    "        return \"X100\"\n",
    "    elif N == 150:\n",
    "        return \"X150\"\n",
    "    elif N == 200:\n",
    "        return \"X200\"\n",
    "    elif N == 250:\n",
    "        return \"X250\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-61BMUFZFQ6X"
   },
   "source": [
    "# Feature Extraction/Selection Module\n",
    "    .Data Slicer for saving RAM;\n",
    "    .TSFRESH feature extraction and selection;\n",
    "    .PCA dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sieut0YSFQ6Z"
   },
   "source": [
    "# . Data Slicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCrplMylFQ6a"
   },
   "outputs": [],
   "source": [
    "def DataSlicer (Output_Id, id_per_group, Choice): #Data Slicer\n",
    "    \n",
    "    print('Data Slicer Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Input/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Input_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "     \n",
    "    # Now change to Input directory\n",
    "    \n",
    "    os.chdir( Input_path )\n",
    "    \n",
    "    # Loading the required input \n",
    "    \n",
    "    Full_data = np.genfromtxt('Output_' + str(int(Output_Id)) + '.csv', delimiter=',')\n",
    "    #E_data = np.genfromtxt('Eminence_Data_' + str(Output_Id) + '.csv', delimiter=',')\n",
    "    columns = Full_data.shape[1]\n",
    "    data = Full_data[:,2:columns-1]\n",
    "    info = Full_data[:,0:2]\n",
    "    #centralizar os dados e colocá-los com desvioPadrão=1\n",
    "    scaler = StandardScaler().fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    P_data = np.concatenate((info,data), axis=1)\n",
    "    \n",
    "    Target = Full_data[:,columns-1]\n",
    "\n",
    "    print('Full Matrix: ' + str(Full_data.shape))\n",
    "    print('Main Data: ' + str(P_data.shape))\n",
    "    print('Labels: ' + str(Target.shape))\n",
    "    #print('Eminence Data: ' + str(E_data.shape))\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "          \n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                     ProDiMes Slicing Parameters                      ###\n",
    "\n",
    "\n",
    "    P_N_Ids = int(np.amax(P_data,axis=0)[0])\n",
    "    P_N_voos = int(np.amax(P_data,axis=0)[1])\n",
    "    P_last_group = int(P_N_Ids % id_per_group)\n",
    "\n",
    "    if P_last_group != 0:\n",
    "        P_N_groups = int((P_N_Ids / id_per_group) + 1)\n",
    "    else:\n",
    "        P_N_groups = int (P_N_Ids / id_per_group)\n",
    "\n",
    "    print ('Main data Number of Ids: ' + str(P_N_Ids ))\n",
    "    print ('Main data Number of mesures: ' + str(P_N_voos ))\n",
    "    print ('Main data Number of groups: ' + str(P_N_groups ))\n",
    "    print ('Main data Last group: ' + str(P_last_group ))\n",
    "    print ('___________________________________________')\n",
    "\n",
    "    ###______________________________________________________________________###\n",
    "    ###                    Eminences Slicing Parameters                      ###\n",
    "\n",
    "    #E_N_Ids = int(np.amax(E_data,axis=0)[0] - np.amax(P_data,axis=0)[0])\n",
    "    #E_N_voos = int(np.amax(E_data,axis=0)[1]) + 1\n",
    "    #E_last_group = int(E_N_Ids % id_per_group)\n",
    "\n",
    "    #if (E_last_group != 0):\n",
    "    #    E_N_groups = int((E_N_Ids / id_per_group) + 1)\n",
    "    #else:\n",
    "    #    E_N_groups = int (E_N_Ids / id_per_group)\n",
    "\n",
    "    #print ('Eminences Number of Ids: ' + str(E_N_Ids ))\n",
    "    #print ('Eminences Number of flights: ' + str(E_N_voos ))\n",
    "    #print ('Eminences Number of groups: ' + str(E_N_groups ))\n",
    "    #print ('Eminences Last group: ' + str(E_last_group ))\n",
    "\n",
    "\n",
    "    ### Formating Final Target ###\n",
    "\n",
    "    Final_Target = np.zeros((P_N_Ids))\n",
    "\n",
    "    for i in range (P_N_Ids):\n",
    "\n",
    "        Final_Target[i] = Target [i*P_N_voos]\n",
    "        \n",
    "    #np.savetxt(('Target_' + str(int(Output_Id)) + '.csv'), Final_Target, delimiter = ',')\n",
    "    \n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                      Slicing Prodimes Data                           ###\n",
    "\n",
    "    if (Choice =='Main Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (P_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * P_N_voos),columns-1))\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (P_N_voos):\n",
    "            \n",
    "                    if (i  < (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "\n",
    "                    elif (P_last_group == 0) and (i == (P_N_groups - 1)):\n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "            \n",
    "            if (P_last_group != 0) and (i == (P_N_groups - 1)):     \n",
    "\n",
    "                Data = np.zeros(((P_last_group * P_N_voos),columns-1))\n",
    "            \n",
    "                for j in range (P_last_group):\n",
    "    \n",
    "                    for k in range (P_N_voos):\n",
    "    \n",
    "                        Data[(j * P_N_voos) + k,:] = P_data [(((i * id_per_group + j) * P_N_voos) + k ) ,:]\n",
    "        \n",
    "            np.savetxt(('Data_' + str(i) + '.csv'), Data, delimiter = ',')\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Slicing Eminences                           ###\n",
    "    '''\n",
    "    if (Choice == 'Eminence Data') or (Choice =='All'):\n",
    "    \n",
    "        for i in range (E_N_groups):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
    "        \n",
    "            Data = np.zeros(((id_per_group * E_N_voos),columns-3))\n",
    "        \n",
    "            for j in range (id_per_group):\n",
    "            \n",
    "                for k in range (E_N_voos):\n",
    "            \n",
    "                    if (i  < (E_N_groups - 1)):\n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:]\n",
    "                \n",
    "        \n",
    "            if (E_last_group != 0) and (i == (E_N_groups - 1)):\n",
    "            \n",
    "                Data = np.zeros(((E_last_group * E_N_voos),columns-3))\n",
    "            \n",
    "                for j in range (E_last_group):\n",
    "    \n",
    "                    for k in range (E_N_voos):\n",
    "    \n",
    "                        Data[(j * E_N_voos) + k,:] = E_data [(((i * id_per_group + j) * E_N_voos) + k ) ,:]\n",
    "    \n",
    "    \n",
    "            np.savetxt(('Eminence_' + str(i) + '.csv'), Data, delimiter = ',')\n",
    "    '''\n",
    "\n",
    "    np.savetxt(('FinalTarget.csv'), Final_Target, delimiter = ',')\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "          \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    np.save(('M_N_groups.npy'), P_N_groups)\n",
    "    np.save(('ID.npy'), Output_Id)\n",
    "    np.save(('N_IDs.npy'), P_N_Ids)\n",
    "    \n",
    "    # Now change back to Base directory\n",
    "          \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    Output = {'FinalTarget': Final_Target,\n",
    "              'M_N_groups': P_N_groups,\n",
    "              'ID': Output_Id,\n",
    "              'N_IDs': P_N_Ids}\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Slicer Control Output\n",
      "----------------------------------\n",
      "Full Matrix: (9250, 12)\n",
      "Main Data: (9250, 11)\n",
      "Labels: (9250,)\n",
      "Main data Number of Ids: 37\n",
      "Main data Number of mesures: 250\n",
      "Main data Number of groups: 2\n",
      "Main data Last group: 17\n",
      "___________________________________________\n"
     ]
    }
   ],
   "source": [
    "D_S_parameters = DataSlicer(3,20,'Main Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_Xgs4xlFQ6d"
   },
   "source": [
    "# .TSFRESH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Extraction(D_S_parameters): #TSFRESH Extraction\n",
    "    \n",
    "    print('             ')\n",
    "    print('TSFRESH Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "        \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    ###______________________________________________________________________###\n",
    "    ###                         Feature Extraction                           ###\n",
    "\n",
    "    #E_N_groups = np.load('E_N_groups.npy')\n",
    "    P_N_groups = D_S_parameters['M_N_groups']\n",
    "    \n",
    "    for i in range(P_N_groups):\n",
    "        \n",
    "        Data = np.genfromtxt('Data_' + str(i) + '.csv', delimiter=',')\n",
    "        data = pd.DataFrame(Data, columns= ['id','time'] + ['Sensor_' + str(x) for x in range(1,(Data.shape[1]-1))])\n",
    "        \n",
    "        Data_extracted_features = extract_features(data,column_id = \"id\", column_sort=\"time\")\n",
    "        extracted_names = list(Data_extracted_features.columns)\n",
    "        np.savetxt('Data_Features_' + str(i) + '.csv', Data_extracted_features.values, delimiter=',')\n",
    "        \n",
    "    #for i in range(E_N_groups):\n",
    "\n",
    "    \n",
    "    #    data = pd.DataFrame(np.genfromtxt('Eminence_' + str(i) + '.csv', delimiter=','), \n",
    "    #                        columns= ['id','time','sensor_1','sensor_2','sensor_3','sensor_4',\n",
    "    #                                            'sensor_5','sensor_6','sensor_7'])\n",
    "    #    extracted_features = extract_features(data, column_id = \"id\", column_sort=\"time\")\n",
    "    #    np.savetxt('Eminence_Features_' + str(i) + '.csv', extracted_features, delimiter=',')\n",
    "    \n",
    "    # Now change to Recovery directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    np.save('extracted_names.npy',extracted_names)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return extracted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1V_-NhwFQ6e"
   },
   "outputs": [],
   "source": [
    "def TSFRESH_Selection(D_S_parameters,extracted_names): #TSFRESH Selection\n",
    "    ###______________________________________________________________________###\n",
    "    ###                          Feature Selection                           ###\n",
    "    \n",
    "    P_N_groups = D_S_parameters['M_N_groups']\n",
    "    Output_Id = D_S_parameters['ID']\n",
    "    y = D_S_parameters['FinalTarget']\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "        \n",
    "    # Now change back to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    Data_Matrix = np.genfromtxt('Data_Features_0.csv', delimiter=',')\n",
    "    print('Data_Features_0.csv')\n",
    "\n",
    "    for i in range(1,P_N_groups):\n",
    "    \n",
    "        new_data = np.genfromtxt('Data_Features_' + str(i) + '.csv', delimiter=',') \n",
    "        \n",
    "        print('Data_Features_' + str(i) + '.csv')\n",
    "\n",
    "        Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0)\n",
    "\n",
    "    \n",
    "    #for i in range(E_N_groups):\n",
    "    \n",
    "    #    new_data = np.genfromtxt('Eminence_Features_' + str(i) + '.csv', delimiter=',') \n",
    "    \n",
    "    #    Data_Matrix = np.concatenate((Data_Matrix, new_data), axis=0)\n",
    "    \n",
    "    #    print('Eminence_Features_' + str(i) + '.csv')\n",
    "    \n",
    "    features = pd.DataFrame(Data_Matrix, columns= extracted_names)\n",
    "    \n",
    "    impute(features)\n",
    "    features_filtered_1 = select_features(features, y)\n",
    "    features_filtered_1.sort_index(inplace = True)\n",
    "    \n",
    "    features_filtered_1.to_csv('features_filtered_' + str(Output_Id) + '.csv', index=False)\n",
    "    \n",
    "    # Now change back to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    Output = {'FeaturesFiltered': features_filtered_1,\n",
    "              'FinalTarget': y,\n",
    "              'ID': Output_Id}\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_tsfresh ():\n",
    "\n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/Input/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Input_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    \n",
    "    # Change folder to Kernel\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    # Load the the filtered features from the seed data-set\n",
    "\n",
    "    features_filtered = pd.read_csv('features_filtered_3.csv')\n",
    "\n",
    "    # Extract the useful information of it\n",
    "\n",
    "    columns = np.array(features_filtered.columns)\n",
    "    kind_to_fc_parameters = tsfresh.feature_extraction.settings.from_columns(features_filtered.columns)\n",
    "\n",
    "    sensors_names = [None] * int(features_filtered.shape[1]);\n",
    "\n",
    "\n",
    "    for i in range (columns.shape[0]):\n",
    "        name = columns[i]\n",
    "        c = '__';\n",
    "        words = name.split(c)\n",
    "\n",
    "        sensors_names[i] = words[0]\n",
    "\n",
    "        '''if i < 20:\n",
    "\n",
    "            print(name)\n",
    "            print(words)\n",
    "            print(features_names[i])\n",
    "            print(sensors_names[i])\n",
    "            print('_______')'''\n",
    "\n",
    "    columns = columns.tolist()\n",
    "    unique_sensors_names = np.unique(np.array(sensors_names))\n",
    "\n",
    "    # Change folder to Input\n",
    "\n",
    "    os.chdir( Input_path )\n",
    "\n",
    "    # Load the incoming data\n",
    "    \n",
    "    Data = np.genfromtxt('Output_3.csv', delimiter=',')\n",
    "    data_frame = pd.DataFrame(Data[:,0:11], columns= ['id','time'] + ['Sensor_' + str(x) for x in range(1,(Data.shape[1]-2))])\n",
    "\n",
    "    # Feature extraction guided by the seed data-set\n",
    "\n",
    "    extraction_df = pd.DataFrame(data_frame.loc[::,'id':unique_sensors_names[0]].values,columns= ['id','time','Sensor'])\n",
    "    #print(extraction_df.head())\n",
    "    arrayList = [] \n",
    "\n",
    "    for sensor in unique_sensors_names:\n",
    "        \n",
    "        #print(extraction_df.head())\n",
    "        #print('_____')\n",
    "        extraction_df.loc[::,'Sensor'] = data_frame.loc[::,sensor]\n",
    "        \n",
    "        #print(extraction_df.head())\n",
    "        #print('_____')\n",
    "        \n",
    "        extraction_df = extraction_df.rename(columns={'Sensor': sensor})\n",
    "        \n",
    "        tsfresh_parameters = kind_to_fc_parameters[sensor]\n",
    "        \n",
    "        extracted_features = extract_features(extraction_df, column_id=\"id\", column_sort=\"time\", default_fc_parameters=tsfresh_parameters)\n",
    "\n",
    "        arrayList.append(extracted_features)\n",
    "\n",
    "        extraction_df = extraction_df.rename(columns={sensor : 'Sensor'})    \n",
    "\n",
    "    original_space_features = pd.concat(arrayList,axis=1)\n",
    "\n",
    "    # Sort the features in accordance with the seed data-set\n",
    "    \n",
    "    original_space_features = original_space_features[columns]\n",
    "    impute(original_space_features)\n",
    "    original_space_features.sort_index(inplace = True)\n",
    "\n",
    "    # Change folder to origin\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return original_space_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jS1d7dxeFQ6j"
   },
   "source": [
    "# . PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_calc (SelectedFeatures,N_PCs,Chose):#PCA\n",
    "    \n",
    "    #%matplotlib%\n",
    "\n",
    "    if (Chose == 'Test') or (Chose == 'Calc') or (Chose == 'Specific') or (Chose == 'Analytics'):\n",
    "        \n",
    "        #Changing Work Folder\n",
    "        \n",
    "        add_path1 = \"/PCA_Analyses/\"\n",
    "        add_path2 = \"/Input/\"\n",
    "        add_path3 = \"/.Kernel/\"\n",
    "        add_path4 = \"/PCA_Analyses/Figures/\"        \n",
    "        base_path = os.getcwd()\n",
    "        working_path = os.getcwd()\n",
    "        PCA_Analyses_path = working_path + add_path1\n",
    "        Input_path = working_path + add_path2\n",
    "        Kernel_path = working_path + add_path3\n",
    "        PCA_Figures_path = working_path + add_path4\n",
    "        \n",
    "        # Now change to PCA Figures directory\n",
    "\n",
    "        os.chdir( Kernel_path )\n",
    "        \n",
    "        print('             ')\n",
    "        print('PCA Control Output')\n",
    "        print('----------------------------------')\n",
    "\n",
    "        Output_Id = SelectedFeatures['ID']\n",
    "        features = SelectedFeatures['FeaturesFiltered']\n",
    "        Target = SelectedFeatures['FinalTarget']\n",
    "        selected_names = list(features.columns)\n",
    "\n",
    "        #centralizar os dados e colocá-los com desvioPadrão=1\n",
    "        scaler = StandardScaler().fit(features)\n",
    "        features_padronizadas = scaler.transform(features)\n",
    "        #features_padronizadas = pd.DataFrame(features_padronizadas)\n",
    "\n",
    "        pca= PCA(n_components = N_PCs)\n",
    "        pca.fit(features_padronizadas)\n",
    "        \n",
    "        # save the model to disk\n",
    "\n",
    "        pickle.dump(pca, open('pca.sav', 'wb'))\n",
    "        \n",
    "        variacao_percentual_pca = np.round(pca.explained_variance_ratio_ * 100, decimals = 2)\n",
    "        \n",
    "        # Now change to PCA Figures directory\n",
    "        \n",
    "        fig = plt.figure(figsize=[16,8])\n",
    "        ax = fig.subplots(1,1)\n",
    "        ax.bar(x=['PC' + str(x) for x in range(1,(N_PCs+1))],height=variacao_percentual_pca[0:N_PCs])\n",
    "\n",
    "        ax.set_ylabel('Percentage of Variance Held',fontsize=20)\n",
    "        ax.set_xlabel('Principal Components',fontsize=20)\n",
    "        ax.tick_params(axis='x', labelsize=14)\n",
    "        ax.tick_params(axis='y', labelsize=18)\n",
    "        ax.grid()\n",
    "        plt.show()\n",
    "        fig.savefig('Percentage_of_Variance_Held_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "        print('Variation maintained: %.2f' % variacao_percentual_pca.sum())\n",
    "        print('                  ')\n",
    "\n",
    "        if (Chose != 'Test'):\n",
    "            features_reduzidas = pca.transform(features)\n",
    "            print('Filtered Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_padronizadas,0))\n",
    "            print(np.size(features_padronizadas,1))\n",
    "            print('-' * 20)\n",
    "            print('Reduced Features')\n",
    "            print('-' * 20)\n",
    "            print(np.size(features_reduzidas,0))\n",
    "            print(np.size(features_reduzidas,1))\n",
    "\n",
    "            if (Chose != 'Test'):\n",
    "                \n",
    "                ### Análise de atributos ###\n",
    "\n",
    "\n",
    "                eigen_matrix = np.array(pca.components_)\n",
    "                eigen_matrix = pow((pow(eigen_matrix,2)),0.5) #invertendo valores negativos\n",
    "\n",
    "                for i in range (eigen_matrix.shape[0]):\n",
    "\n",
    "                    LineSum = sum(eigen_matrix[i,:])\n",
    "                    for j in range (eigen_matrix.shape[1]):\n",
    "                        eigen_matrix[i,j] = ((eigen_matrix[i,j]*100)/LineSum)\n",
    "\n",
    "\n",
    "                if Chose == 'Specific':\n",
    "                ### Análise Expecífica ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,int(8*N_PCs)])\n",
    "\n",
    "                    fig.suptitle('Contribution percentage per PC', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(int(N_PCs),1)\n",
    "\n",
    "                    for i in range (int(N_PCs)):\n",
    "\n",
    "                        s = eigen_matrix[i,:]\n",
    "\n",
    "                        ax[i].bar(x=range(0,(eigen_matrix.shape[1])),height=s)\n",
    "                        ax[i].set(xlabel='Features', ylabel='Contribution Percentage', title = 'PC ' + str(i+1))\n",
    "                        ax[i].grid()\n",
    "\n",
    "\n",
    "                    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "                    for axs in ax.flat:\n",
    "                        axs.label_outer()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Contribution_Percentage_Per_PC_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                if (Chose == 'Analytics'):\n",
    "                    ### Análise Geral ###\n",
    "\n",
    "                    weighted_contribution = np.zeros((2,eigen_matrix.shape[1]))\n",
    "\n",
    "                    for i in range (eigen_matrix.shape[1]):\n",
    "                        NumeratorSum = 0\n",
    "                        for j in range (N_PCs):\n",
    "                            NumeratorSum += eigen_matrix[j,i] * variacao_percentual_pca[j]\n",
    "\n",
    "                        weighted_contribution[0,i] = NumeratorSum / sum(variacao_percentual_pca)\n",
    "\n",
    "                    df_weighted_contribution = pd.DataFrame(weighted_contribution,columns=selected_names)\n",
    "                    df_weighted_contribution = df_weighted_contribution.drop([1])                    \n",
    "                    df_weighted_contribution = df_weighted_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    \n",
    "                    \n",
    "                    #pd.set_option('display.max_rows', len(df_weighted_contribution))\n",
    "                    #print(type(df_weighted_contribution))\n",
    "                    #print(df_weighted_contribution.head())\n",
    "                    #pd.reset_option('display.max_rows')\n",
    "\n",
    "                    #Creating Separated Data Frames por Sensors and Features Contribution \n",
    "\n",
    "                    sensors_names = [None] * int(df_weighted_contribution.shape[1])\n",
    "                    features_names = [None] * int(df_weighted_contribution.shape[1])\n",
    "                    general_features = [None] * int(df_weighted_contribution.shape[1])\n",
    "\n",
    "\n",
    "                    for i, names in zip(range (df_weighted_contribution.shape[1]), df_weighted_contribution.columns):\n",
    "\n",
    "                        c = '__'\n",
    "                        words = names.split(c)\n",
    "                        \n",
    "                        sensors_names[i] = words[0]\n",
    "                        general_features[i]= words[1]\n",
    "                        features_names[i] = c.join(words[1:])\n",
    "\n",
    "                        #print(names)\n",
    "                        #print(words)\n",
    "                        #print(sensors_names[i])\n",
    "                        #print(features_names[i])\n",
    "                        #print(50*'-')\n",
    "\n",
    "                    \n",
    "                    unique_sensors_names = np.ndarray.tolist(np.unique(np.array(sensors_names))) \n",
    "                    unique_general_feature = np.ndarray.tolist(np.unique(np.array(general_features))) \n",
    "                    unique_features_names = np.ndarray.tolist(np.unique(np.array(features_names)))\n",
    "                    sensors_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_sensors_names)[0])), columns=unique_sensors_names)\n",
    "                    general_features_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_general_feature)[0])), columns=unique_general_feature)\n",
    "                    features_contribution = pd.DataFrame (np.zeros((2,np.shape(unique_features_names)[0])), columns=unique_features_names)\n",
    "                    sensors_contribution = sensors_contribution.drop([1])\n",
    "                    general_features_contribution = general_features_contribution.drop([1])\n",
    "                    features_contribution = features_contribution.drop([1])\n",
    "                    \n",
    "                    \n",
    "                    # For the output Formating\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    unique_sensors_names = np.ndarray.tolist(np.unique(np.array(sensors_names))) \n",
    "                    unique_features_names = np.ndarray.tolist(np.unique(np.array(features_names)))\n",
    "                    sensor_dt = np.transpose(np.vstack((unique_sensors_names,np.asarray(np.zeros(np.shape(unique_sensors_names)[0]),object))))\n",
    "                    feature_dt = np.transpose(np.vstack((unique_features_names,np.asarray(np.zeros(np.shape(unique_features_names)[0]),object))))\n",
    "                    sensors_contribution = pd.DataFrame(sensor_dt,columns = ['Sensor','Contribution'])\n",
    "                    features_contribution = pd.DataFrame(feature_dt,columns = ['Feature','Contribution'])\n",
    "                    \"\"\"\n",
    "                    #print(sensors_contribution.head())\n",
    "                    #print(features_contribution.head())\n",
    "\n",
    "                    #Creating dictionaries form Data Frame orientation\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    Creates a mapping from kind names to fc_parameters objects\n",
    "                    (which are itself mappings from feature calculators to settings)\n",
    "                    to extract only the features contained in the columns.\n",
    "                    To do so, for every feature name in columns this method\n",
    "\n",
    "                    1. split the column name into col, feature, params part\n",
    "                    2. decide which feature we are dealing with (aggregate with/without params or apply)\n",
    "                    3. add it to the new name_to_function dict\n",
    "                    4. set up the params\n",
    "\n",
    "                    :param columns: containing the feature names\n",
    "                    :type columns: list of str\n",
    "                    :param columns_to_ignore: columns which do not contain tsfresh feature names\n",
    "                    :type columns_to_ignore: list of str\n",
    "\n",
    "                    :return: The kind_to_fc_parameters object ready to be used in the extract_features function.\n",
    "                    :rtype: dict\n",
    "                    \"\"\"\n",
    "\n",
    "                    weighted_contribution_dic = {}\n",
    "\n",
    "                    for col in df_weighted_contribution.columns:\n",
    "\n",
    "                        # Split according to our separator into <col_name>, <feature_name>, <feature_params>\n",
    "                        parts = col.split('__')\n",
    "                        n_parts = len(parts)\n",
    "\n",
    "                        if n_parts == 1:\n",
    "                            raise ValueError(\"Splitting of columnname {} resulted in only one part.\".format(col))\n",
    "\n",
    "                        kind = parts[0]\n",
    "                        feature = c.join(parts[1:])\n",
    "                        feature_name = parts[1]\n",
    "\n",
    "                        if kind not in weighted_contribution_dic:\n",
    "                            weighted_contribution_dic[kind] = {}\n",
    "\n",
    "                        if not hasattr(feature_calculators, feature_name):\n",
    "                            raise ValueError(\"Unknown feature name {}\".format(feature_name))\n",
    "                            \n",
    "                        sensors_contribution.loc[0,kind] += df_weighted_contribution.loc[0,col]\n",
    "                        general_features_contribution.loc[0,feature_name] += df_weighted_contribution.loc[0,col]\n",
    "                        features_contribution.loc[0,feature] += df_weighted_contribution.loc[0,col]\n",
    "                        weighted_contribution_dic[kind][feature] = df_weighted_contribution.loc[0,col]\n",
    "                    \n",
    "                        \n",
    "                    # End of the tsfresh stolen function\n",
    "\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    sensors_dic = {}\n",
    "                    for i in range(len(unique_sensors_names)):\n",
    "                        sensors_dic[unique_sensors_names[i]] = i\n",
    "\n",
    "                    features_dic = {}\n",
    "                    for i in range(len(unique_features_names)):\n",
    "                        features_dic[unique_features_names[i]] = i\n",
    "\n",
    "                    #Suming the contibution for Sensors and Features\n",
    "\n",
    "                    for i in range(df_weighted_contribution.shape[0]):\n",
    "\n",
    "                        names = df_weighted_contribution.loc[i,'tsfresh_info']\n",
    "                        c = '__'\n",
    "                        words = names.split(c)           \n",
    "                        S= words[0]\n",
    "                        F= c.join(words[1:])\n",
    "\n",
    "                        sensors_contribution.loc[sensors_dic[S],'Contribution'] += df_weighted_contribution.loc[i,'Contribution']\n",
    "                        features_contribution.loc[features_dic[F],'Contribution'] += df_weighted_contribution.loc[i,'Contribution']\n",
    "\n",
    "                    sensors_contribution = sensors_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "                    features_contribution = features_contribution.sort_values(by=['Contribution'], ascending=False)\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    features_contribution = features_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    general_features_contribution = general_features_contribution.sort_values(by=0, axis=1, ascending=False)\n",
    "                    \n",
    "                    features_indexes = [x for x in range(1,(features_contribution.shape[0])+1)]\n",
    "                    general_features_indexes = [x for x in range(1,(general_features_contribution.shape[0])+1)]\n",
    "\n",
    "                    features_contribution.set_index(pd.Index(features_indexes))\n",
    "                    general_features_contribution.set_index(pd.Index(general_features_indexes))\n",
    "                    \n",
    "                    sorted_sensors_contribution = sensors_contribution.values[0,:]\n",
    "                    sorted_features_contribution = features_contribution.values[0,:]\n",
    "                    sorted_general_features_contribution = general_features_contribution.values[0,:]\n",
    "\n",
    "                    #Ploting Cntribution Sensors Results\n",
    "                    \n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Sensors Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_sensors_contribution[:]\n",
    "\n",
    "                    ax.bar(x=sensors_contribution.columns,height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Sensors',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Sensor_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "                    #Ploting Cntribution Features Results\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[:]\n",
    "\n",
    "                    ax.bar(x=range(0,(sorted_features_contribution.shape[0])),height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.xaxis.set_major_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_minor_locator(plt.MultipleLocator(50))\n",
    "                    ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('Features_Weighted_Contribution_Percentage_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    ### Análise Geral para os 20 melhores atributos completos ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Best Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    #print('Porcentagem de pertinência: ', np.sum(sorted_features_contribution[0:140]))\n",
    "                    #print('Number of Selected Features: ', sorted_features_contribution.shape[0])\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[0:20]\n",
    "\n",
    "                    ax.bar(x=['X' + str(x) for x in range(1,(20+1))],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('{}th_Best_Features_Weighted_Contribution_Percentage_{}.png'.format(20,Output_Id), bbox_inches='tight')\n",
    "\n",
    "                    ### Análise Geral para os 20 melhores atributos gerais ###\n",
    "\n",
    "                    fig = plt.figure(figsize=[16,8])\n",
    "\n",
    "                    #fig.suptitle('Best Features Weighted Contribution Percentage', fontsize=16)\n",
    "\n",
    "                    #print('Porcentagem de pertinência: ', np.sum(sorted_features_contribution[0:140]))\n",
    "                    #print('Number of Selected Features: ', sorted_features_contribution.shape[0])\n",
    "\n",
    "                    ax = fig.subplots(1,1)\n",
    "\n",
    "                    s = sorted_features_contribution[0:20]\n",
    "\n",
    "                    ax.bar(x=['X' + str(x) for x in range(1,(20+1))],height=s)\n",
    "                    plt.ylabel('Relevance Percentage',fontsize = 20)\n",
    "                    plt.xlabel('General Features',fontsize = 20)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=18)\n",
    "                    ax.grid()\n",
    "\n",
    "                    plt.show()\n",
    "                    fig.savefig('{}th_Best_Features_Weighted_Contribution_Percentage_{}.png'.format(20,Output_Id), bbox_inches='tight')\n",
    "\n",
    "\n",
    "                    #Ploting the data of the most relevant sensor with the best features\n",
    "\n",
    "                    sensors_contribution.values[:,0]\n",
    "\n",
    "                    name_1 = df_weighted_contribution.columns[0]\n",
    "                    name_2 = df_weighted_contribution.columns[1]\n",
    "                    name_3 = df_weighted_contribution.columns[2]\n",
    "\n",
    "\n",
    "                    #pd.set_option('display.max_columns', len(features))\n",
    "                    #print(features)\n",
    "                    #pd.reset_option('display.max_columns')\n",
    "\n",
    "                    x = features.loc[:,name_1].values\n",
    "                    y = features.loc[:,name_2].values\n",
    "                    z = features.loc[:,name_3].values\n",
    "\n",
    "\n",
    "                    x = scale(x,-1,1)\n",
    "                    y = scale(y,-1,1)\n",
    "                    z = scale(z,-1,1)\n",
    "\n",
    "                    x_bom=[]\n",
    "                    x_ruim=[]\n",
    "                    y_bom=[]\n",
    "                    y_ruim=[]\n",
    "                    z_bom=[]\n",
    "                    z_ruim=[]\n",
    "                    \n",
    "                    for i in range(len(Target)):\n",
    "                        if Target[i] == 0:\n",
    "                            x_bom.append(x[i])\n",
    "                            y_bom.append(y[i])\n",
    "                            z_bom.append(z[i])\n",
    "                        if Target[i] == 1:\n",
    "                            x_ruim.append(x[i])\n",
    "                            y_ruim.append(y[i])\n",
    "                            z_ruim.append(z[i])\n",
    "                            \n",
    "                    os.chdir( base_path )\n",
    "                            \n",
    "                    np.savetxt('x_bom.csv', x_bom, delimiter=',')\n",
    "                    np.savetxt('x_ruim.csv', x_ruim, delimiter=',')\n",
    "                    \n",
    "                    os.chdir( PCA_Figures_path )\n",
    "\n",
    "                    fig = plt.figure(figsize=[14,10])\n",
    "                    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "                    ax.scatter(x_bom, y_bom, z_bom, c = 'blue' )\n",
    "                    ax.scatter(x_ruim, y_ruim, z_ruim, c = 'red' )\n",
    "\n",
    "                    plt.ylabel('X2',fontsize = 20,labelpad=18)\n",
    "                    plt.xlabel('X1',fontsize = 20, labelpad=18)\n",
    "                    ax.set_zlabel('X3', fontsize = 20, labelpad=12)\n",
    "                    plt.tick_params(axis='x', labelsize=16)\n",
    "                    plt.tick_params(axis='y', labelsize=16)\n",
    "                    plt.tick_params(axis='z', labelsize=16)\n",
    "                    ax.grid()\n",
    "                    red_patch = mpatches.Patch(color='red', label='Non-Funcional Tools')\n",
    "                    blue_patch = mpatches.Patch(color='blue', label='Funcional Tools')\n",
    "                    plt.legend(handles=[red_patch,blue_patch],fontsize = 20)\n",
    "                    plt.show()\n",
    "                    fig.savefig('ScatterPlot_PCA_{}.png'.format(Output_Id), bbox_inches='tight')\n",
    "                    \n",
    "                    # Now change to PCA Analyses directory\n",
    "\n",
    "                    os.chdir( PCA_Analyses_path )\n",
    "\n",
    "                    general_features_contribution.to_csv('unique_features_used_{}.csv'.format(Output_Id),index = False)\n",
    "                    sensors_contribution.to_csv('sensors_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "                    features_contribution.to_csv('features_weighted_contribution_{}.csv'.format(Output_Id), index=True)\n",
    "\n",
    "            # Now change to PCA Analyses directory\n",
    "\n",
    "            os.chdir( PCA_Analyses_path )\n",
    "            \n",
    "            np.savetxt(\"features_reduzidas_\" + str(Output_Id) + \".csv\", features_reduzidas, delimiter=',')\n",
    "\n",
    "            Output = {'ReducedFeatures': features_reduzidas,\n",
    "                      'ID': Output_Id} \n",
    "        elif (Chose == 'Test'): \n",
    "\n",
    "            Output = {'ID': Output_Id}\n",
    "        \n",
    "        # Now change back to base directory\n",
    "\n",
    "        os.chdir( base_path )\n",
    "\n",
    "        return Output\n",
    "    \n",
    "        print(\"Wrong Choose entry, verify this input.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_projection (features):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "        \n",
    "    add_path3 = \"/.Kernel/\"\n",
    "  \n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path3\n",
    "        \n",
    "    # Now change to PCA Figures directory\n",
    "\n",
    "    os.chdir( Kernel_path )\n",
    "\n",
    "    # load the model from disk\n",
    "    loaded_pca = pickle.load(open('pca.sav', 'rb'))\n",
    "\n",
    "    scaler = StandardScaler().fit(features)\n",
    "    features_padronizadas = scaler.transform(features)\n",
    "\n",
    "    features_reduzidas = loaded_pca.transform(features_padronizadas)\n",
    "    \n",
    "    print('Filtered Features')\n",
    "    print('-' * 20)\n",
    "    print(np.size(features_padronizadas,0))\n",
    "    print(np.size(features_padronizadas,1))\n",
    "    print('-' * 20)\n",
    "    print('Reduced Features')\n",
    "    print('-' * 20)\n",
    "    print(np.size(features_reduzidas,0))\n",
    "    print(np.size(features_reduzidas,1))\n",
    "    \n",
    "    # Now chance to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    return features_reduzidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEeXLskLFQ6q"
   },
   "source": [
    "# Data Partitioning Module\n",
    "    .SODA\n",
    "    .Grouping Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAunBilsFQ6s"
   },
   "source": [
    "# . SODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-ol3Jpe1bO"
   },
   "outputs": [],
   "source": [
    "class cpu_usage(threading.Thread):### Thread to calculate duration and mean cpu percente usage in a SODA classifier\n",
    "    def __init__(self):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.control = True\n",
    "        \n",
    "    def run(self):\n",
    "        cpu = []\n",
    "        t_inicial = time.time()\n",
    "        while self.control:\n",
    "            cpu.append(cpu_percent(interval=1, percpu=True))\n",
    "        t_final = time.time()\n",
    "        self.deltatime = t_final - t_inicial\n",
    "        self.mean_cpu = np.mean(cpu)\n",
    "        \n",
    "    def stop(self):\n",
    "        self.control = False\n",
    "        \n",
    "    def join(self):\n",
    "        threading.Thread.join(self)\n",
    "        return self.deltatime, self.mean_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ep1uNgbAFQ6t"
   },
   "outputs": [],
   "source": [
    "def grid_set(data, N): #SODA process\n",
    "    _ , W = data.shape\n",
    "    AvD1 = data.mean(0)\n",
    "    X1 = np.mean(np.sum(np.power(data,2),axis=1))\n",
    "    grid_trad = np.sqrt(2*(X1 - AvD1*AvD1.T))/N\n",
    "    Xnorm = np.sqrt(np.sum(np.power(data,2),axis=1))\n",
    "    aux = Xnorm\n",
    "    for _ in range(W-1):\n",
    "        aux = np.insert(aux,0,Xnorm.T,axis=1)\n",
    "    data = data / aux\n",
    "    seq = np.argwhere(np.isnan(data))\n",
    "    if tuple(seq[::]): data[tuple(seq[::])] = 1\n",
    "    AvD2 = data.mean(0)\n",
    "    grid_angl = np.sqrt(1-AvD2*AvD2.T)/N\n",
    "    return X1, AvD1, AvD2, grid_trad, grid_angl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHAa-LjmFQ6y"
   },
   "outputs": [],
   "source": [
    "def pi_calculator(Uniquesample, mode):#SODA process\n",
    "    UN, W = Uniquesample.shape\n",
    "    if mode == 'euclidean' or mode == 'mahalanobis' or mode == 'cityblock' or mode == 'chebyshev' or mode == 'canberra':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA1)\n",
    "        aux2 = [Uniquesample[i]-aux[i] for i in range(UN)]\n",
    "        uspi = np.sum(np.power(aux2,2),axis=1)+DT1\n",
    "    \n",
    "    if mode == 'minkowski':\n",
    "        AA1 = Uniquesample.mean(0)\n",
    "        X1 = sum(sum(np.power(Uniquesample,2)))/UN\n",
    "        DT1 = X1 - sum(np.power(AA1,2))\n",
    "        aux = np.matrix(AA1)\n",
    "        for i in range(UN-1): aux = np.insert(aux,0,AA1,axis=0)\n",
    "        aux = np.array(aux)\n",
    "        uspi = np.sum(np.power(cdist(Uniquesample, aux, mode, p=1.5),2),1)+DT1\n",
    "    \n",
    "    if mode == 'cosine':\n",
    "        Xnorm = np.matrix(np.sqrt(np.sum(np.power(Uniquesample,2),axis=1))).T\n",
    "        aux2 = Xnorm\n",
    "        for i in range(W-1):\n",
    "            aux2 = np.insert(aux2,0,Xnorm.T,axis=1)\n",
    "        Uniquesample1 = Uniquesample / aux2\n",
    "        AA2 = np.mean(Uniquesample1,0)\n",
    "        X2 = 1\n",
    "        DT2 = X2 - np.sum(np.power(AA2,2))\n",
    "        aux = []\n",
    "        for i in range(UN): aux.append(AA2)\n",
    "        aux2 = [Uniquesample1[i]-aux[i] for i in range(UN)]\n",
    "        uspi = np.sum(np.sum(np.power(aux2,2),axis=1),axis=1)+DT2\n",
    "        \n",
    "    return uspi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNU3o7WCFQ64"
   },
   "outputs": [],
   "source": [
    "def Globaldensity_Calculator(data, distancetype):#SODA process\n",
    "    \n",
    "    Uniquesample, J, K = np.unique(data, axis=0, return_index=True, return_inverse=True)\n",
    "    Frequency, _ = np.histogram(K,bins=len(J))\n",
    "    uspi1 = pi_calculator(Uniquesample, distancetype)\n",
    "    sum_uspi1 = sum(uspi1)\n",
    "    Density_1 = uspi1 / sum_uspi1\n",
    "    uspi2 = pi_calculator(Uniquesample, 'cosine')\n",
    "    sum_uspi2 = sum(uspi2)\n",
    "    Density_2 = uspi1 / sum_uspi2\n",
    "    \n",
    "    GD = (Density_2+Density_1) * Frequency\n",
    "\n",
    "    index = GD.argsort()[::-1]\n",
    "    GD = GD[index]\n",
    "    Uniquesample = Uniquesample[index]\n",
    "    Frequency = Frequency[index]\n",
    " \n",
    "    return GD, Uniquesample, Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dCOT-mm5FQ68"
   },
   "outputs": [],
   "source": [
    "def chessboard_division(Uniquesample, MMtypicality, interval1, interval2, distancetype):#SODA process\n",
    "    L, W = Uniquesample.shape\n",
    "    if distancetype == 'euclidean':\n",
    "        W = 1\n",
    "    BOX = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_miu = [Uniquesample[k] for k in range(W)]\n",
    "    BOX_S = [1]*W\n",
    "    BOX_X = [sum(Uniquesample[k]**2) for k in range(W)]\n",
    "    NB = W\n",
    "    BOXMT = [MMtypicality[k] for k in range(W)]\n",
    "    \n",
    "    for i in range(W,L):\n",
    "        if distancetype == 'minkowski':\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype, p=1.5)\n",
    "        else:\n",
    "            a = cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric=distancetype)\n",
    "        \n",
    "        b = np.sqrt(cdist(Uniquesample[i].reshape(1,-1), BOX_miu, metric='cosine'))\n",
    "        distance = np.array([a[0],b[0]]).T\n",
    "        SQ = []\n",
    "        for j,d in enumerate(distance):\n",
    "            if d[0] < interval1 and d[1] < interval2:\n",
    "                SQ.append(j)\n",
    "        #SQ = np.argwhere(distance[::,0]<interval1 and (distance[::,1]<interval2))\n",
    "        COUNT = len(SQ)\n",
    "        if COUNT == 0:\n",
    "            BOX.append(Uniquesample[i])\n",
    "            NB = NB + 1\n",
    "            BOX_S.append(1)\n",
    "            BOX_miu.append(Uniquesample[i])\n",
    "            BOX_X.append(sum(Uniquesample[i]**2))\n",
    "            BOXMT.append(MMtypicality[i])\n",
    "        if COUNT >= 1:\n",
    "            DIS = distance[SQ[::],0]/interval1 + distance[SQ[::],1]/interval2 # pylint: disable=E1136  # pylint/issues/3139\n",
    "            b = np.argmin(DIS)\n",
    "            BOX_S[SQ[b]] = BOX_S[SQ[b]] + 1\n",
    "            BOX_miu[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_miu[SQ[b]] + Uniquesample[i]/BOX_S[SQ[b]]\n",
    "            BOX_X[SQ[b]] = (BOX_S[SQ[b]]-1)/BOX_S[SQ[b]]*BOX_X[SQ[b]] + sum(Uniquesample[i]**2)/BOX_S[SQ[b]]\n",
    "            BOXMT[SQ[b]] = BOXMT[SQ[b]] + MMtypicality[i]\n",
    "\n",
    "\n",
    "    return BOX, BOX_miu, BOX_X, BOX_S, BOXMT, NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2TZty8dFQ6_"
   },
   "outputs": [],
   "source": [
    "def ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,Internval1,Internval2, distancetype):#SODA process\n",
    "    Centers = []\n",
    "    n = 2\n",
    "    ModeNumber = 0\n",
    "           \n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype, p=1.5))\n",
    "    else:\n",
    "        distance1 = squareform(pdist(BOX_miu,metric=distancetype))        \n",
    "\n",
    "    distance2 = np.sqrt(squareform(pdist(BOX_miu,metric='cosine')))\n",
    "      \n",
    "    for i in range(NB):\n",
    "        seq = []\n",
    "        for j,(d1,d2) in enumerate(zip(distance1[i],distance2[i])):\n",
    "            if d1 < n*Internval1 and d2 < n*Internval2:\n",
    "                seq.append(j)\n",
    "        Chessblocak_typicality = [BOXMT[j] for j in seq]\n",
    "\n",
    "        if max(Chessblocak_typicality) == BOXMT[i]:\n",
    "            Centers.append(BOX_miu[i])\n",
    "            ModeNumber = ModeNumber + 1\n",
    "\n",
    "    return Centers, ModeNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gNgPN7dFQ7D"
   },
   "outputs": [],
   "source": [
    "def cloud_member_recruitment(ModelNumber,Center_samples,Uniquesample,grid_trad,grid_angl, distancetype):#SODA process\n",
    "    L, W = Uniquesample.shape\n",
    "    Membership = np.zeros((L,ModelNumber))\n",
    "    Members = np.zeros((L,ModelNumber*W))\n",
    "    Count = []\n",
    "    \n",
    "    if distancetype == 'minkowski':\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype, p=1.5)/grid_trad\n",
    "    else:\n",
    "        distance1 = cdist(Uniquesample,Center_samples, metric=distancetype)/grid_trad\n",
    "\n",
    "    distance2 = np.sqrt(cdist(Uniquesample, Center_samples, metric='cosine'))/grid_angl\n",
    "    distance3 = distance1 + distance2\n",
    "    B = distance3.argmin(1)\n",
    "\n",
    "    for i in range(ModelNumber):\n",
    "        seq = []\n",
    "        for j,b in enumerate(B):\n",
    "            if b == i:\n",
    "                seq.append(j)\n",
    "        Count.append(len(seq))\n",
    "        Membership[:Count[i]:,i] = seq\n",
    "        Members[:Count[i]:,W*i:W*(i+1)] = [Uniquesample[j] for j in seq]\n",
    "    MemberNumber = Count\n",
    "    \n",
    "    #Converte a matriz para vetor E SOMA +1 PARA NAO TER CONJUNTO 0'\n",
    "    B = B.A1\n",
    "    B = [x+1 for x in B]\n",
    "    return Members,MemberNumber,Membership,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzmeIoLIFQ7I"
   },
   "outputs": [],
   "source": [
    "def SelfOrganisedDirectionAwareDataPartitioning(Input, Mode):#SODA process\n",
    "    \n",
    "    \"\"\"\n",
    "    Self-organising Direction-Aware Data Partitioning (offline version)\n",
    "    :params:\n",
    "    \n",
    "    :Input: dict containing gridsize, data and distance methodology\n",
    "    :Mode: Offline or Evolving (online)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if Mode == 'Offline':\n",
    "        data = Input['StaticData']\n",
    "\n",
    "        L = data.shape[0]\n",
    "        N = Input['GridSize']\n",
    "        distancetype = Input['DistanceType']\n",
    "        X1, AvD1, AvD2, grid_trad, grid_angl = grid_set(data,N)\n",
    "        GD, Uniquesample, Frequency = Globaldensity_Calculator(data, distancetype)\n",
    "\n",
    "        BOX,BOX_miu,BOX_X,BOX_S,BOXMT,NB = chessboard_division(Uniquesample,GD,grid_trad,grid_angl, distancetype)\n",
    "        Center,ModeNumber = ChessBoard_PeakIdentification(BOX_miu,BOXMT,NB,grid_trad,grid_angl, distancetype)\n",
    "        Members,Membernumber,Membership,IDX = cloud_member_recruitment(ModeNumber,Center,data,grid_trad,grid_angl, distancetype)\n",
    "        \n",
    "        Boxparameter = {'BOX': BOX,\n",
    "                'BOX_miu': BOX_miu,\n",
    "                'BOX_S': BOX_S,\n",
    "                'NB': NB,\n",
    "                'XM': X1,\n",
    "                'L': L,\n",
    "                'AvM': AvD1,\n",
    "                'AvA': AvD2,\n",
    "                'GridSize': N}\n",
    "        \n",
    "    if Mode == 'Evolving':\n",
    "        print(Mode)\n",
    "\n",
    "    Output = {'C': Center,\n",
    "              'IDX': IDX,\n",
    "              'SystemParams': Boxparameter,\n",
    "              'DistanceType': distancetype}\n",
    "           \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tayjmyeOFQ7N"
   },
   "source": [
    "#### Distâncias ####\n",
    " \n",
    " - euclidean - linha reta entre os pontos\n",
    " - mahalanobis - correlação entre as variaveis (determina similaridade)\n",
    " - cityblock - distancia das projeções dos pontos (taxicab/manhattan)\n",
    " - chebyshev - maior distancia entre as coordenadas (rei)\n",
    " - minkowski - generalização de outras distâncias:\n",
    "  - p = 1 $\\rightarrow$ cityblock,\n",
    "  - p = 2 $\\rightarrow$ euclidean,\n",
    "  - p = $\\infty$ $\\rightarrow$ chebyshev.\n",
    " - canberra - versão com pesos da cityblock, sensivel para pontos proximos à origem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PE6QHo8bFQ7O"
   },
   "outputs": [],
   "source": [
    "def SODA (ReducedFeatures, min_granularity, max_granularity, pace):#SODA\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    \n",
    "    DataSetID = ReducedFeatures['ID']\n",
    "    data = ReducedFeatures['ReducedFeatures']\n",
    "    data = np.matrix(data)\n",
    "\n",
    "    distances = ['euclidean']#, 'mahalanobis', 'cityblock', 'chebyshev', 'minkowski', 'canberra']\n",
    "    processing_parameters = []\n",
    "    \n",
    "    #### Looping SODA within the chosen granularities and distances ####\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "\n",
    "        for d in distances:\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "            \n",
    "            Input = {'GridSize':g, 'StaticData':data, 'DistanceType': d}\n",
    "            \n",
    "            out = SelfOrganisedDirectionAwareDataPartitioning(Input,'Offline')\n",
    "            \n",
    "            ### Interrupt Thread and Calculate Parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            pp = {'DistanceType': d,\n",
    "                  'Granularity': g,\n",
    "                  'Time': deltatime,\n",
    "                  'CPUPercent': mean_cpu}\n",
    "            processing_parameters.append(pp)\n",
    "\n",
    "            \n",
    "            np.savetxt('SODA_' + d + '_label_' + str (DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', out['IDX'],delimiter=',')\n",
    "\n",
    "            \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "    \n",
    "    np.save(('processing_parameters.npy'), processing_parameters)\n",
    "    np.save(('distances.npy'), distances)\n",
    "    np.save(('Min_g.npy'), min_granularity)\n",
    "    np.save(('Max_g.npy'), max_granularity)\n",
    "    np.save(('Pace.npy'), pace)\n",
    "    \n",
    "    Output = {'Distances': distances,\n",
    "              'Min_g': min_granularity,\n",
    "              'Max_g': max_granularity,\n",
    "              'Pace': pace,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "    \n",
    "    return Output, processing_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ijABlt6FQ7R"
   },
   "source": [
    "# . Grouping Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlPWb0RZFQ7g"
   },
   "outputs": [],
   "source": [
    "def GroupingAlgorithm (SODA_parameters,define_percent, processing_parameters): #Grouping Algorithm\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path1 = \"/PCA_Analyses/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    add_path4 = \"/Grouping_Analyses/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    PCA_Analyses_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "    Grouping_Analyses_path = working_path + add_path4\n",
    "    \n",
    "    print('             ')\n",
    "    print('Grouping Algorithm Control Output')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    \n",
    "    ####   imput data    ####\n",
    "    DataSetID = SODA_parameters['ID']\n",
    "    min_granularity = SODA_parameters['Min_g']\n",
    "    max_granularity = SODA_parameters['Max_g']\n",
    "    pace = SODA_parameters['Pace']\n",
    "    distances = SODA_parameters['Distances']\n",
    "\n",
    "    # Change to Kernel directory\n",
    "    os.chdir(Kernel_path)\n",
    "\n",
    "    y_original = np.genfromtxt('FinalTarget.csv', delimiter=',')\n",
    "\n",
    "    # functional engines\n",
    "    #n_IDs_gp1 = 0 # non-functional engines\n",
    "    #n_IDs_gp2 = 3598 # eminent  fault  engines\n",
    "\n",
    "    for d in distances:\n",
    "        \n",
    "        for g in np.arange(int(min_granularity), int (max_granularity + pace), pace):\n",
    "            ### Start Thread\n",
    "            time_cpu_thread = cpu_usage()\n",
    "            time_cpu_thread.start()\n",
    "    \n",
    "            s = 'SODA_' + d + '_label_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv'\n",
    "            \n",
    "\n",
    "            #### Data-base Imput ####\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            SodaOutput = np.genfromtxt( s , delimiter=',')\n",
    "            \n",
    "            # Now change to PCA Analyses directory\n",
    "    \n",
    "            os.chdir( PCA_Analyses_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "            \n",
    "            SelectedFeatures = np.genfromtxt('features_reduzidas_' + str(DataSetID) + '.csv' , delimiter=',')\n",
    "\n",
    "            #### Program Matrix's and Variables ####\n",
    "\n",
    "            n_DA_planes = np.max(SodaOutput)\n",
    "            Percent = np.zeros((int(n_DA_planes),3))\n",
    "            n_IDs_per_gp = np.zeros((int(n_DA_planes),2))\n",
    "            n_tot_Id_per_DA = np.zeros((int(n_DA_planes),1))\n",
    "            decision = np.zeros(int(n_DA_planes))\n",
    "            selected_samples = np.zeros(2)\n",
    "            n_DA_excluded = 0\n",
    "            n_excluded = 0\n",
    "            n_gp0 = 0\n",
    "            n_gp1 = 0\n",
    "            n_gp2 = 0\n",
    "            n_data_def = 0\n",
    "            k = 0\n",
    "\n",
    "            #### Definition Percentage Calculation #####\n",
    "\n",
    "            for i in range(y_original.shape[0]):\n",
    "    \n",
    "                if y_original[i] == 0:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]-1),0] += 1 \n",
    "                else:\n",
    "                    n_IDs_per_gp [int(SodaOutput[i]-1),1] += 1 \n",
    "\n",
    "                n_tot_Id_per_DA [int(SodaOutput[i]-1)] += 1 \n",
    "\n",
    "\n",
    "            for i in range(int(n_DA_planes)):\n",
    "    \n",
    "                Percent[i,0] = (n_IDs_per_gp[i,0] / n_tot_Id_per_DA[i]) * 100\n",
    "                Percent[i,1] = (n_IDs_per_gp[i,1] / n_tot_Id_per_DA[i]) * 100\n",
    "                #Percent[i,2] = ((n_tot_Id_per_DA[i]  -  (n_IDs_per_gp[i,0] + n_IDs_per_g[i,1])) / n_tot_Id_per_DA[i]) * 100\n",
    "    \n",
    "            #### Using Definition Percentage as Decision Parameter ####\n",
    "\n",
    "            for i in range(Percent.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "    \n",
    "                if (Percent[i,0] >= define_percent):\n",
    "                    n_gp0 = n_gp0 + 1         \n",
    "                    decision[i] = 0\n",
    "                elif (Percent[i,1] >= define_percent):    \n",
    "                    n_gp1 = n_gp1 + 1 \n",
    "                    decision[i] = 1\n",
    "                elif (Percent[i,2] >= define_percent):\n",
    "                    n_gp2 = n_gp2 + 1 \n",
    "                    decision[i] = 2\n",
    "                else:\n",
    "                    n_DA_excluded += 1\n",
    "                    decision[i] = -1\n",
    "            \n",
    "            #### Using decision matrix to determine the number of excluded data\n",
    "                       \n",
    "            for i in range (len (decision)):\n",
    "\n",
    "                if decision[i] == -1:\n",
    "                    \n",
    "                    n_excluded += np.sum(n_IDs_per_gp[i,:])\n",
    "                    \n",
    "        \n",
    "            #### Passing data of well defined DA planes to SelectedData and defining labels\n",
    "\n",
    "            SelectedData = np.zeros((int(SelectedFeatures.shape[0] - n_excluded),int(SelectedFeatures.shape[1])))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            ClassifiersLabel = np.zeros((int(SelectedFeatures.shape[0] - n_excluded)))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            ComparisonLabel = np.zeros((int(y_original.shape[0] - n_excluded)))# pylint: disable=E1136  # pylint/issues/3139\n",
    "            \n",
    "            for i in range (SodaOutput.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "                if decision[int (SodaOutput[i]-1)] != -1:\n",
    "    \n",
    "                    SelectedData[k] = SelectedFeatures[i]\n",
    "                    ClassifiersLabel [k] = decision[int (SodaOutput[i]-1)]\n",
    "                    ComparisonLabel [k] = y_original[i]\n",
    "                    k += 1\n",
    "\n",
    "            for i in range (decision.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "\n",
    "                if decision[i] != -1:\n",
    "                    \n",
    "                    selected_samples[0] += n_IDs_per_gp[i,0]\n",
    "                    selected_samples[1] += n_IDs_per_gp[i,1]      \n",
    "\n",
    "            #### Printing Processed Data, ID's and Percentage\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Kernel_path )\n",
    "    \n",
    "            #retval = os.getcwd()\n",
    "            #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "            #np.savetxt('X_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', SelectedData, delimiter=',')\n",
    "            #np.savetxt('Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', ClassifiersLabel, delimiter=',')\n",
    "            #np.savetxt('Original_Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.csv', ComparisonLabel, delimiter=',')\n",
    "            np.save('X_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', SelectedData)\n",
    "            np.save('Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', ClassifiersLabel)\n",
    "            np.save('Original_Y_' + str(define_percent) + '_' + d + '_Labels_' + str(DataSetID) + '_' + str(\"%.2f\" % g) + '.npy', ComparisonLabel)\n",
    "            ### Interrupt Thread and recalculate parameters\n",
    "            time_cpu_thread.stop()\n",
    "            deltatime, mean_cpu = time_cpu_thread.join()\n",
    "            for pp in processing_parameters:\n",
    "                if pp['DistanceType'] == d and pp['Granularity'] == g:\n",
    "                    aux = pp\n",
    "                    break\n",
    "            totaltime = deltatime + aux['Time']\n",
    "            cpu_percent = (mean_cpu + aux['CPUPercent'])/2\n",
    "            \n",
    "            \n",
    "            ### Printig Analitics results\n",
    "            \n",
    "            print(s)\n",
    "            print('Number of data clouds: %d' % n_DA_planes)\n",
    "            print('Number of good tools groups: %d' % n_gp0)\n",
    "            print('Number of worn tools groups: %d' % n_gp1)\n",
    "            print('Number of excluded data clouds: %d' % n_DA_excluded)\n",
    "            print('Number of samples: %d' % int(SodaOutput.shape[0])) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            print('Number of good tools samples: %d' % int(selected_samples[0]))\n",
    "            print('Number of worn tools samples: %d' % int(selected_samples[1]))\n",
    "            print('Number of excluded samples: %d' % n_excluded)\n",
    "            print('Data representation loss: %.2f' % (100-((SelectedData.shape[0] / SelectedFeatures.shape[0]) * 100))) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            print('Analyse execution time: %.6f segundos' % totaltime)\n",
    "            print('Avarage CPU usage: %.2f' % cpu_percent)\n",
    "            print('---------------------------------------------------')\n",
    "            \n",
    "            #### Saving Processed Data, ID's and Percentage\n",
    "            \n",
    "            # Now change to Kernel directory\n",
    "    \n",
    "            os.chdir( Grouping_Analyses_path )\n",
    "            \n",
    "            Grouping_Analyse = open(\"Grouping_Analyse_ID_\" + str(DataSetID) + \"_min_\" + str(min_granularity) + \"_max_\" + str(max_granularity) + '_' + str(define_percent) +\"%.txt\",\"w+\")\n",
    "            Grouping_Analyse.write(s)\n",
    "            Grouping_Analyse.write('\\nNumber of data clouds: %d\\n' % n_DA_planes)\n",
    "            Grouping_Analyse.write('Number of good tools groups: %d\\n' % n_gp0)\n",
    "            Grouping_Analyse.write('Number of worn tools groups: %d\\n' % n_gp1)\n",
    "            Grouping_Analyse.write('Number of excluded data clouds: %d\\n' % n_DA_excluded)\n",
    "            Grouping_Analyse.write('Number of samples: %d\\n' % int(SodaOutput.shape[0]))\n",
    "            Grouping_Analyse.write('Number of good tools samples: %d\\n' % int(selected_samples[0]))\n",
    "            Grouping_Analyse.write('Number of worn tools samples: %d\\n' % int(selected_samples[1]))\n",
    "            Grouping_Analyse.write('Number of excluded samples: %d\\n' % n_excluded)\n",
    "            Grouping_Analyse.write('Data representation loss: %.2f\\n' % (100-((SelectedData.shape[0] / SelectedFeatures.shape[0]) * 100))) # pylint: disable=E1136  # pylint/issues/3139\n",
    "            Grouping_Analyse.write('Analyse execution time: %.6f segundos\\n' % totaltime)\n",
    "            Grouping_Analyse.write('Avarage CPU usage: %.2f\\n' % cpu_percent)\n",
    "            Grouping_Analyse.write('---------------------------------------------------')\n",
    "            \n",
    "            Grouping_Analyse.close()\n",
    "    #np.savetxt('Percent.csv',define_percent,delimiter = ',')\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( Recovery_path )\n",
    "\n",
    "    np.save(\"define_percent.npy\",define_percent)\n",
    "    \n",
    "    Output = {'Percent': define_percent,\n",
    "              'Distances': distances,\n",
    "              'Pace': pace,\n",
    "              'ID': DataSetID}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path )\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             \n",
      "Grouping Algorithm Control Output\n",
      "----------------------------------\n",
      "SODA_euclidean_label_21_2.00.csv\n",
      "Number of data clouds: 3\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 0\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 1\n",
      "Number of worn tools samples: 0\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.003248 segundos\n",
      "Avarage CPU usage: 29.94\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.25.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.018388 segundos\n",
      "Avarage CPU usage: 30.45\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.50.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 144\n",
      "Number of worn tools samples: 116\n",
      "Number of excluded samples: 31\n",
      "Data representation loss: 10.65\n",
      "Analyse execution time: 2.004582 segundos\n",
      "Avarage CPU usage: 32.95\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.75.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 121\n",
      "Number of excluded samples: 24\n",
      "Data representation loss: 8.25\n",
      "Analyse execution time: 2.005439 segundos\n",
      "Avarage CPU usage: 21.40\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 121\n",
      "Number of excluded samples: 24\n",
      "Data representation loss: 8.25\n",
      "Analyse execution time: 2.005231 segundos\n",
      "Avarage CPU usage: 24.10\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 131\n",
      "Number of worn tools samples: 75\n",
      "Number of excluded samples: 85\n",
      "Data representation loss: 29.21\n",
      "Analyse execution time: 2.005377 segundos\n",
      "Avarage CPU usage: 26.56\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.50.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 5\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 124\n",
      "Number of excluded samples: 21\n",
      "Data representation loss: 7.22\n",
      "Analyse execution time: 2.004250 segundos\n",
      "Avarage CPU usage: 21.80\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.75.csv\n",
      "Number of data clouds: 10\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 120\n",
      "Number of worn tools samples: 113\n",
      "Number of excluded samples: 58\n",
      "Data representation loss: 19.93\n",
      "Analyse execution time: 2.009412 segundos\n",
      "Avarage CPU usage: 17.54\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.00.csv\n",
      "Number of data clouds: 14\n",
      "Number of good tools groups: 6\n",
      "Number of worn tools groups: 5\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 121\n",
      "Number of worn tools samples: 113\n",
      "Number of excluded samples: 57\n",
      "Data representation loss: 19.59\n",
      "Analyse execution time: 2.004953 segundos\n",
      "Avarage CPU usage: 17.60\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.25.csv\n",
      "Number of data clouds: 18\n",
      "Number of good tools groups: 7\n",
      "Number of worn tools groups: 8\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 132\n",
      "Number of worn tools samples: 133\n",
      "Number of excluded samples: 26\n",
      "Data representation loss: 8.93\n",
      "Analyse execution time: 2.020795 segundos\n",
      "Avarage CPU usage: 19.64\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.50.csv\n",
      "Number of data clouds: 20\n",
      "Number of good tools groups: 9\n",
      "Number of worn tools groups: 9\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 140\n",
      "Number of worn tools samples: 128\n",
      "Number of excluded samples: 23\n",
      "Data representation loss: 7.90\n",
      "Analyse execution time: 2.011532 segundos\n",
      "Avarage CPU usage: 18.96\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.75.csv\n",
      "Number of data clouds: 21\n",
      "Number of good tools groups: 9\n",
      "Number of worn tools groups: 9\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 126\n",
      "Number of worn tools samples: 120\n",
      "Number of excluded samples: 45\n",
      "Data representation loss: 15.46\n",
      "Analyse execution time: 2.011815 segundos\n",
      "Avarage CPU usage: 22.31\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ClassificationPar = GroupingAlgorithm(SODA_parameters,85, processing_parameters) # (Labels do SODA, Porcentagem de definição, numero de ID's boas, parametros de processamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "882l7VYuFQ7s"
   },
   "source": [
    "# Classification Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNNEQZlzrRuC"
   },
   "source": [
    "# .Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR3VB7RRFQ7t"
   },
   "outputs": [],
   "source": [
    "def Classification (ClassificationPar, min_granularity,max_granularity,n_a, plot_matrix=False): #Classifiers\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    add_path1 = \"/Classification/\"\n",
    "    add_path2 = \"/.Kernel/\"\n",
    "    add_path3 = \"/.Recovery/\"\n",
    "    base_path = os.getcwd()\n",
    "    working_path = os.getcwd()\n",
    "    Classification_path = working_path + add_path1\n",
    "    Kernel_path = working_path + add_path2\n",
    "    Recovery_path = working_path + add_path3\n",
    "\n",
    "    # Change to Kernel directory\n",
    "    os.chdir(Kernel_path)\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma='scale'),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "    Output_ID = ClassificationPar['ID']\n",
    "    distances = ClassificationPar['Distances']\n",
    "    pace = ClassificationPar['Pace']\n",
    "    gra = np.arange(min_granularity,max_granularity,pace)\n",
    "\n",
    "    for d in distances:\n",
    "        for g in gra:\n",
    "            try:\n",
    "                # Now change to Kernel directory\n",
    "\n",
    "                os.chdir( Kernel_path )\n",
    "\n",
    "                #retval = os.getcwd()\n",
    "                #print (\"Current working directory %s\" % retval)  \n",
    "                # preprocess dataset, split into training and test part\n",
    "                Accuracy = np.zeros((n_a, len(names)))\n",
    "                #\"Y_60_euclidean_Labels_7_1.25.csv\"\n",
    "                s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.npy'\n",
    "                X = np.load('X_' + s)    \n",
    "                y_soda = np.load('Y_' + s)\n",
    "                y_original = np.load('Original_Y_' + s)\n",
    "\n",
    "                X_train, X_test, y_train_soda, y_test_soda, y_train_original, y_test_original = train_test_split(X, y_soda, y_original, test_size=.4, random_state=42)\n",
    "\n",
    "                #Loop into numbeer od samples\n",
    "                for i in range(Accuracy.shape[0]): # pylint: disable=E1136  # pylint/issues/3139\n",
    "                    k = 0\n",
    "                    # iterate over classifiers\n",
    "                    for name, clf in zip(names, classifiers):\n",
    "\n",
    "                        clf.fit(X_train, y_train_soda)\n",
    "                        score = clf.score(X_test, y_test_original)\n",
    "                        Accuracy[i,k] = (score*100)\n",
    "                        k +=1\n",
    "                            #if plot_matrix:\n",
    "                             #   ClassifiersLabel = list(clf.predict(X_test))\n",
    "                              #  confusionmatrix(ClassificationPar['ID'], d, g, ClassifiersLabel, 'Classifiers', name, y_test_original,plot=True)\n",
    "                              #  print('Claasificatiojn funfando')\n",
    "\n",
    "\n",
    "                        ClassifiersLabel = list(clf.predict(X_test))\n",
    "                            #confusionmatrix(ClassificationPar['ID'], d, g, ClassifiersLabel, 'Classifiers', name, y_test_original)\n",
    "                    #Creating Matrix for Mean an Std. Derivatio\n",
    "                results = np.zeros((len(names),2))\n",
    "\n",
    "                #Calculinng Mean and Std. Derivation \n",
    "                for i in range(len(names)):\n",
    "                    results[i,0] = round (np.mean(Accuracy[:,i]), 2 )\n",
    "                    results[i,1] = round (np.std(Accuracy[:,i]), 2)\n",
    "\n",
    "                # Now change to Grouping Analyses directory\n",
    "\n",
    "                os.chdir( Classification_path )\n",
    "\n",
    "                    #retval = os.getcwd()\n",
    "                    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "                results = pd.DataFrame(results, index = names, columns = ['Media','Desvio'])       \n",
    "                results.to_csv((\"Classification_result_\" + s) )\n",
    "\n",
    "\n",
    "                print('*** {} - {} - {:.2f}  ***'.format(ClassificationPar['ID'], d, g))\n",
    "                print('-------------------------------------')\n",
    "                print(results)\n",
    "                print(' ')\n",
    "\n",
    "            except:\n",
    "                print('*** {} - {} - {:.2f}  ***'.format(Output_ID, d, g))\n",
    "        \n",
    "    # Now change to base directory\n",
    "\n",
    "    os.chdir( base_path )\n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 21 - euclidean - 2.00  ***\n",
      "*** 21 - euclidean - 2.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  86.32     0.0\n",
      "Linear SVM         94.02     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   82.91     0.0\n",
      "Decision Tree      85.47     0.0\n",
      "Random Forest      86.32     0.0\n",
      "Neural Net         89.74     0.0\n",
      "AdaBoost           87.18     0.0\n",
      "Naive Bayes        61.54     0.0\n",
      "QDA                88.89     0.0\n",
      " \n",
      "*** 21 - euclidean - 2.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  91.35     0.0\n",
      "Linear SVM         96.15     0.0\n",
      "RBF SVM            53.85     0.0\n",
      "Gaussian Process   85.58     0.0\n",
      "Decision Tree      88.46     0.0\n",
      "Random Forest      88.46     0.0\n",
      "Neural Net         90.38     0.0\n",
      "AdaBoost           88.46     0.0\n",
      "Naive Bayes        56.73     0.0\n",
      "QDA                92.31     0.0\n",
      " \n",
      "*** 21 - euclidean - 2.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  92.52     0.0\n",
      "Linear SVM         94.39     0.0\n",
      "RBF SVM            54.21     0.0\n",
      "Gaussian Process   85.98     0.0\n",
      "Decision Tree      94.39     0.0\n",
      "Random Forest      91.59     0.0\n",
      "Neural Net         96.26     0.0\n",
      "AdaBoost           96.26     0.0\n",
      "Naive Bayes        57.01     0.0\n",
      "QDA                93.46     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  95.33     0.0\n",
      "Linear SVM         94.39     0.0\n",
      "RBF SVM            54.21     0.0\n",
      "Gaussian Process   86.92     0.0\n",
      "Decision Tree      94.39     0.0\n",
      "Random Forest      94.39     0.0\n",
      "Neural Net         94.39     0.0\n",
      "AdaBoost           97.20     0.0\n",
      "Naive Bayes        61.68     0.0\n",
      "QDA                97.20     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  90.36     0.0\n",
      "Linear SVM         90.36     0.0\n",
      "RBF SVM            67.47     0.0\n",
      "Gaussian Process   86.75     0.0\n",
      "Decision Tree      91.57     0.0\n",
      "Random Forest      90.36     0.0\n",
      "Neural Net         90.36     0.0\n",
      "AdaBoost           89.16     0.0\n",
      "Naive Bayes        87.95     0.0\n",
      "QDA                91.57     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.44     0.0\n",
      "Linear SVM         87.04     0.0\n",
      "RBF SVM            50.00     0.0\n",
      "Gaussian Process   86.11     0.0\n",
      "Decision Tree      91.67     0.0\n",
      "Random Forest      88.89     0.0\n",
      "Neural Net         95.37     0.0\n",
      "AdaBoost           92.59     0.0\n",
      "Naive Bayes        83.33     0.0\n",
      "QDA                98.15     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  93.62     0.0\n",
      "Linear SVM         93.62     0.0\n",
      "RBF SVM            44.68     0.0\n",
      "Gaussian Process   91.49     0.0\n",
      "Decision Tree      90.43     0.0\n",
      "Random Forest      93.62     0.0\n",
      "Neural Net         93.62     0.0\n",
      "AdaBoost           95.74     0.0\n",
      "Naive Bayes        50.00     0.0\n",
      "QDA                94.68     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.68     0.0\n",
      "Linear SVM         91.49     0.0\n",
      "RBF SVM            54.26     0.0\n",
      "Gaussian Process   93.62     0.0\n",
      "Decision Tree      93.62     0.0\n",
      "Random Forest      93.62     0.0\n",
      "Neural Net         96.81     0.0\n",
      "AdaBoost           96.81     0.0\n",
      "Naive Bayes        87.23     0.0\n",
      "QDA                98.94     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.34     0.0\n",
      "Linear SVM         93.40     0.0\n",
      "RBF SVM            44.34     0.0\n",
      "Gaussian Process   89.62     0.0\n",
      "Decision Tree      94.34     0.0\n",
      "Random Forest      94.34     0.0\n",
      "Neural Net         94.34     0.0\n",
      "AdaBoost           94.34     0.0\n",
      "Naive Bayes        58.49     0.0\n",
      "QDA                96.23     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  96.30     0.0\n",
      "Linear SVM         92.59     0.0\n",
      "RBF SVM            50.00     0.0\n",
      "Gaussian Process   87.96     0.0\n",
      "Decision Tree      97.22     0.0\n",
      "Random Forest      91.67     0.0\n",
      "Neural Net         96.30     0.0\n",
      "AdaBoost           94.44     0.0\n",
      "Naive Bayes        71.30     0.0\n",
      "QDA                96.30     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  97.98     0.0\n",
      "Linear SVM         95.96     0.0\n",
      "RBF SVM            44.44     0.0\n",
      "Gaussian Process   90.91     0.0\n",
      "Decision Tree      93.94     0.0\n",
      "Random Forest      95.96     0.0\n",
      "Neural Net         96.97     0.0\n",
      "AdaBoost           95.96     0.0\n",
      "Naive Bayes        91.92     0.0\n",
      "QDA                95.96     0.0\n",
      " \n",
      "*** 21 - euclidean - 5.00  ***\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "Classification (ClassificationPar, 2,5.25, 1, plot_matrix=False) #(Parametros do data-set,  min_grid_size, max_grid_size, numero de vezes a simular, plotar matriz de confusão (True or False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/thiago/Repositories/Lathes_Tool_Project/Model/IPython/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RNHpxwRre-e"
   },
   "source": [
    "# .Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7AB1utxrfIg"
   },
   "outputs": [],
   "source": [
    "def Model_Train (ClassificationPar,d, Model_Name, g):\n",
    "    \n",
    "    #Changing Work Folder\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.getcwd();\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    Recovery_path = base_path + add_path3;\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "    \n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        MLPClassifier(alpha=1,max_iter=500),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "    \n",
    "    for name,clf in zip(names ,classifiers):\n",
    "        \n",
    "        if  name == Model_Name:\n",
    "            \n",
    "            model = clf;\n",
    "            \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "    \n",
    "    s = str (int(ClassificationPar['Percent'] )) + '_' + d + '_Labels_' + str(int(ClassificationPar['ID'])) + '_' + str(\"%.2f\" % g) + '.csv';\n",
    "    X = np.genfromtxt(('X_' + s) , delimiter=',')    \n",
    "    y = np.genfromtxt(('Y_' + s), delimiter=',') \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # save the model to disk\n",
    "\n",
    "    pickle.dump(model, open('model.sav', 'wb'))\n",
    "    \n",
    "    \n",
    "    Output = {'Model': model,\n",
    "              'X': X_test,\n",
    "              'Y': y_test}\n",
    "    \n",
    "    # Now change to base directory\n",
    "    \n",
    "    os.chdir( base_path );\n",
    "    \n",
    "    #retval = os.getcwd()\n",
    "    #print (\"Current working directory %s\" % retval)\n",
    "\n",
    "\n",
    "    return Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ivl2X0OrfQO"
   },
   "source": [
    "# .Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWrXWuGQrfXq"
   },
   "outputs": [],
   "source": [
    "def Model_Predict (projected_data):\n",
    "    \n",
    "    add_path2 = \"/.Kernel/\";\n",
    "    add_path3 = \"/.Recovery/\";\n",
    "    base_path = os.path.dirname(os.path.abspath(\"Model_Unified_Code.ipynb\"));\n",
    "    Kernel_path = base_path + add_path2;\n",
    "    \n",
    "    # Now change to Kernel directory\n",
    "    \n",
    "    os.chdir( Kernel_path )\n",
    "    \n",
    "    model = pickle.load(open('model.sav', 'rb'))\n",
    "    \n",
    "    for i in range (projected_data.shape[0]):\n",
    "        \n",
    "        y_predict = model.predict(projected_data[i,:].reshape(1, -1))\n",
    "    \n",
    "        if y_predict[0] == 0:\n",
    "            print('Ferramenta Boa')\n",
    "        else:\n",
    "            print('Ferramenta Ruim')\n",
    "    \n",
    "        #print ('Label de Teste: %d' % int (projected_data[i]))\n",
    "        print ('Label dado pale NN: %d' % int (y_predict[0]))\n",
    "        print('___________________')\n",
    "        print('                   ')\n",
    "        \n",
    "    # Now change to the base directory\n",
    "    \n",
    "    os.chdir( base_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZFxLQalFQ7y"
   },
   "source": [
    "# Main Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Slicer Control Output\n",
      "----------------------------------\n",
      "Full Matrix: (72750, 5)\n",
      "Main Data: (72750, 4)\n",
      "Labels: (72750,)\n",
      "Main data Number of Ids: 291\n",
      "Main data Number of mesures: 250\n",
      "Main data Number of groups: 15\n",
      "Main data Last group: 11\n",
      "___________________________________________\n",
      "             \n",
      "TSFRESH Control Output\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  3.36it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.88it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.92it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.98it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  3.11it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.92it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.90it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:03<00:00,  2.91it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:04<00:00,  2.40it/s]\n",
      "Feature Extraction: 100%|██████████| 8/8 [00:02<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "D_S_parameters = DataSlicer(20,20,'Main Data')\n",
    "\n",
    "ExtractedNames = TSFRESH_Extraction(D_S_parameters) #(Extração de atributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/thiago/Repositories/Lathes_Tool_Project/Model/IPython/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Features_0.csv\n",
      "Data_Features_1.csv\n",
      "Data_Features_2.csv\n",
      "Data_Features_3.csv\n",
      "Data_Features_4.csv\n",
      "Data_Features_5.csv\n",
      "Data_Features_6.csv\n",
      "Data_Features_7.csv\n",
      "Data_Features_8.csv\n",
      "Data_Features_9.csv\n",
      "Data_Features_10.csv\n",
      "Data_Features_11.csv\n",
      "Data_Features_12.csv\n",
      "Data_Features_13.csv\n",
      "Data_Features_14.csv\n",
      "             \n",
      "PCA Control Output\n",
      "----------------------------------\n",
      "Variation maintained: 71.68\n",
      "                  \n",
      "Filtered Features\n",
      "--------------------\n",
      "291\n",
      "326\n",
      "--------------------\n",
      "Reduced Features\n",
      "--------------------\n",
      "291\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "SelectedFeatures = TSFRESH_Selection(D_S_parameters,ExtractedNames) # (Parametros e resultados da divisão de dados)\n",
    "\n",
    "ReducedFeatures = PCA_calc(SelectedFeatures,12,'Analytics') # (Feautures selecionadas, numero de PC's a manter, mode ('Test','Calc','Specific', 'Analytics'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "SODA_parameters, processing_parameters = SODA(ReducedFeatures,2,8.25,0.25) # (Features reduzidas, granularidade mínima, granularidade máxima, passo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             \n",
      "Grouping Algorithm Control Output\n",
      "----------------------------------\n",
      "SODA_euclidean_label_21_2.00.csv\n",
      "Number of data clouds: 3\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 0\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 1\n",
      "Number of worn tools samples: 0\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.005222 segundos\n",
      "Avarage CPU usage: 16.34\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.25.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.009680 segundos\n",
      "Avarage CPU usage: 12.95\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.50.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.022236 segundos\n",
      "Avarage CPU usage: 11.10\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.75.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 4\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.010704 segundos\n",
      "Avarage CPU usage: 9.23\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 4\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.012573 segundos\n",
      "Avarage CPU usage: 8.64\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 135\n",
      "Number of worn tools samples: 94\n",
      "Number of excluded samples: 62\n",
      "Data representation loss: 21.31\n",
      "Analyse execution time: 2.013195 segundos\n",
      "Avarage CPU usage: 8.90\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.50.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 6\n",
      "Number of excluded data clouds: 0\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 150\n",
      "Number of worn tools samples: 141\n",
      "Number of excluded samples: 0\n",
      "Data representation loss: 0.00\n",
      "Analyse execution time: 2.012910 segundos\n",
      "Avarage CPU usage: 11.28\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.75.csv\n",
      "Number of data clouds: 10\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 4\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 124\n",
      "Number of worn tools samples: 130\n",
      "Number of excluded samples: 37\n",
      "Data representation loss: 12.71\n",
      "Analyse execution time: 2.007283 segundos\n",
      "Avarage CPU usage: 10.70\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.00.csv\n",
      "Number of data clouds: 14\n",
      "Number of good tools groups: 6\n",
      "Number of worn tools groups: 6\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 125\n",
      "Number of worn tools samples: 129\n",
      "Number of excluded samples: 37\n",
      "Data representation loss: 12.71\n",
      "Analyse execution time: 2.007983 segundos\n",
      "Avarage CPU usage: 11.96\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.25.csv\n",
      "Number of data clouds: 18\n",
      "Number of good tools groups: 7\n",
      "Number of worn tools groups: 8\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 132\n",
      "Number of worn tools samples: 133\n",
      "Number of excluded samples: 26\n",
      "Data representation loss: 8.93\n",
      "Analyse execution time: 2.011366 segundos\n",
      "Avarage CPU usage: 9.50\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.50.csv\n",
      "Number of data clouds: 20\n",
      "Number of good tools groups: 9\n",
      "Number of worn tools groups: 9\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 140\n",
      "Number of worn tools samples: 128\n",
      "Number of excluded samples: 23\n",
      "Data representation loss: 7.90\n",
      "Analyse execution time: 2.012050 segundos\n",
      "Avarage CPU usage: 13.04\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.75.csv\n",
      "Number of data clouds: 21\n",
      "Number of good tools groups: 10\n",
      "Number of worn tools groups: 9\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 140\n",
      "Number of worn tools samples: 123\n",
      "Number of excluded samples: 28\n",
      "Data representation loss: 9.62\n",
      "Analyse execution time: 2.005064 segundos\n",
      "Avarage CPU usage: 10.98\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_5.00.csv\n",
      "Number of data clouds: 22\n",
      "Number of good tools groups: 10\n",
      "Number of worn tools groups: 10\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 140\n",
      "Number of worn tools samples: 123\n",
      "Number of excluded samples: 28\n",
      "Data representation loss: 9.62\n",
      "Analyse execution time: 2.004822 segundos\n",
      "Avarage CPU usage: 9.65\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_5.25.csv\n",
      "Number of data clouds: 25\n",
      "Number of good tools groups: 11\n",
      "Number of worn tools groups: 12\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 139\n",
      "Number of worn tools samples: 131\n",
      "Number of excluded samples: 21\n",
      "Data representation loss: 7.22\n",
      "Analyse execution time: 2.010500 segundos\n",
      "Avarage CPU usage: 11.45\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_5.50.csv\n",
      "Number of data clouds: 24\n",
      "Number of good tools groups: 11\n",
      "Number of worn tools groups: 12\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 136\n",
      "Number of excluded samples: 9\n",
      "Data representation loss: 3.09\n",
      "Analyse execution time: 2.005429 segundos\n",
      "Avarage CPU usage: 13.26\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_5.75.csv\n",
      "Number of data clouds: 25\n",
      "Number of good tools groups: 10\n",
      "Number of worn tools groups: 13\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 145\n",
      "Number of worn tools samples: 135\n",
      "Number of excluded samples: 11\n",
      "Data representation loss: 3.78\n",
      "Analyse execution time: 2.005520 segundos\n",
      "Avarage CPU usage: 10.26\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_6.00.csv\n",
      "Number of data clouds: 26\n",
      "Number of good tools groups: 11\n",
      "Number of worn tools groups: 13\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 145\n",
      "Number of worn tools samples: 135\n",
      "Number of excluded samples: 11\n",
      "Data representation loss: 3.78\n",
      "Analyse execution time: 2.006930 segundos\n",
      "Avarage CPU usage: 12.43\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_6.25.csv\n",
      "Number of data clouds: 27\n",
      "Number of good tools groups: 10\n",
      "Number of worn tools groups: 14\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 143\n",
      "Number of worn tools samples: 134\n",
      "Number of excluded samples: 14\n",
      "Data representation loss: 4.81\n",
      "Analyse execution time: 2.005702 segundos\n",
      "Avarage CPU usage: 13.25\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_6.50.csv\n",
      "Number of data clouds: 30\n",
      "Number of good tools groups: 13\n",
      "Number of worn tools groups: 15\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 140\n",
      "Number of worn tools samples: 131\n",
      "Number of excluded samples: 20\n",
      "Data representation loss: 6.87\n",
      "Analyse execution time: 2.006448 segundos\n",
      "Avarage CPU usage: 13.20\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SODA_euclidean_label_21_6.75.csv\n",
      "Number of data clouds: 32\n",
      "Number of good tools groups: 15\n",
      "Number of worn tools groups: 16\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 136\n",
      "Number of excluded samples: 9\n",
      "Data representation loss: 3.09\n",
      "Analyse execution time: 2.011799 segundos\n",
      "Avarage CPU usage: 11.62\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_7.00.csv\n",
      "Number of data clouds: 37\n",
      "Number of good tools groups: 15\n",
      "Number of worn tools groups: 20\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 146\n",
      "Number of worn tools samples: 132\n",
      "Number of excluded samples: 13\n",
      "Data representation loss: 4.47\n",
      "Analyse execution time: 2.006398 segundos\n",
      "Avarage CPU usage: 18.10\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_7.25.csv\n",
      "Number of data clouds: 38\n",
      "Number of good tools groups: 15\n",
      "Number of worn tools groups: 21\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 145\n",
      "Number of worn tools samples: 137\n",
      "Number of excluded samples: 9\n",
      "Data representation loss: 3.09\n",
      "Analyse execution time: 2.011567 segundos\n",
      "Avarage CPU usage: 12.50\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_7.50.csv\n",
      "Number of data clouds: 39\n",
      "Number of good tools groups: 16\n",
      "Number of worn tools groups: 21\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 145\n",
      "Number of worn tools samples: 137\n",
      "Number of excluded samples: 9\n",
      "Data representation loss: 3.09\n",
      "Analyse execution time: 2.006710 segundos\n",
      "Avarage CPU usage: 15.05\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_7.75.csv\n",
      "Number of data clouds: 47\n",
      "Number of good tools groups: 18\n",
      "Number of worn tools groups: 28\n",
      "Number of excluded data clouds: 1\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 148\n",
      "Number of worn tools samples: 137\n",
      "Number of excluded samples: 6\n",
      "Data representation loss: 2.06\n",
      "Analyse execution time: 2.005858 segundos\n",
      "Avarage CPU usage: 17.50\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ClassificationPar = GroupingAlgorithm(SODA_parameters,80, processing_parameters) # (Labels do SODA, Porcentagem de definição, numero de ID's boas, parametros de processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 21 - euclidean - 2.00  ***\n",
      "*** 21 - euclidean - 2.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  86.32     0.0\n",
      "Linear SVM         94.02     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   82.91     0.0\n",
      "Decision Tree      83.76     0.0\n",
      "Random Forest      86.32     0.0\n",
      "Neural Net         88.03     0.0\n",
      "AdaBoost           87.18     0.0\n",
      "Naive Bayes        61.54     0.0\n",
      "QDA                88.89     0.0\n",
      " \n",
      "*** 21 - euclidean - 2.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  87.18     0.0\n",
      "Linear SVM         93.16     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   82.91     0.0\n",
      "Decision Tree      84.62     0.0\n",
      "Random Forest      86.32     0.0\n",
      "Neural Net         89.74     0.0\n",
      "AdaBoost           87.18     0.0\n",
      "Naive Bayes        61.54     0.0\n",
      "QDA                88.89     0.0\n",
      " \n",
      "*** 21 - euclidean - 2.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  88.89     0.0\n",
      "Linear SVM         91.45     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   85.47     0.0\n",
      "Decision Tree      85.47     0.0\n",
      "Random Forest      88.89     0.0\n",
      "Neural Net         83.76     0.0\n",
      "AdaBoost           88.03     0.0\n",
      "Naive Bayes        60.68     0.0\n",
      "QDA                88.03     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.02     0.0\n",
      "Linear SVM         89.74     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   86.32     0.0\n",
      "Decision Tree      89.74     0.0\n",
      "Random Forest      91.45     0.0\n",
      "Neural Net         91.45     0.0\n",
      "AdaBoost           89.74     0.0\n",
      "Naive Bayes        62.39     0.0\n",
      "QDA                94.02     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  91.30     0.0\n",
      "Linear SVM         89.13     0.0\n",
      "RBF SVM            57.61     0.0\n",
      "Gaussian Process   82.61     0.0\n",
      "Decision Tree      86.96     0.0\n",
      "Random Forest      90.22     0.0\n",
      "Neural Net         89.13     0.0\n",
      "AdaBoost           85.87     0.0\n",
      "Naive Bayes        83.70     0.0\n",
      "QDA                89.13     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  91.45     0.0\n",
      "Linear SVM         86.32     0.0\n",
      "RBF SVM            40.17     0.0\n",
      "Gaussian Process   88.89     0.0\n",
      "Decision Tree      88.89     0.0\n",
      "Random Forest      89.74     0.0\n",
      "Neural Net         90.60     0.0\n",
      "AdaBoost           92.31     0.0\n",
      "Naive Bayes        65.81     0.0\n",
      "QDA                95.73     0.0\n",
      " \n",
      "*** 21 - euclidean - 3.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.12     0.0\n",
      "Linear SVM         92.16     0.0\n",
      "RBF SVM            49.02     0.0\n",
      "Gaussian Process   88.24     0.0\n",
      "Decision Tree      91.18     0.0\n",
      "Random Forest      94.12     0.0\n",
      "Neural Net         94.12     0.0\n",
      "AdaBoost           94.12     0.0\n",
      "Naive Bayes        90.20     0.0\n",
      "QDA                96.08     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  95.10     0.0\n",
      "Linear SVM         91.18     0.0\n",
      "RBF SVM            47.06     0.0\n",
      "Gaussian Process   87.25     0.0\n",
      "Decision Tree      90.20     0.0\n",
      "Random Forest      92.16     0.0\n",
      "Neural Net         95.10     0.0\n",
      "AdaBoost           93.14     0.0\n",
      "Naive Bayes        87.25     0.0\n",
      "QDA                97.06     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.34     0.0\n",
      "Linear SVM         93.40     0.0\n",
      "RBF SVM            44.34     0.0\n",
      "Gaussian Process   89.62     0.0\n",
      "Decision Tree      96.23     0.0\n",
      "Random Forest      95.28     0.0\n",
      "Neural Net         96.23     0.0\n",
      "AdaBoost           94.34     0.0\n",
      "Naive Bayes        58.49     0.0\n",
      "QDA                96.23     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  96.30     0.0\n",
      "Linear SVM         92.59     0.0\n",
      "RBF SVM            50.00     0.0\n",
      "Gaussian Process   87.96     0.0\n",
      "Decision Tree      96.30     0.0\n",
      "Random Forest      91.67     0.0\n",
      "Neural Net         93.52     0.0\n",
      "AdaBoost           94.44     0.0\n",
      "Naive Bayes        71.30     0.0\n",
      "QDA                96.30     0.0\n",
      " \n",
      "*** 21 - euclidean - 4.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  95.28     0.0\n",
      "Linear SVM         90.57     0.0\n",
      "RBF SVM            52.83     0.0\n",
      "Gaussian Process   89.62     0.0\n",
      "Decision Tree      91.51     0.0\n",
      "Random Forest      93.40     0.0\n",
      "Neural Net         95.28     0.0\n",
      "AdaBoost           95.28     0.0\n",
      "Naive Bayes        60.38     0.0\n",
      "QDA                96.23     0.0\n",
      " \n",
      "*** 21 - euclidean - 5.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  95.28     0.0\n",
      "Linear SVM         90.57     0.0\n",
      "RBF SVM            52.83     0.0\n",
      "Gaussian Process   89.62     0.0\n",
      "Decision Tree      92.45     0.0\n",
      "Random Forest      95.28     0.0\n",
      "Neural Net         92.45     0.0\n",
      "AdaBoost           95.28     0.0\n",
      "Naive Bayes        60.38     0.0\n",
      "QDA                96.23     0.0\n",
      " \n",
      "*** 21 - euclidean - 5.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  98.15     0.0\n",
      "Linear SVM         91.67     0.0\n",
      "RBF SVM            41.67     0.0\n",
      "Gaussian Process   93.52     0.0\n",
      "Decision Tree      97.22     0.0\n",
      "Random Forest      97.22     0.0\n",
      "Neural Net         98.15     0.0\n",
      "AdaBoost           99.07     0.0\n",
      "Naive Bayes        62.04     0.0\n",
      "QDA                98.15     0.0\n",
      " \n",
      "*** 21 - euclidean - 5.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  92.04     0.0\n",
      "Linear SVM         92.04     0.0\n",
      "RBF SVM            39.82     0.0\n",
      "Gaussian Process   91.15     0.0\n",
      "Decision Tree      91.15     0.0\n",
      "Random Forest      90.27     0.0\n",
      "Neural Net         98.23     0.0\n",
      "AdaBoost           92.04     0.0\n",
      "Naive Bayes        87.61     0.0\n",
      "QDA                95.58     0.0\n",
      " \n",
      "*** 21 - euclidean - 5.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.64     0.0\n",
      "Linear SVM         91.07     0.0\n",
      "RBF SVM            41.96     0.0\n",
      "Gaussian Process   90.18     0.0\n",
      "Decision Tree      90.18     0.0\n",
      "Random Forest      92.86     0.0\n",
      "Neural Net         92.86     0.0\n",
      "AdaBoost           94.64     0.0\n",
      "Naive Bayes        54.46     0.0\n",
      "QDA                93.75     0.0\n",
      " \n",
      "*** 21 - euclidean - 6.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  94.64     0.0\n",
      "Linear SVM         91.07     0.0\n",
      "RBF SVM            41.96     0.0\n",
      "Gaussian Process   90.18     0.0\n",
      "Decision Tree      89.29     0.0\n",
      "Random Forest      91.96     0.0\n",
      "Neural Net         94.64     0.0\n",
      "AdaBoost           94.64     0.0\n",
      "Naive Bayes        54.46     0.0\n",
      "QDA                93.75     0.0\n",
      " \n",
      "*** 21 - euclidean - 6.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  95.50     0.0\n",
      "Linear SVM         93.69     0.0\n",
      "RBF SVM            43.24     0.0\n",
      "Gaussian Process   88.29     0.0\n",
      "Decision Tree      91.89     0.0\n",
      "Random Forest      92.79     0.0\n",
      "Neural Net         91.89     0.0\n",
      "AdaBoost           93.69     0.0\n",
      "Naive Bayes        90.09     0.0\n",
      "QDA                97.30     0.0\n",
      " \n",
      "*** 21 - euclidean - 6.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  96.33     0.0\n",
      "Linear SVM         92.66     0.0\n",
      "RBF SVM            52.29     0.0\n",
      "Gaussian Process   89.91     0.0\n",
      "Decision Tree      92.66     0.0\n",
      "Random Forest      96.33     0.0\n",
      "Neural Net         97.25     0.0\n",
      "AdaBoost           97.25     0.0\n",
      "Naive Bayes        64.22     0.0\n",
      "QDA                94.50     0.0\n",
      " \n",
      "*** 21 - euclidean - 6.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  92.92     0.0\n",
      "Linear SVM         92.04     0.0\n",
      "RBF SVM            39.82     0.0\n",
      "Gaussian Process   92.92     0.0\n",
      "Decision Tree      91.15     0.0\n",
      "Random Forest      92.04     0.0\n",
      "Neural Net         95.58     0.0\n",
      "AdaBoost           96.46     0.0\n",
      "Naive Bayes        89.38     0.0\n",
      "QDA                96.46     0.0\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 21 - euclidean - 7.00  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  99.11     0.0\n",
      "Linear SVM         91.07     0.0\n",
      "RBF SVM            53.57     0.0\n",
      "Gaussian Process   91.07     0.0\n",
      "Decision Tree      95.54     0.0\n",
      "Random Forest      96.43     0.0\n",
      "Neural Net         98.21     0.0\n",
      "AdaBoost           98.21     0.0\n",
      "Naive Bayes        67.86     0.0\n",
      "QDA                97.32     0.0\n",
      " \n",
      "*** 21 - euclidean - 7.25  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  93.81     0.0\n",
      "Linear SVM         91.15     0.0\n",
      "RBF SVM            41.59     0.0\n",
      "Gaussian Process   95.58     0.0\n",
      "Decision Tree      94.69     0.0\n",
      "Random Forest      92.92     0.0\n",
      "Neural Net         95.58     0.0\n",
      "AdaBoost           99.12     0.0\n",
      "Naive Bayes        53.98     0.0\n",
      "QDA                93.81     0.0\n",
      " \n",
      "*** 21 - euclidean - 7.50  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  93.81     0.0\n",
      "Linear SVM         91.15     0.0\n",
      "RBF SVM            41.59     0.0\n",
      "Gaussian Process   95.58     0.0\n",
      "Decision Tree      93.81     0.0\n",
      "Random Forest      92.92     0.0\n",
      "Neural Net         95.58     0.0\n",
      "AdaBoost           99.12     0.0\n",
      "Naive Bayes        53.98     0.0\n",
      "QDA                93.81     0.0\n",
      " \n",
      "*** 21 - euclidean - 7.75  ***\n",
      "-------------------------------------\n",
      "                   Media  Desvio\n",
      "Nearest Neighbors  98.25     0.0\n",
      "Linear SVM         92.11     0.0\n",
      "RBF SVM            38.60     0.0\n",
      "Gaussian Process   93.86     0.0\n",
      "Decision Tree      96.49     0.0\n",
      "Random Forest      97.37     0.0\n",
      "Neural Net         98.25     0.0\n",
      "AdaBoost           98.25     0.0\n",
      "Naive Bayes        43.86     0.0\n",
      "QDA                94.74     0.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "        \n",
    "Classification (ClassificationPar, 2,8, 1, plot_matrix=False) #(Parametros do data-set,  min_grid_size, max_grid_size, numero de vezes a simular, plotar matriz de confusão (True or False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             \n",
      "Grouping Algorithm Control Output\n",
      "----------------------------------\n",
      "SODA_euclidean_label_21_2.00.csv\n",
      "Number of data clouds: 3\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.012036 segundos\n",
      "Avarage CPU usage: 33.44\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.25.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.006015 segundos\n",
      "Avarage CPU usage: 20.84\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.50.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.009543 segundos\n",
      "Avarage CPU usage: 18.62\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_2.75.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.004911 segundos\n",
      "Avarage CPU usage: 28.32\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.003639 segundos\n",
      "Avarage CPU usage: 19.33\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.004862 segundos\n",
      "Avarage CPU usage: 34.10\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.50.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 9\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.013557 segundos\n",
      "Avarage CPU usage: 24.59\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_3.75.csv\n",
      "Number of data clouds: 10\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 8\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005337 segundos\n",
      "Avarage CPU usage: 22.74\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.00.csv\n",
      "Number of data clouds: 14\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 12\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005782 segundos\n",
      "Avarage CPU usage: 28.14\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.25.csv\n",
      "Number of data clouds: 18\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 13\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 15\n",
      "Number of worn tools samples: 8\n",
      "Number of excluded samples: 268\n",
      "Data representation loss: 92.10\n",
      "Analyse execution time: 2.008759 segundos\n",
      "Avarage CPU usage: 31.54\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.50.csv\n",
      "Number of data clouds: 20\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 16\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 14\n",
      "Number of worn tools samples: 2\n",
      "Number of excluded samples: 275\n",
      "Data representation loss: 94.50\n",
      "Analyse execution time: 2.006161 segundos\n",
      "Avarage CPU usage: 22.77\n",
      "---------------------------------------------------\n",
      "SODA_euclidean_label_21_4.75.csv\n",
      "Number of data clouds: 21\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 15\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 19\n",
      "Number of worn tools samples: 3\n",
      "Number of excluded samples: 269\n",
      "Data representation loss: 92.44\n",
      "Analyse execution time: 2.011155 segundos\n",
      "Avarage CPU usage: 28.48\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_2.00.csv\n",
      "Number of data clouds: 3\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 0\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 0\n",
      "Number of excluded samples: 291\n",
      "Data representation loss: 100.00\n",
      "Analyse execution time: 2.006831 segundos\n",
      "Avarage CPU usage: 28.49\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_2.25.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 0\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 0\n",
      "Number of excluded samples: 291\n",
      "Data representation loss: 100.00\n",
      "Analyse execution time: 2.006101 segundos\n",
      "Avarage CPU usage: 16.06\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_2.50.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 0\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 0\n",
      "Number of excluded samples: 291\n",
      "Data representation loss: 100.00\n",
      "Analyse execution time: 2.005742 segundos\n",
      "Avarage CPU usage: 19.79\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_2.75.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005909 segundos\n",
      "Avarage CPU usage: 21.38\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005499 segundos\n",
      "Avarage CPU usage: 19.76\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.007308 segundos\n",
      "Avarage CPU usage: 33.94\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_3.50.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 9\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.007399 segundos\n",
      "Avarage CPU usage: 22.05\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_3.75.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 8\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 10\n",
      "Number of worn tools samples: 2\n",
      "Number of excluded samples: 279\n",
      "Data representation loss: 95.88\n",
      "Analyse execution time: 2.005301 segundos\n",
      "Avarage CPU usage: 32.31\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SODA_mahalanobis_label_21_4.00.csv\n",
      "Number of data clouds: 14\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 11\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 286\n",
      "Data representation loss: 98.28\n",
      "Analyse execution time: 2.010908 segundos\n",
      "Avarage CPU usage: 26.34\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_4.25.csv\n",
      "Number of data clouds: 18\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 13\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 15\n",
      "Number of worn tools samples: 8\n",
      "Number of excluded samples: 268\n",
      "Data representation loss: 92.10\n",
      "Analyse execution time: 2.006595 segundos\n",
      "Avarage CPU usage: 24.44\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_4.50.csv\n",
      "Number of data clouds: 20\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 15\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 18\n",
      "Number of worn tools samples: 3\n",
      "Number of excluded samples: 270\n",
      "Data representation loss: 92.78\n",
      "Analyse execution time: 2.008969 segundos\n",
      "Avarage CPU usage: 26.59\n",
      "---------------------------------------------------\n",
      "SODA_mahalanobis_label_21_4.75.csv\n",
      "Number of data clouds: 20\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 14\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 19\n",
      "Number of worn tools samples: 3\n",
      "Number of excluded samples: 269\n",
      "Data representation loss: 92.44\n",
      "Analyse execution time: 2.007423 segundos\n",
      "Avarage CPU usage: 24.40\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_2.00.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.006508 segundos\n",
      "Avarage CPU usage: 14.44\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_2.25.csv\n",
      "Number of data clouds: 7\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.016691 segundos\n",
      "Avarage CPU usage: 13.33\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_2.50.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006128 segundos\n",
      "Avarage CPU usage: 18.40\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_2.75.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.021101 segundos\n",
      "Avarage CPU usage: 24.11\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005185 segundos\n",
      "Avarage CPU usage: 27.66\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006113 segundos\n",
      "Avarage CPU usage: 26.21\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_3.50.csv\n",
      "Number of data clouds: 12\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 10\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.010133 segundos\n",
      "Avarage CPU usage: 20.57\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_3.75.csv\n",
      "Number of data clouds: 17\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 13\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 7\n",
      "Number of excluded samples: 280\n",
      "Data representation loss: 96.22\n",
      "Analyse execution time: 2.012119 segundos\n",
      "Avarage CPU usage: 18.51\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_4.00.csv\n",
      "Number of data clouds: 23\n",
      "Number of good tools groups: 4\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 16\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 9\n",
      "Number of worn tools samples: 11\n",
      "Number of excluded samples: 271\n",
      "Data representation loss: 93.13\n",
      "Analyse execution time: 2.007247 segundos\n",
      "Avarage CPU usage: 20.55\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_4.25.csv\n",
      "Number of data clouds: 27\n",
      "Number of good tools groups: 6\n",
      "Number of worn tools groups: 4\n",
      "Number of excluded data clouds: 17\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 21\n",
      "Number of worn tools samples: 17\n",
      "Number of excluded samples: 253\n",
      "Data representation loss: 86.94\n",
      "Analyse execution time: 2.005392 segundos\n",
      "Avarage CPU usage: 29.69\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_4.50.csv\n",
      "Number of data clouds: 28\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 3\n",
      "Number of excluded data clouds: 20\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 19\n",
      "Number of worn tools samples: 11\n",
      "Number of excluded samples: 261\n",
      "Data representation loss: 89.69\n",
      "Analyse execution time: 2.004984 segundos\n",
      "Avarage CPU usage: 23.27\n",
      "---------------------------------------------------\n",
      "SODA_cityblock_label_21_4.75.csv\n",
      "Number of data clouds: 32\n",
      "Number of good tools groups: 7\n",
      "Number of worn tools groups: 5\n",
      "Number of excluded data clouds: 20\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 20\n",
      "Number of worn tools samples: 14\n",
      "Number of excluded samples: 257\n",
      "Data representation loss: 88.32\n",
      "Analyse execution time: 2.005873 segundos\n",
      "Avarage CPU usage: 19.26\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_2.00.csv\n",
      "Number of data clouds: 3\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 2\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.005905 segundos\n",
      "Avarage CPU usage: 20.02\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_2.25.csv\n",
      "Number of data clouds: 5\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.006591 segundos\n",
      "Avarage CPU usage: 17.62\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_2.50.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 4\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.009075 segundos\n",
      "Avarage CPU usage: 24.76\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_2.75.csv\n",
      "Number of data clouds: 7\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005622 segundos\n",
      "Avarage CPU usage: 26.63\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SODA_chebyshev_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.004664 segundos\n",
      "Avarage CPU usage: 27.84\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006570 segundos\n",
      "Avarage CPU usage: 35.74\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_3.50.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 9\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005548 segundos\n",
      "Avarage CPU usage: 18.24\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_3.75.csv\n",
      "Number of data clouds: 11\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 9\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006257 segundos\n",
      "Avarage CPU usage: 24.98\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_4.00.csv\n",
      "Number of data clouds: 14\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 12\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.014997 segundos\n",
      "Avarage CPU usage: 32.77\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_4.25.csv\n",
      "Number of data clouds: 17\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 13\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 5\n",
      "Number of worn tools samples: 6\n",
      "Number of excluded samples: 280\n",
      "Data representation loss: 96.22\n",
      "Analyse execution time: 2.008257 segundos\n",
      "Avarage CPU usage: 35.03\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_4.50.csv\n",
      "Number of data clouds: 19\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 16\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 286\n",
      "Data representation loss: 98.28\n",
      "Analyse execution time: 2.005158 segundos\n",
      "Avarage CPU usage: 30.04\n",
      "---------------------------------------------------\n",
      "SODA_chebyshev_label_21_4.75.csv\n",
      "Number of data clouds: 21\n",
      "Number of good tools groups: 5\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 15\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 18\n",
      "Number of worn tools samples: 3\n",
      "Number of excluded samples: 270\n",
      "Data representation loss: 92.78\n",
      "Analyse execution time: 2.007134 segundos\n",
      "Avarage CPU usage: 25.55\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_2.00.csv\n",
      "Number of data clouds: 4\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 3\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.005312 segundos\n",
      "Avarage CPU usage: 20.23\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_2.25.csv\n",
      "Number of data clouds: 6\n",
      "Number of good tools groups: 0\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 0\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 290\n",
      "Data representation loss: 99.66\n",
      "Analyse execution time: 2.005969 segundos\n",
      "Avarage CPU usage: 29.14\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_2.50.csv\n",
      "Number of data clouds: 7\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006250 segundos\n",
      "Avarage CPU usage: 32.50\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_2.75.csv\n",
      "Number of data clouds: 7\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 5\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.005615 segundos\n",
      "Avarage CPU usage: 25.47\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_3.00.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.006922 segundos\n",
      "Avarage CPU usage: 23.30\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_3.25.csv\n",
      "Number of data clouds: 8\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 6\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.010276 segundos\n",
      "Avarage CPU usage: 25.31\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_3.50.csv\n",
      "Number of data clouds: 10\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 8\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 3\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 287\n",
      "Data representation loss: 98.63\n",
      "Analyse execution time: 2.010860 segundos\n",
      "Avarage CPU usage: 19.94\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_3.75.csv\n",
      "Number of data clouds: 12\n",
      "Number of good tools groups: 1\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 9\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 5\n",
      "Number of excluded samples: 282\n",
      "Data representation loss: 96.91\n",
      "Analyse execution time: 2.006863 segundos\n",
      "Avarage CPU usage: 20.12\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_4.00.csv\n",
      "Number of data clouds: 13\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 10\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 286\n",
      "Data representation loss: 98.28\n",
      "Analyse execution time: 2.006099 segundos\n",
      "Avarage CPU usage: 18.00\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_4.25.csv\n",
      "Number of data clouds: 17\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 2\n",
      "Number of excluded data clouds: 12\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 6\n",
      "Number of worn tools samples: 7\n",
      "Number of excluded samples: 278\n",
      "Data representation loss: 95.53\n",
      "Analyse execution time: 2.010248 segundos\n",
      "Avarage CPU usage: 22.30\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_4.50.csv\n",
      "Number of data clouds: 19\n",
      "Number of good tools groups: 2\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 16\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 4\n",
      "Number of worn tools samples: 1\n",
      "Number of excluded samples: 286\n",
      "Data representation loss: 98.28\n",
      "Analyse execution time: 2.005196 segundos\n",
      "Avarage CPU usage: 23.84\n",
      "---------------------------------------------------\n",
      "SODA_minkowski_label_21_4.75.csv\n",
      "Number of data clouds: 19\n",
      "Number of good tools groups: 3\n",
      "Number of worn tools groups: 1\n",
      "Number of excluded data clouds: 15\n",
      "Number of samples: 291\n",
      "Number of good tools samples: 8\n",
      "Number of worn tools samples: 2\n",
      "Number of excluded samples: 281\n",
      "Data representation loss: 96.56\n",
      "Analyse execution time: 2.012257 segundos\n",
      "Avarage CPU usage: 29.39\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-72030f387ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSODA_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSODA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReducedFeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Features reduzidas, granularidade mínima, granularidade máxima, passo)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mClassificationPar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupingAlgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSODA_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_parameters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (Labels do SODA, Porcentagem de definição, numero de ID's boas, parametros de processamento)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mClassification\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mClassificationPar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(Parametros do data-set,  min_grid_size, max_grid_size, numero de vezes a simular, plotar matriz de confusão (True or False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-2d33869f13fd>\u001b[0m in \u001b[0;36mGroupingAlgorithm\u001b[0;34m(SODA_parameters, define_percent, processing_parameters)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m### Interrupt Thread and recalculate parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mtime_cpu_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mdeltatime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_cpu_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessing_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DistanceType'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Granularity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-7d1aab6401a1>\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltatime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_cpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ModelPar = Model_Train(ClassificationPar,'euclidean',\"Neural Net\",2) #(Parametros da data-set, distância, Modelo\n",
    "                                                                        # granularidade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZFxLQalFQ7y"
   },
   "source": [
    "# Raspberry Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  3.92it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.74it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:01<00:00,  5.38it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:01<00:00,  5.32it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.42it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:01<00:00,  5.50it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.03it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.98it/s]\n",
      "Feature Extraction: 100%|██████████| 10/10 [00:02<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Features\n",
      "--------------------\n",
      "37\n",
      "2362\n",
      "--------------------\n",
      "Reduced Features\n",
      "--------------------\n",
      "37\n",
      "3\n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Ruim\n",
      "Label dado pale NN: 1\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n",
      "Ferramenta Boa\n",
      "Label dado pale NN: 0\n",
      "___________________\n",
      "                   \n"
     ]
    }
   ],
   "source": [
    "features = dynamic_tsfresh()\n",
    "\n",
    "projected_data = PCA_projection(features)\n",
    "\n",
    "Model_Predict(projected_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/thiago/Repositories/Lathes_Tool_Project/Model/IPython/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model_Unified_Code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
